# -*- coding: utf-8 -*-
"""
MC Dropout CV åˆ†ä½ˆåˆ†æè…³æœ¬
ç”¨æ–¼æ¸¬è©¦ T+1 å’Œ T+5 æ¨¡å‹çš„ MC Dropout è®Šç•°ä¿‚æ•¸ (CV) åˆ†ä½ˆ

ç›®çš„ï¼š
- äº†è§£ç›®å‰ T+1 æ¨¡å‹æ˜¯å¦æœ‰ Dropoutï¼ˆç„¡ Dropout å‰‡ CV = 0ï¼‰
- æ¯”è¼ƒ T+1 èˆ‡ T+5 çš„ CV åˆ†ä½ˆå·®ç•°
- ç‚ºä¿¡å¿ƒåº¦é–€æª»è¨­å®šæä¾›æ•¸æ“šä¾æ“š

ä½¿ç”¨æ–¹å¼ï¼š
  python analyze_cv_distribution.py
"""

import os
import sys
import numpy as np
import pandas as pd
from datetime import date
from pathlib import Path

# æŠ‘åˆ¶ TensorFlow è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# è¼‰å…¥æ¨¡å‹
# =============================================================================
def load_models():
    """è¼‰å…¥ T+1 å’Œ T+5 LSTM æ¨¡å‹"""
    import twii_model_registry_multivariate as lstm_1d_module
    import twii_model_registry_5d as lstm_5d_module
    
    print("=" * 60)
    print("ğŸ“¦ è¼‰å…¥ LSTM æ¨¡å‹")
    print("=" * 60)
    
    # T+1 æ¨¡å‹
    meta_1d = lstm_1d_module.select_best_model(date.today())
    if meta_1d is None:
        raise RuntimeError("æ‰¾ä¸åˆ° T+1 æ¨¡å‹")
    model_1d, scaler_feat_1d, scaler_tgt_1d, _ = lstm_1d_module.load_artifacts(
        meta_1d['train_start'], meta_1d['train_end'])
    print(f"âœ… T+1 Model: {meta_1d['train_start']} ~ {meta_1d['train_end']}")
    print(f"   Lookback: {meta_1d.get('lookback', 10)}")
    
    # T+5 æ¨¡å‹
    meta_5d = lstm_5d_module.select_best_model(date.today())
    if meta_5d is None:
        raise RuntimeError("æ‰¾ä¸åˆ° T+5 æ¨¡å‹")
    model_5d, scaler_feat_5d, scaler_tgt_5d, _ = lstm_5d_module.load_artifacts(
        meta_5d['train_start'], meta_5d['train_end'])
    print(f"âœ… T+5 Model: {meta_5d['train_start']} ~ {meta_5d['train_end']}")
    print(f"   Lookback: {meta_5d.get('lookback', 30)}")
    print(f"   Dropout Rate: {meta_5d.get('dropout_rate', 'N/A')}")
    
    return {
        '1d': {'model': model_1d, 'scaler_feat': scaler_feat_1d, 
               'scaler_tgt': scaler_tgt_1d, 'meta': meta_1d},
        '5d': {'model': model_5d, 'scaler_feat': scaler_feat_5d, 
               'scaler_tgt': scaler_tgt_5d, 'meta': meta_5d}
    }


def load_test_data():
    """è¼‰å…¥æ¸¬è©¦è³‡æ–™ï¼ˆæœ€è¿‘ 100 å¤©ï¼‰"""
    import twii_model_registry_multivariate as lstm_1d_module
    
    print("\nğŸ“Š è¼‰å…¥æ¸¬è©¦è³‡æ–™...")
    df = lstm_1d_module.load_recent_data(lookback_days=100)
    df = lstm_1d_module.add_technical_indicators(df)
    
    if 'Adj Close' not in df.columns:
        df['Adj Close'] = df['Close']
    
    print(f"âœ… æ¸¬è©¦è³‡æ–™ç­†æ•¸: {len(df)}")
    print(f"   æ—¥æœŸç¯„åœ: {df.index[0].date()} ~ {df.index[-1].date()}")
    
    return df


def run_mc_analysis(models: dict, df: pd.DataFrame, n_iter: int = 30):
    """
    åŸ·è¡Œ MC Dropout åˆ†æ
    
    å°æ¯å€‹æ¨¡å‹é€²è¡Œ n_iter æ¬¡é æ¸¬ï¼Œè¨ˆç®— CV åˆ†ä½ˆ
    """
    feature_cols = ['Adj Close', 'Volume_Log', 'K', 'D', 'MACD_Hist']
    features = df[feature_cols].values
    
    results = {}
    
    for model_name, model_data in models.items():
        model = model_data['model']
        scaler_feat = model_data['scaler_feat']
        scaler_tgt = model_data['scaler_tgt']
        lookback = model_data['meta'].get('lookback', 10 if model_name == '1d' else 30)
        
        print(f"\nğŸ”¬ åˆ†æ T+{model_name.replace('d', '')} æ¨¡å‹ (MC Dropout x{n_iter})...")
        
        # ç¸®æ”¾ç‰¹å¾µ
        scaled_features = scaler_feat.transform(features)
        
        # å»ºç«‹æ‰¹æ¬¡è¼¸å…¥
        batch = []
        for i in range(lookback, len(scaled_features)):
            batch.append(scaled_features[i - lookback:i])
        batch = np.array(batch)
        
        print(f"   æ‰¹æ¬¡å½¢ç‹€: {batch.shape}")
        
        # MC Dropout æ¡æ¨£
        mc_results = []
        for i in range(n_iter):
            # ä½¿ç”¨ training=True å•Ÿç”¨ Dropoutï¼ˆè‹¥æ¨¡å‹æœ‰ Dropout å±¤ï¼‰
            preds_scaled = model(batch, training=True).numpy()
            preds = scaler_tgt.inverse_transform(preds_scaled).flatten()
            mc_results.append(preds)
        
        mc_results = np.array(mc_results)  # shape: (n_iter, n_samples)
        
        # è¨ˆç®—çµ±è¨ˆé‡
        mc_mean = np.mean(mc_results, axis=0)
        mc_std = np.std(mc_results, axis=0)
        
        # è¨ˆç®— CV (Coefficient of Variation)
        # CV = std / meanï¼Œä½†é¿å…é™¤ä»¥é›¶
        cv = np.where(mc_mean > 0, mc_std / mc_mean, 0)
        
        results[model_name] = {
            'mc_mean': mc_mean,
            'mc_std': mc_std,
            'cv': cv,
            'n_samples': len(mc_mean)
        }
        
        # è¼¸å‡ºçµ±è¨ˆæ‘˜è¦
        print(f"\n   ğŸ“ˆ CV åˆ†ä½ˆçµ±è¨ˆ (T+{model_name.replace('d', '')}):")
        print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"   æ¨£æœ¬æ•¸: {len(cv)}")
        print(f"   CV æœ€å°å€¼: {cv.min():.6f} ({cv.min() * 100:.4f}%)")
        print(f"   CV æœ€å¤§å€¼: {cv.max():.6f} ({cv.max() * 100:.4f}%)")
        print(f"   CV å¹³å‡å€¼: {cv.mean():.6f} ({cv.mean() * 100:.4f}%)")
        print(f"   CV ä¸­ä½æ•¸: {np.median(cv):.6f} ({np.median(cv) * 100:.4f}%)")
        print(f"   CV æ¨™æº–å·®: {cv.std():.6f}")
        
        # ç™¾åˆ†ä½æ•¸åˆ†ä½ˆ
        percentiles = [10, 25, 50, 75, 90, 95, 99]
        print(f"\n   ğŸ“Š CV ç™¾åˆ†ä½æ•¸:")
        for p in percentiles:
            val = np.percentile(cv, p)
            print(f"      P{p:02d}: {val:.6f} ({val * 100:.4f}%)")
    
    return results


def suggest_thresholds(results: dict):
    """æ ¹æ“š CV åˆ†ä½ˆå»ºè­°ä¿¡å¿ƒåº¦é–€æª»"""
    print("\n" + "=" * 60)
    print("ğŸ’¡ ä¿¡å¿ƒåº¦é–€æª»å»ºè­°")
    print("=" * 60)
    
    for model_name, data in results.items():
        cv = data['cv']
        
        # å¦‚æœ CV å…¨éƒ¨ç‚º 0ï¼Œè¡¨ç¤ºæ¨¡å‹æ²’æœ‰ Dropout
        if cv.max() < 1e-10:
            print(f"\nâš ï¸  T+{model_name.replace('d', '')} æ¨¡å‹:")
            print(f"   CV å…¨éƒ¨ç‚º 0ï¼é€™è¡¨ç¤ºæ¨¡å‹ **æ²’æœ‰ Dropout å±¤**ã€‚")
            print(f"   å»ºè­°ï¼šä¿®æ”¹æ¨¡å‹æ¶æ§‹åŠ å…¥ Dropout å¾Œé‡æ–°è¨“ç·´ã€‚")
            continue
        
        # ä½¿ç”¨ç™¾åˆ†ä½æ•¸è¨­å®šé–€æª»
        p10 = np.percentile(cv, 10)  # é«˜ä¿¡å¿ƒé–€æª»
        p90 = np.percentile(cv, 90)  # ä½ä¿¡å¿ƒé–€æª»
        
        print(f"\nğŸ“ T+{model_name.replace('d', '')} æ¨¡å‹å»ºè­°é–€æª»:")
        print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"   threshold_high (é«˜ä¿¡å¿ƒ): {p10:.6f} (P10)")
        print(f"   threshold_low (ä½ä¿¡å¿ƒ):  {p90:.6f} (P90)")
        print(f"\n   ä¿¡å¿ƒåº¦å…¬å¼:")
        print(f"   score = 1.0 - (cv - {p10:.6f}) / ({p90:.6f} - {p10:.6f})")
        print(f"   confidence = clip(score, 0.0, 1.0)")
        
        # èˆ‡ T+5 ç›®å‰è¨­å®šæ¯”è¼ƒï¼ˆè‹¥ç‚º 5d æ¨¡å‹ï¼‰
        if model_name == '5d':
            print(f"\n   ğŸ“Œ ç›®å‰ T+5 é–€æª»è¨­å®š:")
            print(f"      threshold_high = 0.001 (0.1%)")
            print(f"      threshold_low  = 0.010 (1.0%)")


def main():
    print("\n" + "=" * 60)
    print("ğŸ” MC Dropout CV åˆ†ä½ˆåˆ†æ")
    print("=" * 60)
    
    # 1. è¼‰å…¥æ¨¡å‹
    models = load_models()
    
    # 2. è¼‰å…¥æ¸¬è©¦è³‡æ–™
    df = load_test_data()
    
    # 3. åŸ·è¡Œ MC åˆ†æ
    results = run_mc_analysis(models, df, n_iter=30)
    
    # 4. å»ºè­°é–€æª»
    suggest_thresholds(results)
    
    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
