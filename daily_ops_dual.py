# -*- coding: utf-8 -*-
"""
================================================================================
Daily Operations with Dual Strategy & Versioning (v2.1 - Patched)
================================================================================
æ¯æ—¥ç¶­é‹è…³æœ¬ - é›™ç­–ç•¥æ¨è«–èˆ‡ç‰ˆæœ¬æ§ç®¡

ä¿®æ­£ç´€éŒ„ (v2.2):
1. [Fix] Step 1 æ”¹ç‚ºç›´æ¥å‘¼å« model registry è…³æœ¬ï¼Œä¸¦å‚³å…¥å‹•æ…‹æ—¥æœŸ (ç¢ºä¿æ¨¡å‹æ›´æ–°è‡³ä»Šæ—¥)
2. [Fix] Step 2 è£œä¸Š target_scaler çš„è¼‰å…¥èˆ‡æ³¨å…¥ (é˜²æ­¢ inverse_transform å¤±æ•—)
3. [Safety] å¢åŠ  import æª¢æŸ¥èˆ‡éŒ¯èª¤è™•ç†
4. [Fix] yfinance end_date åŠ ä¸€å¤© (å› ç‚º yf.download çš„ end æ˜¯ exclusive)
5. [Fix] ä½¿ç”¨å¯¦éš›ä¸‹è¼‰è³‡æ–™çš„æœ€å¾Œæ—¥æœŸä½œç‚ºå·¥ä½œå€æ—¥æœŸ (é¿å…é€±æœ«/ç›¤ä¸­åŸ·è¡Œæ™‚æ—¥æœŸä¸ç¬¦)
6. [Safety] meta.json è¼‰å…¥åŠ ä¸Š try-except é˜²è­·

ä½œè€…ï¼šPhil Liang (Fixed by Gemini)
æ—¥æœŸï¼š2025-12-07 (v2.2 Updated)
================================================================================
"""

import os
import sys
import shutil
import pickle
import subprocess
import json
import glob
from datetime import datetime, timedelta

# è¨­å®š UTF-8 è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import yfinance as yf
from tensorflow import keras
from keras import layers

# =============================================================================
# å¼•ç”¨ä¸»ç³»çµ±
# =============================================================================
import ptrl_hybrid_system as core_system

# =============================================================================
# è¨­å®šè·¯å¾‘
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DAILY_RUNS_PATH = os.path.join(PROJECT_PATH, 'daily_runs')

# RL æ¨¡å‹è·¯å¾‘ (V3 vs V4)
STRATEGY_A_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v3')  # V3 (è¼•é‡åŒ–å¾®èª¿)
STRATEGY_B_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v4')  # V4 (æ¨™æº–å®Œæ•´å¾®èª¿)

# LSTM è¨“ç·´è…³æœ¬åç¨± (å¿…é ˆå­˜åœ¨æ–¼åŒä¸€ç›®éŒ„ä¸‹)
SCRIPT_5D = "twii_model_registry_5d.py"
SCRIPT_1D = "twii_model_registry_multivariate.py"

# LSTM æ¨¡å‹é è¨­è¼¸å‡ºè·¯å¾‘ (è¨“ç·´è…³æœ¬é è¨­æœƒå­˜åˆ°é€™è£¡)
DEFAULT_LSTM_5D_DIR = os.path.join(PROJECT_PATH, 'saved_models_5d')
DEFAULT_LSTM_1D_DIR = os.path.join(PROJECT_PATH, 'saved_models_multivariate')


# =============================================================================
# Step 0: å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€
# =============================================================================
def create_daily_workspace(date_str: str) -> dict:
    daily_path = os.path.join(DAILY_RUNS_PATH, date_str)
    paths = {
        'root': daily_path,
        'lstm_models': os.path.join(daily_path, 'lstm_models'),
        'lstm_5d': os.path.join(daily_path, 'lstm_models', 'saved_models_5d'),
        'lstm_1d': os.path.join(daily_path, 'lstm_models', 'saved_models_multivariate'),
        'cache': os.path.join(daily_path, 'cache'),
        'reports': os.path.join(daily_path, 'reports'),
    }
    for key, path in paths.items():
        os.makedirs(path, exist_ok=True)
    print(f"[Workspace] å»ºç«‹ç•¶æ—¥å·¥ä½œå€: {daily_path}")
    return paths


# =============================================================================
# Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜ (v2.3 - å‹•æ…‹å¤©æ•¸ + å…¨é‡å­¸ç¿’)
# =============================================================================
def train_and_archive_lstm(workspace: dict, end_date: str):
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜")
    print("=" * 60)
    
    # [v2.3] å‹•æ…‹è¨ˆç®—èµ·å§‹æ—¥æœŸ
    # T+5 æ¨¡å‹ï¼šä½¿ç”¨éå» 2200 å¤©ï¼ˆç´„ 2020-01 èµ·ï¼‰- æ•æ‰æ›´é•·è¶¨å‹¢
    # T+1 æ¨¡å‹ï¼šä½¿ç”¨éå» 2000 å¤©ï¼ˆç´„ 2020-07 èµ·ï¼‰- å°ˆæ³¨è¿‘æœŸå¸‚å ´
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    start_5d = (end_dt - timedelta(days=2200)).strftime('%Y-%m-%d')
    start_1d = (end_dt - timedelta(days=2000)).strftime('%Y-%m-%d')
    
    # [v2.3] å•Ÿç”¨å…¨é‡å­¸ç¿’ï¼šsplit_ratio = 0.99
    # åªä¿ç•™ 1% ä½œç‚º Early Stopping ç›£æ§ï¼Œç¢ºä¿æ¨¡å‹å­¸ç¿’åˆ°æœ€æ–°è³‡æ–™
    split_ratio = "0.99"
    
    # 1. åŸ·è¡Œ T+5 è¨“ç·´ (å‚³å…¥å‹•æ…‹æ—¥æœŸ + split_ratio)
    print(f"\n[Training] T+5 Model ({start_5d} ~ {end_date}, split={split_ratio})...")
    script_5d_path = os.path.join(PROJECT_PATH, SCRIPT_5D)
    cmd_5d = [sys.executable, script_5d_path, "train", "--start", start_5d, "--end", end_date, "--split_ratio", split_ratio]
    try:
        subprocess.run(cmd_5d, check=True, timeout=1200, cwd=PROJECT_PATH)  # ç¢ºä¿å·¥ä½œç›®éŒ„æ­£ç¢º
        print("[Training] âœ… T+5 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+5 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_5d_path}")
        return False
    except Exception as e:
        print(f"[Error] åŸ·è¡ŒéŒ¯èª¤: {e}")
        return False

    # 2. åŸ·è¡Œ T+1 è¨“ç·´ (å‚³å…¥å‹•æ…‹æ—¥æœŸ + split_ratio)
    print(f"\n[Training] T+1 Model ({start_1d} ~ {end_date}, split={split_ratio})...")
    script_1d_path = os.path.join(PROJECT_PATH, SCRIPT_1D)
    cmd_1d = [sys.executable, script_1d_path, "train", "--start", start_1d, "--end", end_date, "--split_ratio", split_ratio]
    try:
        subprocess.run(cmd_1d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+1 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+1 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_1d_path}")
        return False

    # 3. å°å­˜æ¨¡å‹ (Copy from default dir to daily dir)
    print("\n[Archive] å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€...")
    
    def archive_dir(src_dir, dest_dir):
        if os.path.exists(src_dir):
            if os.path.exists(dest_dir):
                shutil.rmtree(dest_dir) # æ¸…ç©ºèˆŠçš„
            shutil.copytree(src_dir, dest_dir)
            print(f"  âœ… å·²å°å­˜: {os.path.basename(src_dir)} -> {dest_dir}")
        else:
            print(f"  âš ï¸ ä¾†æºç›®éŒ„ä¸å­˜åœ¨: {src_dir}")

    archive_dir(DEFAULT_LSTM_5D_DIR, workspace['lstm_5d'])
    archive_dir(DEFAULT_LSTM_1D_DIR, workspace['lstm_1d'])
    
    return True


# =============================================================================
# Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (ä¿®æ­£ç‰ˆ)
# =============================================================================
def isolated_feature_engineering(workspace: dict, end_date: str) -> pd.DataFrame:
    print("\n" + "=" * 60)
    print("ğŸ”§ Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)")
    print("=" * 60)
    
    # [ä¿®æ­£] ç›´æ¥å¾åŸå§‹è¨“ç·´è…³æœ¬å¼•ç”¨æ­£ç¢ºçš„ Layer å®šç¾©
    # é€™æ¨£ç¢ºä¿æ•¸å­¸é‹ç®—é‚è¼¯ (Attention Score è¨ˆç®—) èˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´
    try:
        from twii_model_registry_5d import SelfAttention
        print("[System] æˆåŠŸå¼•ç”¨åŸå§‹ SelfAttention é¡åˆ¥")
    except ImportError:
        print("[Error] ç„¡æ³•å¼•ç”¨ twii_model_registry_5dï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨")
        sys.exit(1)

    # è¼”åŠ©å‡½å¼ï¼šè¼‰å…¥æ•´çµ„æ¨¡å‹å…ƒä»¶
    def load_model_components(model_dir):
        keras_files = glob.glob(os.path.join(model_dir, "*.keras"))
        if not keras_files: return None, None, None, None
        
        # æ‰¾æœ€æ–°çš„æ¨¡å‹æª”
        latest_keras = sorted(keras_files)[-1]
        print(f"  ...Loading {os.path.basename(latest_keras)}")
        
        # [ä¿®æ­£] è¼‰å…¥æ¨¡å‹æ™‚ä½¿ç”¨æ­£ç¢ºçš„ Custom Object
        model = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})

        # è¼‰å…¥ Meta (åŠ ä¸ŠéŒ¯èª¤é˜²è­·)
        meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
        meta = {}
        if os.path.exists(meta_file):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
            except Exception as e:
                print(f"  âš ï¸ è¼‰å…¥ meta å¤±æ•—: {e}")

        # è¼‰å…¥ Feature Scaler
        scaler_feat_file = latest_keras.replace('model_', 'feature_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_feat_file):
             scaler_feat_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
        
        scaler_feat = None
        if os.path.exists(scaler_feat_file):
            with open(scaler_feat_file, 'rb') as f:
                scaler_feat = pickle.load(f)

        # è¼‰å…¥ Target Scaler
        scaler_tgt_file = latest_keras.replace('model_', 'target_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_tgt_file):
             scaler_tgt = scaler_feat
        else:
             with open(scaler_tgt_file, 'rb') as f:
                 scaler_tgt = pickle.load(f)

        return model, scaler_feat, scaler_tgt, meta

    # 1. è¼‰å…¥æ¨¡å‹
    print("\n[Model Injection] è¼‰å…¥ç•¶æ—¥å°å­˜çš„ LSTM æ¨¡å‹...")
    m5d, sf5d, st5d, meta5d = load_model_components(workspace['lstm_5d'])
    m1d, sf1d, st1d, meta1d = load_model_components(workspace['lstm_1d'])
    
    if m5d is None or m1d is None:
        print("[Error] æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œç‰¹å¾µå·¥ç¨‹")
        sys.exit(1)

    # 2. æ³¨å…¥ä¸»ç³»çµ±
    print("\n[Model Injection] æ³¨å…¥ core_system._LSTM_MODELS...")
    if not hasattr(core_system, '_LSTM_MODELS'):
        core_system._LSTM_MODELS = {}
    
    core_system._LSTM_MODELS.update({
        'model_5d': m5d, 'scaler_feat_5d': sf5d, 'scaler_tgt_5d': st5d, 'meta_5d': meta5d,
        'model_1d': m1d, 'scaler_feat_1d': sf1d, 'scaler_tgt_1d': st1d, 'meta_1d': meta1d,
        'loaded': True
    })
    print("  âœ… æ³¨å…¥å®Œæˆ (å« Target Scalers)")

    # 3. ä¸‹è¼‰æ•¸æ“š & è¨ˆç®—ç‰¹å¾µ
    # [ä¿®æ­£] yfinance çš„ end åƒæ•¸æ˜¯ exclusiveï¼Œéœ€è¦åŠ ä¸€å¤©æ‰èƒ½åŒ…å«ç•¶æ—¥
    end_dt = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1)
    download_end = end_dt.strftime('%Y-%m-%d')
    print(f"\n[Compute] Loading Data (2020-01-01 ~ {end_date})...")
    # [Modify] ä½¿ç”¨æœ¬åœ°è³‡æ–™è¼‰å…¥å‡½æ•¸
    raw_df = core_system._load_local_twii_data(start_date="2020-01-01")
    
    # ç¯©é¸æ—¥æœŸç¯„åœ (ç¢ºä¿ä¸è¶…é end_date)
    end_dt_ts = pd.Timestamp(end_date)
    raw_df = raw_df[raw_df.index <= end_dt_ts]
    
    # [ä¿®æ­£] å–å¾—å¯¦éš›ä¸‹è¼‰è³‡æ–™çš„æœ€å¾Œæ—¥æœŸ (é¿å…é€±æœ«/ç›¤ä¸­åŸ·è¡Œæ™‚æ—¥æœŸä¸ç¬¦)
    actual_last_date = raw_df.index[-1].strftime('%Y-%m-%d')
    print(f"[Data] å¯¦éš›è³‡æ–™æœ€å¾Œæ—¥æœŸ: {actual_last_date}")

    # [Note] Local CSV æˆäº¤é‡å·²æ˜¯æ­£ç¢ºå–®ä½ (å„„å…ƒ) ä¸”å·²è£œå€¼ï¼Œç„¡éœ€é¡å¤–è™•ç†
    
    # [v2.6] åŒ¯å‡ºåŸå§‹æ•¸æ“š CSV (ä¿®è£œå¾Œ)
    raw_csv_path = os.path.join(workspace['cache'], 'raw_data.csv')
    raw_df.to_csv(raw_csv_path)
    print(f"[Export] åŸå§‹æ•¸æ“šå·²å­˜æª”: {raw_csv_path}")
    
    print(f"[Compute] è¨ˆç®—ç‰¹å¾µä¸­ (ä½¿ç”¨ç•¶æ—¥æ¨¡å‹)...")
    # å¼·åˆ¶ä¸ä½¿ç”¨å¿«å–ï¼Œç¢ºä¿é‡æ–°è¨ˆç®—
    df = core_system.calculate_features(raw_df, raw_df, ticker="^TWII", use_cache=False)
    
    # [v2.6] åŒ¯å‡ºè™•ç†å¾Œç‰¹å¾µæ•¸æ“š CSV
    features_csv_path = os.path.join(workspace['cache'], 'processed_features.csv')
    df.to_csv(features_csv_path)
    print(f"[Export] ç‰¹å¾µæ•¸æ“šå·²å­˜æª”: {features_csv_path}")
    
    # å­˜å…¥ç•¶æ—¥å¿«å– (pkl æ ¼å¼ï¼Œä¾›å¾ŒçºŒè¼‰å…¥ä½¿ç”¨)
    cache_file = os.path.join(workspace['cache'], 'twii_features.pkl')
    with open(cache_file, 'wb') as f:
        pickle.dump(df, f)
    print(f"[Cache] ç‰¹å¾µå·²å­˜æª”: {cache_file}")
    
    return df, actual_last_date  # [ä¿®æ­£] å›å‚³å¯¦éš›æ—¥æœŸä¾›å ±å‘Šä½¿ç”¨


# =============================================================================
# Step 3: é›™æ¨¡å‹æ¨è«– (v2.4 - æ¿¾ç¶² + æƒ…å¢ƒåˆ†æ)
# =============================================================================
def dual_inference(workspace: dict, df: pd.DataFrame) -> dict:
    print("\n" + "=" * 60)
    print("ğŸ¯ Step 3: é›™æ¨¡å‹æ¨è«– (å«æ¿¾ç¶²èˆ‡æƒ…å¢ƒåˆ†æ)")
    print("=" * 60)
    
    from stable_baselines3 import PPO
    
    # æº–å‚™ç‰¹å¾µ
    FEATURE_COLS = core_system.FEATURE_COLS
    latest = df.iloc[-1]
    
    # [v2.4] ç²å–æ¿¾ç¶²ç‹€æ…‹
    signal_buy_filter = bool(latest.get('Signal_Buy_Filter', False))
    print(f"  [æ¿¾ç¶²] Signal_Buy_Filter = {signal_buy_filter}")
    
    # ç¢ºä¿ç‰¹å¾µæ¬„ä½å°é½Š
    features = []
    for col in FEATURE_COLS:
        val = latest.get(col, 0.0)
        features.append(val)
    features = np.array(features, dtype=np.float32).reshape(1, -1)
    
    # è™•ç† NaN/Inf
    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
    
    results = {'filter_status': signal_buy_filter}
    
    # [v2.4] ä¸‰ç¨®æŒå€‰æƒ…å¢ƒ
    SELL_SCENARIOS = {
        'cost': 1.00,    # æˆæœ¬å€ (å‰›é€²å ´)
        'profit': 1.10,  # ç²åˆ©ä¸­ (+10%)
        'loss': 0.95,    # è™§æä¸­ (-5%)
    }
    
    def run_strategy(name, path, key):
        buy_path = os.path.join(path, 'ppo_buy_twii_final.zip')
        sell_path = os.path.join(path, 'ppo_sell_twii_final.zip')
        
        if not os.path.exists(buy_path):
            results[key] = {'error': 'Model not found'}
            print(f"  [Warning] {name}: æ¨¡å‹ä¸å­˜åœ¨")
            return

        try:
            buy_agent = PPO.load(buy_path)
            sell_agent = PPO.load(sell_path)
            
            # =====================================================================
            # Buy Logic (v2.5 - å…¨æ™‚æ¨è«– + ç‹€æ…‹æ¨™è¨˜)
            # =====================================================================
            # æ­¥é©Ÿ A: ç„¡è«–æ¿¾ç¶²ç‹€æ…‹ï¼Œä¸€å¾‹åŸ·è¡Œ AI é æ¸¬
            b_act, _ = buy_agent.predict(features, deterministic=True)
            b_obs = buy_agent.policy.obs_to_tensor(features)[0]
            b_prob = buy_agent.policy.get_distribution(b_obs).distribution.probs.detach().cpu().numpy()[0]
            
            ai_action = 'BUY' if b_act[0] == 1 else 'WAIT'
            buy_prob = float(b_prob[1]) if b_act[0] == 1 else float(b_prob[0])
            
            # æ­¥é©Ÿ B: æ ¹æ“šæ¿¾ç¶²ç‹€æ…‹æ±ºå®šæœ€çµ‚é¡¯ç¤ºå­—ä¸²
            if signal_buy_filter:
                # æ¿¾ç¶²é€šé
                buy_signal = ai_action  # "BUY" æˆ– "WAIT"
            else:
                # æ¿¾ç¶²æœªéï¼šæ¨™è¨˜ç‚º FILTERED ä½†é¡¯ç¤º AI åŸå§‹åˆ¤æ–·
                buy_signal = f"FILTERED (AI: {ai_action})"
            
            print(f"  [{name}] Buy: {buy_signal} ({buy_prob:.1%})")
            
            # =====================================================================
            # Sell Logic (æƒ…å¢ƒåˆ†æ) - ä¿æŒä¸è®Š
            # =====================================================================
            sell_scenarios = {}
            for scenario_name, return_value in SELL_SCENARIOS.items():
                s_feat = np.concatenate([features[0], [return_value]]).reshape(1, -1)
                s_act, _ = sell_agent.predict(s_feat, deterministic=True)
                sell_scenarios[scenario_name] = 'SELL' if s_act[0] == 1 else 'HOLD'
            
            print(f"  [{name}] Sell: æˆæœ¬={sell_scenarios['cost']} | ç²åˆ©={sell_scenarios['profit']} | è™§æ={sell_scenarios['loss']}")
            
            results[key] = {
                'name': name,
                'buy_signal': buy_signal,
                'buy_prob': buy_prob,
                'ai_action': ai_action,  # æ–°å¢ï¼šAI åŸå§‹åˆ¤æ–·
                'sell_scenarios': sell_scenarios,
            }
            
        except Exception as e:
            results[key] = {'error': str(e)}
            print(f"  [Error] {name}: {e}")
            import traceback
            traceback.print_exc()

    # åŸ·è¡Œ A (V3 - è¼•é‡åŒ–)
    run_strategy("V3 (Lightweight 200K)", STRATEGY_A_PATH, 'A')
    
    # åŸ·è¡Œ B (V4 - æ¨™æº–)
    run_strategy("V4 (Standard 1M)", STRATEGY_B_PATH, 'B')
    
    return results


# =============================================================================
# Step 4: è¼¸å‡ºå ±å‘Š (v2.4 - æ¿¾ç¶² + æƒ…å¢ƒåˆ†æ)
# =============================================================================
def generate_report(workspace: dict, df: pd.DataFrame, res: dict, date_str: str):
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: æˆ°æƒ…å„€è¡¨æ¿ (v2.4)")
    print("=" * 60)
    
    last = df.iloc[-1]
    filter_status = res.get('filter_status', False)
    
    lines = []
    lines.append("=" * 50)
    lines.append(f"ğŸ“… æ—¥æœŸ: {date_str}")
    lines.append("=" * 50)
    lines.append(f"ğŸ“Š æ”¶ç›¤: {last['Close']:.2f} | é‡: {last['Volume']:.2f} å„„å…ƒ")
    lines.append("-" * 50)
    
    # æ¿¾ç¶²ç‹€æ…‹
    filter_icon = "âœ…" if filter_status else "ğŸš«"
    filter_text = "é€šé (Donchian çªç ´)" if filter_status else "æœªé€šé (éçªç ´æ—¥)"
    lines.append(f"ï¿½ [æ¿¾ç¶²ç‹€æ…‹] {filter_icon} {filter_text}")
    lines.append("-" * 50)
    
    # LSTM
    lines.append("ğŸ”® [åˆ†æå¸« LSTM]")
    lines.append(f"   T+1 æ¼²è·Œ: {last.get('LSTM_Pred_1d', 0)*100:+.2f}% (ä¿¡å¿ƒåº¦: {last.get('LSTM_Conf_1d', 0)*100:.1f}%)")
    lines.append(f"   T+5 æ¼²è·Œ: {last.get('LSTM_Pred_5d', 0)*100:+.2f}% (ä¿¡å¿ƒåº¦: {last.get('LSTM_Conf_5d', 0)*100:.1f}%)")
    lines.append("-" * 50)
    
    # RL ç­–ç•¥ (å«æƒ…å¢ƒåˆ†æ)
    lines.append("ğŸ¤– [æ“ç›¤æ‰‹ RL]")
    
    def format_strategy(key, label):
        if key not in res or 'error' in res[key]:
            return [f"   {label}: âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—"]
        
        r = res[key]
        result_lines = []
        
        # Buy (v2.5 å…¨æ™‚æ¨è«–æ ¼å¼)
        buy_signal = r['buy_signal']
        buy_prob = r['buy_prob']
        
        if buy_signal == 'BUY':
            buy_icon = "ğŸš€"
        elif buy_signal == 'WAIT':
            buy_icon = "ğŸ’¤"
        elif 'FILTERED' in buy_signal:
            buy_icon = "ğŸš«"
        else:
            buy_icon = "â“"
        
        result_lines.append(f"   ğŸ›’ {label} è²·å…¥: {buy_icon} {buy_signal} ({buy_prob:.1%})")
        
        # Sell (æƒ…å¢ƒçŸ©é™£)
        ss = r.get('sell_scenarios', {})
        result_lines.append(f"   ğŸ“¦ {label} è³£å‡º:")
        result_lines.append(f"      â”œâ”€ æˆæœ¬å€ (0%):  {ss.get('cost', 'N/A')}")
        result_lines.append(f"      â”œâ”€ ç²åˆ©ä¸­ (+10%): {ss.get('profit', 'N/A')}")
        result_lines.append(f"      â””â”€ è™§æä¸­ (-5%):  {ss.get('loss', 'N/A')}")
        
        return result_lines
    
    lines.extend(format_strategy('A', 'V3'))
    lines.append("")
    lines.extend(format_strategy('B', 'V4'))
    lines.append("-" * 50)
    
    # ç¶œåˆå»ºè­° (ä½¿ç”¨ ai_action è€Œé buy_signal åˆ¤æ–· AI æ„åœ–)
    ai_a = res.get('A', {}).get('ai_action', 'N/A')
    ai_b = res.get('B', {}).get('ai_action', 'N/A')
    
    if not filter_status:
        # æ¿¾ç¶²æœªéï¼Œä½†é¡¯ç¤º AI æƒ³æ³•
        if ai_a == 'BUY' and ai_b == 'BUY':
            advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: é›™è²·é€² (è¢«æ“‹ä¸‹)"
        elif ai_a == 'BUY' or ai_b == 'BUY':
            advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: æœ‰æ„è²·é€² (è¢«æ“‹ä¸‹)"
        else:
            advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: è§€æœ›"
    elif ai_a == 'BUY' and ai_b == 'BUY':
        advice = "â­â­ V3+V4 é›™è²·é€² (Strong Buy) â­â­"
    elif ai_a == 'WAIT' and ai_b == 'WAIT':
        advice = "ğŸ’¤ ç©ºæ‰‹è§€æœ› (Wait)"
    elif ai_a == 'BUY':
        advice = "âš ï¸ åƒ… V3 è²·é€² (V3 Only)"
    elif ai_b == 'BUY':
        advice = "âš ï¸ åƒ… V4 è²·é€² (V4 Only)"
    else:
        advice = "â“ è¨Šè™Ÿä¸æ˜"
        
    lines.append(f"ğŸ’¡ ç¶œåˆå»ºè­°: {advice}")
    lines.append("=" * 50)
    
    report = "\n".join(lines)
    print(report)
    
    # å­˜æª” TXT
    txt_path = os.path.join(workspace['reports'], 'summary.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # å­˜æª” JSON
    json_path = os.path.join(workspace['reports'], 'summary.json')
    json_data = {
        'date': date_str,
        'generated_at': datetime.now().isoformat(),
        'filter_status': filter_status,
        'market': {
            'close': float(last.get('Close', 0)),
            'volume': float(last.get('Volume', 0)),
        },
        'lstm': {
            'pred_1d': float(last.get('LSTM_Pred_1d', 0)),
            'conf_1d': float(last.get('LSTM_Conf_1d', 0)),
            'pred_5d': float(last.get('LSTM_Pred_5d', 0)),
            'conf_5d': float(last.get('LSTM_Conf_5d', 0)),
        },
        'strategies': {
            'A': res.get('A', {}),
            'B': res.get('B', {}),
        },
        'advice': advice,
    }
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n[Report] å·²å„²å­˜: {txt_path}")
    print(f"[Report] å·²å„²å­˜: {json_path}")


# =============================================================================
# Main
# =============================================================================
def main():
    today = datetime.now()
    # è™•ç†é€±æœ« (å¾€å‰æ¨åˆ°é€±äº”) - ç”¨æ–¼åˆæ­¥ä¼°è¨ˆæ—¥æœŸ
    if today.weekday() == 5: today -= timedelta(days=1)
    elif today.weekday() == 6: today -= timedelta(days=2)
    
    date_str = today.strftime('%Y-%m-%d')
    print(f"ğŸš€ å•Ÿå‹•æ¯æ—¥ç¶­é‹ç³»çµ± - {date_str}")
    
    # Step 0
    ws = create_daily_workspace(date_str)
    
    # Step 1 (Train up to Today)
    train_and_archive_lstm(ws, date_str)
    
    # Step 2 - [ä¿®æ­£] æ¥æ”¶å¯¦éš›è³‡æ–™æ—¥æœŸ
    df, actual_date = isolated_feature_engineering(ws, date_str)
    
    # [ä¿®æ­£] å¦‚æœå¯¦éš›æ—¥æœŸèˆ‡é ä¼°æ—¥æœŸä¸åŒï¼Œé¡¯ç¤ºè­¦å‘Š
    if actual_date != date_str:
        print(f"[Warning] é ä¼°æ—¥æœŸ {date_str} èˆ‡å¯¦éš›è³‡æ–™æ—¥æœŸ {actual_date} ä¸åŒ")
        print(f"[Info] å ±å‘Šå°‡ä½¿ç”¨å¯¦éš›è³‡æ–™æ—¥æœŸ: {actual_date}")
    
    # Step 3
    res = dual_inference(ws, df)
    
    # Step 4 - [ä¿®æ­£] ä½¿ç”¨å¯¦éš›è³‡æ–™æ—¥æœŸç”Ÿæˆå ±å‘Š
    generate_report(ws, df, res, actual_date)

if __name__ == "__main__":
    main()