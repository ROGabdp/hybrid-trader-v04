# -*- coding: utf-8 -*-
"""
================================================================================
Daily Operations - Intraday Version (ç›¤ä¸­å®Œæ•´è¨“ç·´+é æ¸¬ç‰ˆ)
================================================================================
ç›¤ä¸­åŸ·è¡Œè…³æœ¬ - å®Œæ•´ç¨ç«‹é‹ä½œ

æµç¨‹:
1. å¾ yfinance ä¸‹è¼‰ç•¶æ—¥ OHLC è³‡æ–™ (ç›¤ä¸­å³æ™‚)
2. æˆäº¤é‡ä½¿ç”¨ CSV æ­·å²è³‡æ–™çš„å‰ 5 æ—¥å¹³å‡ (å› ç›¤ä¸­ç„¡æ³•å–å¾—æº–ç¢ºæˆäº¤é‡)
3. ä½¿ç”¨ä¸Šè¿°è³‡æ–™è¨“ç·´ LSTM æ¨¡å‹ (T+5 åŠ T+1)
4. ä½¿ç”¨æ–°è¨“ç·´çš„ LSTM é€²è¡Œç‰¹å¾µå·¥ç¨‹èˆ‡é æ¸¬
5. è¼¸å‡ºçµæœåˆ° intraday_runs/{date}_{time}/ (ç¨ç«‹è³‡æ–™å¤¾ï¼Œä¸å½±éŸ¿ daily_runs)
6. ä¸å¯«å…¥ twii_data_from_2000_01_01.csv

ä½œè€…ï¼šPhil Liang
æ—¥æœŸï¼š2025-12-11
================================================================================
"""

import os
import sys
import shutil
import pickle
import subprocess
import json
import glob
from datetime import datetime, timedelta

# è¨­å®š UTF-8 è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import yfinance as yf
from tensorflow import keras
from keras import layers

# =============================================================================
# å¼•ç”¨ä¸»ç³»çµ±
# =============================================================================
import ptrl_hybrid_system as core_system

# =============================================================================
# è¨­å®šè·¯å¾‘
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
INTRADAY_RUNS_PATH = os.path.join(PROJECT_PATH, 'intraday_runs')  # ç¨ç«‹è³‡æ–™å¤¾
CSV_FILE = os.path.join(PROJECT_PATH, 'twii_data_from_2000_01_01.csv')

# RL æ¨¡å‹è·¯å¾‘ (V3 vs V4)
STRATEGY_A_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v3')  # V3 (è¼•é‡åŒ–å¾®èª¿)
STRATEGY_B_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v4')  # V4 (æ¨™æº–å®Œæ•´å¾®èª¿)

# LSTM è¨“ç·´è…³æœ¬åç¨±
SCRIPT_5D = "twii_model_registry_5d.py"
SCRIPT_1D = "twii_model_registry_multivariate.py"

# LSTM æ¨¡å‹é è¨­è¼¸å‡ºè·¯å¾‘
DEFAULT_LSTM_5D_DIR = os.path.join(PROJECT_PATH, 'saved_models_5d')
DEFAULT_LSTM_1D_DIR = os.path.join(PROJECT_PATH, 'saved_models_multivariate')


# =============================================================================
# Step 0: å»ºç«‹ç›¤ä¸­å°ˆå±¬å·¥ä½œå€ (ä½¿ç”¨ intraday_runs)
# =============================================================================
def create_intraday_workspace(date_str: str, time_str: str) -> dict:
    folder_name = f"{date_str}_{time_str}"
    intraday_path = os.path.join(INTRADAY_RUNS_PATH, folder_name)
    paths = {
        'root': intraday_path,
        'lstm_models': os.path.join(intraday_path, 'lstm_models'),
        'lstm_5d': os.path.join(intraday_path, 'lstm_models', 'saved_models_5d'),
        'lstm_1d': os.path.join(intraday_path, 'lstm_models', 'saved_models_multivariate'),
        'cache': os.path.join(intraday_path, 'cache'),
        'reports': os.path.join(intraday_path, 'reports'),
    }
    for key, path in paths.items():
        os.makedirs(path, exist_ok=True)
    print(f"[Workspace] å»ºç«‹ç›¤ä¸­å·¥ä½œå€: {intraday_path}")
    return paths


# =============================================================================
# è¼”åŠ©å‡½å¼: å–å¾—ç›¤ä¸­ OHLC (from è­‰äº¤æ‰€å³æ™‚ API)
# =============================================================================
def fetch_intraday_ohlc(ticker: str = "^TWII") -> tuple:
    """
    å¾è­‰äº¤æ‰€ç›¤ä¸­å³æ™‚ API ä¸‹è¼‰ç•¶æ—¥ OHLC è³‡æ–™
    API: https://mis.twse.com.tw/stock/api/getStockInfo.jsp
    
    Returns:
        tuple: (date_str, open, high, low, close) or None if failed
    """
    import requests
    
    print(f"\n[Download] æ­£åœ¨å¾è­‰äº¤æ‰€ç›¤ä¸­ API ä¸‹è¼‰å³æ™‚è³‡æ–™...")
    
    # è­‰äº¤æ‰€ç›¤ä¸­å³æ™‚å ±åƒ¹ API
    # tse_t00.tw = ç™¼è¡Œé‡åŠ æ¬Šè‚¡åƒ¹æŒ‡æ•¸
    url = "https://mis.twse.com.tw/stock/api/getStockInfo.jsp?ex_ch=tse_t00.tw"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/json',
        'Referer': 'https://mis.twse.com.tw/stock/index.jsp'
    }
    
    try:
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()
        
        data = r.json()
        msg_array = data.get('msgArray', [])
        
        if not msg_array:
            print("[Error] è­‰äº¤æ‰€ API ç„¡è³‡æ–™ (å¯èƒ½éäº¤æ˜“æ™‚æ®µ)")
            return None
        
        item = msg_array[0]
        
        # è§£ææ—¥æœŸ (æ ¼å¼: 20251212 -> 2025-12-12)
        raw_date = item.get('d', '')
        if len(raw_date) == 8:
            date_str = f"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:8]}"
        else:
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        # è§£æ OHLC
        o = float(item.get('o', 0))
        h = float(item.get('h', 0))
        l = float(item.get('l', 0))
        z = float(item.get('z', 0))  # z = å³æ™‚æˆäº¤åƒ¹ (ç•¶ä½œ Close)
        
        # å–å¾—æ™‚é–“
        time_str = item.get('t', 'N/A')
        
        print(f"  ğŸ“… æ—¥æœŸ: {date_str}")
        print(f"  â° æ™‚é–“: {time_str}")
        print(f"  ğŸ“ˆ Open: {o:.2f}")
        print(f"  ğŸ“Š High: {h:.2f}")
        print(f"  ğŸ“‰ Low: {l:.2f}")
        print(f"  ğŸ’° å³æ™‚åƒ¹: {z:.2f}")
        
        return (date_str, o, h, l, z)
        
    except requests.exceptions.RequestException as e:
        print(f"[Error] è­‰äº¤æ‰€ API é€£ç·šå¤±æ•—: {e}")
        return None
    except (ValueError, KeyError) as e:
        print(f"[Error] è­‰äº¤æ‰€ API è³‡æ–™è§£æå¤±æ•—: {e}")
        return None
    except Exception as e:
        print(f"[Error] æœªé æœŸçš„éŒ¯èª¤: {e}")
        return None


# =============================================================================
# è¼”åŠ©å‡½å¼: å–å¾—å‰ 5 æ—¥æˆäº¤é‡å¹³å‡
# =============================================================================
def get_avg_volume_from_csv(n_days: int = 5) -> float:
    """
    å¾ CSV æª”æ¡ˆè®€å–æœ€è¿‘ N æ—¥çš„æˆäº¤é‡å¹³å‡
    """
    print(f"\n[Volume] å¾ CSV è¨ˆç®—å‰ {n_days} æ—¥æˆäº¤é‡å¹³å‡...")
    
    try:
        df = pd.read_csv(CSV_FILE)
        volumes = df['volume'].tail(n_days)
        avg_vol = volumes.mean()
        
        print(f"  ğŸ“Š å‰ {n_days} æ—¥æˆäº¤é‡: {volumes.tolist()}")
        print(f"  ğŸ“ˆ å¹³å‡æˆäº¤é‡: {avg_vol:.2f} å„„å…ƒ")
        
        return avg_vol
        
    except Exception as e:
        print(f"[Error] è®€å– CSV å¤±æ•—: {e}")
        return 3000.0


# =============================================================================
# è¼”åŠ©å‡½å¼: å»ºç«‹æš«å­˜ CSV (ç”¨æ–¼ LSTM è¨“ç·´)
# =============================================================================
def create_temp_csv_with_intraday(intraday_data: tuple, avg_volume: float, workspace: dict) -> str:
    """
    å»ºç«‹æš«å­˜ CSV æª”æ¡ˆï¼ŒåŒ…å«æ­·å²è³‡æ–™ + ç›¤ä¸­è³‡æ–™
    åƒ…ç”¨æ–¼ LSTM è¨“ç·´ï¼Œä¸å½±éŸ¿åŸå§‹ CSV
    
    Returns:
        str: æš«å­˜ CSV è·¯å¾‘
    """
    print("\n[TempCSV] å»ºç«‹æš«å­˜è¨“ç·´è³‡æ–™...")
    
    # 1. è®€å–åŸå§‹ CSV
    df = pd.read_csv(CSV_FILE)
    
    # 2. åŠ å…¥ç•¶æ—¥è³‡æ–™
    date_str, o, h, l, c = intraday_data
    # è½‰æ›æ—¥æœŸæ ¼å¼ç‚º CSV æ ¼å¼ (YYYY/M/D)
    dt = datetime.strptime(date_str, '%Y-%m-%d')
    csv_date = f"{dt.year}/{dt.month}/{dt.day}"
    
    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç•¶æ—¥è³‡æ–™
    last_date = df['date'].iloc[-1]
    last_dt = datetime.strptime(last_date, '%Y/%m/%d')
    
    if last_dt.date() == dt.date():
        # æ›´æ–°æœ€å¾Œä¸€ç­†
        print(f"  [Info] æ›´æ–° {csv_date} çš„è³‡æ–™ç‚ºç›¤ä¸­æ•¸æ“š")
        df.iloc[-1] = [csv_date, o, h, l, c, avg_volume]
    else:
        # æ–°å¢ä¸€ç­†
        print(f"  [Info] åŠ å…¥ç›¤ä¸­è³‡æ–™: {csv_date}")
        new_row = pd.DataFrame({
            'date': [csv_date],
            'open': [o],
            'high': [h],
            'low': [l],
            'close': [c],
            'volume': [avg_volume]
        })
        df = pd.concat([df, new_row], ignore_index=True)
    
    # 3. å­˜æª”åˆ°æš«å­˜ä½ç½®
    temp_csv_path = os.path.join(workspace['cache'], 'temp_twii_data.csv')
    df.to_csv(temp_csv_path, index=False)
    print(f"  âœ… æš«å­˜ CSV å·²å»ºç«‹: {temp_csv_path}")
    print(f"  ğŸ“Š è³‡æ–™ç¯„åœ: {df['date'].iloc[0]} ~ {df['date'].iloc[-1]} ({len(df)} ç­†)")
    
    return temp_csv_path


# =============================================================================
# Step 1: LSTM è¨“ç·´ (ä½¿ç”¨ CSV äº¤æ›ç­–ç•¥)
# =============================================================================
def train_lstm_with_intraday(workspace: dict, temp_csv_path: str, end_date: str):
    """
    ä½¿ç”¨å«ç›¤ä¸­è³‡æ–™è¨“ç·´ LSTM æ¨¡å‹
    
    ç­–ç•¥ï¼šæš«æ™‚äº¤æ› CSV æª”æ¡ˆ
    1. å‚™ä»½åŸå§‹ CSV
    2. ç”¨æš«å­˜ CSV è¦†è“‹åŸå§‹ CSV
    3. åŸ·è¡Œè¨“ç·´
    4. æ¢å¾©åŸå§‹ CSV (ç„¡è«–æˆåŠŸèˆ‡å¦)
    """
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: LSTM è¨“ç·´ (å«ç›¤ä¸­è³‡æ–™)")
    print("=" * 60)
    
    # è¨ˆç®—è¨“ç·´æ—¥æœŸç¯„åœ
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    start_5d = (end_dt - timedelta(days=2200)).strftime('%Y-%m-%d')
    start_1d = (end_dt - timedelta(days=2000)).strftime('%Y-%m-%d')
    split_ratio = "0.99"
    
    # å‚™ä»½åŸå§‹ CSV
    backup_csv_path = CSV_FILE + ".bak"
    print(f"\n[Backup] å‚™ä»½åŸå§‹ CSV -> {backup_csv_path}")
    shutil.copy2(CSV_FILE, backup_csv_path)
    
    # ç”¨æš«å­˜ CSV è¦†è“‹åŸå§‹ CSV
    print(f"[Swap] è¦†è“‹åŸå§‹ CSV ç‚ºç›¤ä¸­è³‡æ–™")
    shutil.copy2(temp_csv_path, CSV_FILE)
    
    training_success = True
    
    try:
        # è¨“ç·´ T+5 æ¨¡å‹
        print(f"\n[Training] T+5 Model ({start_5d} ~ {end_date})...")
        script_5d_path = os.path.join(PROJECT_PATH, SCRIPT_5D)
        cmd_5d = [
            sys.executable, script_5d_path, "train",
            "--start", start_5d,
            "--end", end_date,
            "--split_ratio", split_ratio
        ]
        subprocess.run(cmd_5d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+5 è¨“ç·´å®Œæˆ")

        # è¨“ç·´ T+1 æ¨¡å‹
        print(f"\n[Training] T+1 Model ({start_1d} ~ {end_date})...")
        script_1d_path = os.path.join(PROJECT_PATH, SCRIPT_1D)
        cmd_1d = [
            sys.executable, script_1d_path, "train",
            "--start", start_1d,
            "--end", end_date,
            "--split_ratio", split_ratio
        ]
        subprocess.run(cmd_1d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+1 è¨“ç·´å®Œæˆ")
        
    except subprocess.CalledProcessError as e:
        print(f"[Error] è¨“ç·´å¤±æ•—: {e}")
        training_success = False
    except FileNotFoundError as e:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {e}")
        training_success = False
    except Exception as e:
        print(f"[Error] åŸ·è¡ŒéŒ¯èª¤: {e}")
        training_success = False
    finally:
        # ç„¡è«–æˆåŠŸèˆ‡å¦ï¼Œéƒ½è¦æ¢å¾©åŸå§‹ CSV
        print(f"\n[Restore] æ¢å¾©åŸå§‹ CSV")
        shutil.copy2(backup_csv_path, CSV_FILE)
        os.remove(backup_csv_path)
        print("[Restore] âœ… åŸå§‹ CSV å·²æ¢å¾©")
    
    if not training_success:
        return False

    # å°å­˜æ¨¡å‹åˆ°ç›¤ä¸­å·¥ä½œå€
    print("\n[Archive] å°å­˜æ¨¡å‹åˆ°ç›¤ä¸­å·¥ä½œå€...")
    
    def archive_dir(src_dir, dest_dir):
        if os.path.exists(src_dir):
            if os.path.exists(dest_dir):
                shutil.rmtree(dest_dir)
            shutil.copytree(src_dir, dest_dir)
            print(f"  âœ… å·²å°å­˜: {os.path.basename(src_dir)} -> {dest_dir}")
        else:
            print(f"  âš ï¸ ä¾†æºç›®éŒ„ä¸å­˜åœ¨: {src_dir}")

    archive_dir(DEFAULT_LSTM_5D_DIR, workspace['lstm_5d'])
    archive_dir(DEFAULT_LSTM_1D_DIR, workspace['lstm_1d'])
    
    return True


# =============================================================================
# Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (ä½¿ç”¨ç›¤ä¸­è¨“ç·´çš„æ¨¡å‹)
# =============================================================================
def isolated_feature_engineering_intraday(workspace: dict, intraday_data: tuple, avg_volume: float) -> pd.DataFrame:
    """
    ç›¤ä¸­ç‰ˆç‰¹å¾µå·¥ç¨‹ - ä½¿ç”¨å‰›è¨“ç·´å®Œçš„ LSTM æ¨¡å‹
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ Step 2: ç‰¹å¾µå·¥ç¨‹ (ä½¿ç”¨ç›¤ä¸­è¨“ç·´æ¨¡å‹)")
    print("=" * 60)
    
    # å¼•ç”¨ SelfAttention
    try:
        from twii_model_registry_5d import SelfAttention
        print("[System] æˆåŠŸå¼•ç”¨åŸå§‹ SelfAttention é¡åˆ¥")
    except ImportError:
        print("[Error] ç„¡æ³•å¼•ç”¨ twii_model_registry_5d")
        sys.exit(1)

    def load_model_components(model_dir):
        keras_files = glob.glob(os.path.join(model_dir, "*.keras"))
        if not keras_files: return None, None, None, None
        
        latest_keras = sorted(keras_files)[-1]
        print(f"  ...Loading {os.path.basename(latest_keras)}")
        
        model = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})

        meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
        meta = {}
        if os.path.exists(meta_file):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
            except Exception as e:
                print(f"  âš ï¸ è¼‰å…¥ meta å¤±æ•—: {e}")

        scaler_feat_file = latest_keras.replace('model_', 'feature_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_feat_file):
             scaler_feat_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
        
        scaler_feat = None
        if os.path.exists(scaler_feat_file):
            with open(scaler_feat_file, 'rb') as f:
                scaler_feat = pickle.load(f)

        scaler_tgt_file = latest_keras.replace('model_', 'target_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_tgt_file):
             scaler_tgt = scaler_feat
        else:
             with open(scaler_tgt_file, 'rb') as f:
                 scaler_tgt = pickle.load(f)

        return model, scaler_feat, scaler_tgt, meta

    # 1. å¾ç›¤ä¸­å·¥ä½œå€è¼‰å…¥æ¨¡å‹ (å‰›è¨“ç·´å®Œçš„)
    print("\n[Model Injection] è¼‰å…¥ç›¤ä¸­è¨“ç·´çš„ LSTM æ¨¡å‹...")
    m5d, sf5d, st5d, meta5d = load_model_components(workspace['lstm_5d'])
    m1d, sf1d, st1d, meta1d = load_model_components(workspace['lstm_1d'])
    
    if m5d is None or m1d is None:
        print("[Error] æ¨¡å‹è¼‰å…¥å¤±æ•—")
        sys.exit(1)

    # 2. æ³¨å…¥ä¸»ç³»çµ±
    print("\n[Model Injection] æ³¨å…¥ core_system._LSTM_MODELS...")
    if not hasattr(core_system, '_LSTM_MODELS'):
        core_system._LSTM_MODELS = {}
    
    core_system._LSTM_MODELS.update({
        'model_5d': m5d, 'scaler_feat_5d': sf5d, 'scaler_tgt_5d': st5d, 'meta_5d': meta5d,
        'model_1d': m1d, 'scaler_feat_1d': sf1d, 'scaler_tgt_1d': st1d, 'meta_1d': meta1d,
        'loaded': True
    })
    print("  âœ… æ³¨å…¥å®Œæˆ (å« Target Scalers)")

    # 3. åˆä½µæ­·å²èˆ‡ç›¤ä¸­è³‡æ–™
    print("\n[Merge] åˆä½µæ­·å²è³‡æ–™èˆ‡ç›¤ä¸­è³‡æ–™...")
    df = pd.read_csv(CSV_FILE)
    df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d')
    df = df.set_index('date')
    df = df.rename(columns={
        'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'
    })
    
    date_str, o, h, l, c = intraday_data
    intraday_dt = pd.Timestamp(date_str)
    
    if intraday_dt in df.index:
        df.loc[intraday_dt] = [o, h, l, c, avg_volume]
    else:
        new_row = pd.DataFrame({
            'Open': [o], 'High': [h], 'Low': [l], 'Close': [c], 'Volume': [avg_volume]
        }, index=[intraday_dt])
        df = pd.concat([df, new_row])
    
    df = df.sort_index()
    raw_df = df
    
    # 4. è¨ˆç®—ç‰¹å¾µ
    print(f"\n[Compute] è¨ˆç®—ç‰¹å¾µä¸­ (ä½¿ç”¨ç›¤ä¸­è¨“ç·´æ¨¡å‹)...")
    df = core_system.calculate_features(raw_df, raw_df, ticker="^TWII", use_cache=False)
    
    # åŒ¯å‡º
    features_csv_path = os.path.join(workspace['cache'], 'intraday_features.csv')
    df.to_csv(features_csv_path)
    print(f"[Export] ç›¤ä¸­ç‰¹å¾µæ•¸æ“šå·²å­˜æª”: {features_csv_path}")
    
    return df


# =============================================================================
# Step 3: é›™æ¨¡å‹æ¨è«–
# =============================================================================
def dual_inference(workspace: dict, df: pd.DataFrame) -> dict:
    print("\n" + "=" * 60)
    print("ğŸ¯ Step 3: é›™æ¨¡å‹æ¨è«– (ç›¤ä¸­é æ¸¬)")
    print("=" * 60)
    
    from stable_baselines3 import PPO
    
    FEATURE_COLS = core_system.FEATURE_COLS
    latest = df.iloc[-1]
    
    signal_buy_filter = bool(latest.get('Signal_Buy_Filter', False))
    print(f"  [æ¿¾ç¶²] Signal_Buy_Filter = {signal_buy_filter}")
    
    features = []
    for col in FEATURE_COLS:
        val = latest.get(col, 0.0)
        features.append(val)
    features = np.array(features, dtype=np.float32).reshape(1, -1)
    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
    
    results = {'filter_status': signal_buy_filter}
    
    SELL_SCENARIOS = {
        'cost': 1.00,
        'profit': 1.10,
        'loss': 0.95,
    }
    
    def run_strategy(name, path, key):
        buy_path = os.path.join(path, 'ppo_buy_twii_final.zip')
        sell_path = os.path.join(path, 'ppo_sell_twii_final.zip')
        
        if not os.path.exists(buy_path):
            results[key] = {'error': 'Model not found'}
            print(f"  [Warning] {name}: æ¨¡å‹ä¸å­˜åœ¨")
            return

        try:
            buy_agent = PPO.load(buy_path)
            sell_agent = PPO.load(sell_path)
            
            b_act, _ = buy_agent.predict(features, deterministic=True)
            b_obs = buy_agent.policy.obs_to_tensor(features)[0]
            b_prob = buy_agent.policy.get_distribution(b_obs).distribution.probs.detach().cpu().numpy()[0]
            
            ai_action = 'BUY' if b_act[0] == 1 else 'WAIT'
            buy_prob = float(b_prob[1]) if b_act[0] == 1 else float(b_prob[0])
            
            if signal_buy_filter:
                buy_signal = ai_action
            else:
                buy_signal = f"FILTERED (AI: {ai_action})"
            
            print(f"  [{name}] Buy: {buy_signal} ({buy_prob:.1%})")
            
            sell_scenarios = {}
            for scenario_name, return_value in SELL_SCENARIOS.items():
                s_feat = np.concatenate([features[0], [return_value]]).reshape(1, -1)
                s_act, _ = sell_agent.predict(s_feat, deterministic=True)
                sell_scenarios[scenario_name] = 'SELL' if s_act[0] == 1 else 'HOLD'
            
            print(f"  [{name}] Sell: æˆæœ¬={sell_scenarios['cost']} | ç²åˆ©={sell_scenarios['profit']} | è™§æ={sell_scenarios['loss']}")
            
            results[key] = {
                'name': name,
                'buy_signal': buy_signal,
                'buy_prob': buy_prob,
                'ai_action': ai_action,
                'sell_scenarios': sell_scenarios,
            }
            
        except Exception as e:
            results[key] = {'error': str(e)}
            print(f"  [Error] {name}: {e}")
            import traceback
            traceback.print_exc()

    run_strategy("V3 (Lightweight 200K)", STRATEGY_A_PATH, 'A')
    run_strategy("V4 (Standard 1M)", STRATEGY_B_PATH, 'B')
    
    return results


# =============================================================================
# Step 4: è¼¸å‡ºç›¤ä¸­å ±å‘Š
# =============================================================================
def generate_intraday_report(workspace: dict, df: pd.DataFrame, res: dict, date_str: str, intraday_data: tuple, avg_volume: float):
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: ç›¤ä¸­æˆ°æƒ…å„€è¡¨æ¿")
    print("=" * 60)
    
    last = df.iloc[-1]
    filter_status = res.get('filter_status', False)
    
    _, o, h, l, c = intraday_data
    
    lines = []
    lines.append("=" * 50)
    lines.append(f"ğŸ“… ç›¤ä¸­å³æ™‚åˆ†æ - {date_str}")
    lines.append(f"â° æ›´æ–°æ™‚é–“: {datetime.now().strftime('%H:%M:%S')}")
    lines.append("=" * 50)
    lines.append(f"ğŸ“Š Open:  {o:.2f}")
    lines.append(f"ğŸ“ˆ High:  {h:.2f}")
    lines.append(f"ğŸ“‰ Low:   {l:.2f}")
    lines.append(f"ğŸ’° Close: {c:.2f} (å³æ™‚)")
    lines.append(f"ğŸ“¦ Volume: {avg_volume:.2f} å„„å…ƒ (å‰5æ—¥å¹³å‡ä¼°è¨ˆ)")
    lines.append("-" * 50)
    
    filter_icon = "âœ…" if filter_status else "ğŸš«"
    filter_text = "é€šé (Donchian çªç ´)" if filter_status else "æœªé€šé (éçªç ´æ—¥)"
    lines.append(f"ğŸ¯ [æ¿¾ç¶²ç‹€æ…‹] {filter_icon} {filter_text}")
    lines.append("-" * 50)
    
    lines.append("ğŸ”® [åˆ†æå¸« LSTM] (ç›¤ä¸­è¨“ç·´)")
    lines.append(f"   T+1 æ¼²è·Œ: {last.get('LSTM_Pred_1d', 0)*100:+.2f}% (ä¿¡å¿ƒåº¦: {last.get('LSTM_Conf_1d', 0)*100:.1f}%)")
    lines.append(f"   T+5 æ¼²è·Œ: {last.get('LSTM_Pred_5d', 0)*100:+.2f}% (ä¿¡å¿ƒåº¦: {last.get('LSTM_Conf_5d', 0)*100:.1f}%)")
    lines.append("-" * 50)
    
    lines.append("ğŸ¤– [æ“ç›¤æ‰‹ RL]")
    
    def format_strategy(key, label):
        if key not in res or 'error' in res[key]:
            return [f"   {label}: âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—"]
        
        r = res[key]
        result_lines = []
        
        buy_signal = r['buy_signal']
        buy_prob = r['buy_prob']
        
        if buy_signal == 'BUY':
            buy_icon = "ğŸš€"
        elif buy_signal == 'WAIT':
            buy_icon = "ğŸ’¤"
        elif 'FILTERED' in buy_signal:
            buy_icon = "ğŸš«"
        else:
            buy_icon = "â“"
        
        result_lines.append(f"   ğŸ›’ {label} è²·å…¥: {buy_icon} {buy_signal} ({buy_prob:.1%})")
        
        ss = r.get('sell_scenarios', {})
        result_lines.append(f"   ğŸ“¦ {label} è³£å‡º:")
        result_lines.append(f"      â”œâ”€ æˆæœ¬å€ (0%):  {ss.get('cost', 'N/A')}")
        result_lines.append(f"      â”œâ”€ ç²åˆ©ä¸­ (+10%): {ss.get('profit', 'N/A')}")
        result_lines.append(f"      â””â”€ è™§æä¸­ (-5%):  {ss.get('loss', 'N/A')}")
        
        return result_lines
    
    lines.extend(format_strategy('A', 'V3'))
    lines.append("")
    lines.extend(format_strategy('B', 'V4'))
    lines.append("-" * 50)
    
    ai_a = res.get('A', {}).get('ai_action', 'N/A')
    ai_b = res.get('B', {}).get('ai_action', 'N/A')
    
    if not filter_status:
        if ai_a == 'BUY' and ai_b == 'BUY':
            advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: é›™è²·é€² (è¢«æ“‹ä¸‹)"
        elif ai_a == 'BUY' or ai_b == 'BUY':
            advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: æœ‰æ„è²·é€² (è¢«æ“‹ä¸‹)"
        else:
            advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: è§€æœ›"
    elif ai_a == 'BUY' and ai_b == 'BUY':
        advice = "â­â­ V3+V4 é›™è²·é€² (Strong Buy) â­â­"
    elif ai_a == 'WAIT' and ai_b == 'WAIT':
        advice = "ğŸ’¤ ç©ºæ‰‹è§€æœ› (Wait)"
    elif ai_a == 'BUY':
        advice = "âš ï¸ åƒ… V3 è²·é€² (V3 Only)"
    elif ai_b == 'BUY':
        advice = "âš ï¸ åƒ… V4 è²·é€² (V4 Only)"
    else:
        advice = "â“ è¨Šè™Ÿä¸æ˜"
        
    lines.append(f"ğŸ’¡ ç›¤ä¸­å»ºè­°: {advice}")
    lines.append("=" * 50)
    lines.append("")
    lines.append("âš ï¸ æ³¨æ„ï¼šæ­¤ç‚ºç›¤ä¸­å³æ™‚åˆ†æ")
    lines.append("   â€¢ LSTM ä½¿ç”¨ç›¤ä¸­åƒ¹æ ¼ + 5æ—¥å‡é‡è¨“ç·´")
    lines.append("   â€¢ æˆäº¤é‡ç‚ºé ä¼°å€¼ï¼Œå¯¦éš›çµæœå¯èƒ½æœ‰å·®ç•°")
    lines.append("ğŸ“Œ æ”¶ç›¤å¾ŒåŸ·è¡Œ daily_ops_dual.py å–å¾—æ­£å¼åˆ†æ")
    
    report = "\n".join(lines)
    print(report)
    
    # å­˜æª” TXT
    txt_path = os.path.join(workspace['reports'], 'intraday_summary.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # å­˜æª” JSON
    json_path = os.path.join(workspace['reports'], 'intraday_summary.json')
    json_data = {
        'date': date_str,
        'generated_at': datetime.now().isoformat(),
        'type': 'intraday_with_training',
        'workspace': workspace['root'],
        'filter_status': filter_status,
        'market': {
            'open': float(o),
            'high': float(h),
            'low': float(l),
            'close': float(c),
            'volume_estimated': float(avg_volume),
        },
        'lstm': {
            'pred_1d': float(last.get('LSTM_Pred_1d', 0)),
            'conf_1d': float(last.get('LSTM_Conf_1d', 0)),
            'pred_5d': float(last.get('LSTM_Pred_5d', 0)),
            'conf_5d': float(last.get('LSTM_Conf_5d', 0)),
        },
        'strategies': {
            'A': res.get('A', {}),
            'B': res.get('B', {}),
        },
        'advice': advice,
    }
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n[Report] å·²å„²å­˜: {txt_path}")
    print(f"[Report] å·²å„²å­˜: {json_path}")


# =============================================================================
# Main
# =============================================================================
def main():
    now = datetime.now()
    date_str = now.strftime('%Y-%m-%d')
    time_str = now.strftime('%H%M%S')
    
    print("=" * 60)
    print(f"ğŸš€ ç›¤ä¸­å®Œæ•´åˆ†æç³»çµ±å•Ÿå‹• - {date_str}")
    print(f"â° åŸ·è¡Œæ™‚é–“: {now.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Step 0: å»ºç«‹ç›¤ä¸­å°ˆå±¬å·¥ä½œå€
    ws = create_intraday_workspace(date_str, time_str)
    
    # Step 0.5: å–å¾—ç›¤ä¸­ OHLC
    intraday_data = fetch_intraday_ohlc("^TWII")
    if intraday_data is None:
        print("[Error] ç„¡æ³•å–å¾—ç›¤ä¸­è³‡æ–™ï¼ŒçµæŸåŸ·è¡Œ")
        sys.exit(1)
    
    actual_date = intraday_data[0]
    
    # Step 0.6: å–å¾—å‰ 5 æ—¥æˆäº¤é‡å¹³å‡
    avg_volume = get_avg_volume_from_csv(n_days=5)
    
    # Step 0.7: å»ºç«‹æš«å­˜ CSV (ç”¨æ–¼è¨“ç·´)
    temp_csv_path = create_temp_csv_with_intraday(intraday_data, avg_volume, ws)
    
    # Step 1: LSTM è¨“ç·´ (ä½¿ç”¨æš«å­˜ CSV)
    success = train_lstm_with_intraday(ws, temp_csv_path, actual_date)
    if not success:
        print("[Error] LSTM è¨“ç·´å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œé æ¸¬")
        sys.exit(1)
    
    # Step 2: ç‰¹å¾µå·¥ç¨‹ (ä½¿ç”¨å‰›è¨“ç·´å®Œçš„æ¨¡å‹)
    df = isolated_feature_engineering_intraday(ws, intraday_data, avg_volume)
    
    # Step 3: é›™æ¨¡å‹æ¨è«–
    res = dual_inference(ws, df)
    
    # Step 4: è¼¸å‡ºå ±å‘Š
    generate_intraday_report(ws, df, res, actual_date, intraday_data, avg_volume)
    
    print(f"\nğŸ‰ ç›¤ä¸­åˆ†æå®Œæˆï¼çµæœå­˜æ”¾æ–¼: {ws['root']}")


if __name__ == "__main__":
    main()
