# -*- coding: utf-8 -*-
"""
================================================================================
Daily Operations V3 - å–®ä¸€ç­–ç•¥æ¯æ—¥ç¶­é‹è…³æœ¬ (è¼•é‡åŒ–å¾®èª¿ç‰ˆæœ¬)
================================================================================
å°ˆç‚º V3 æ¨¡å‹è¨­è¨ˆçš„æ¯æ—¥ç¶­é‹è…³æœ¬ (è¼•é‡åŒ– Fine-tune)

åŠŸèƒ½ï¼š
1. LSTM æ¯æ—¥é‡è¨“èˆ‡å°å­˜ (å‹•æ…‹å¤©æ•¸ + split_ratio 0.99)
2. éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)
3. V3 å–®ä¸€ç­–ç•¥æ¨è«–
4. ç°¡åŒ–å ±å‘Šè¼¸å‡º

V3 ç‰¹é»ï¼š
- Buy Fine-tune: 200K æ­¥ (ä¿ç•™æ›´å¤šé è¨“ç·´çŸ¥è­˜)
- Sell Fine-tune: 100K æ­¥

ä½œè€…ï¼šPhil Liang (Generated by Gemini)
æ—¥æœŸï¼š2025-12-07
================================================================================
"""

import os
import sys
import shutil
import pickle
import subprocess
import json
import glob
from datetime import datetime, timedelta

# è¨­å®š UTF-8 è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import yfinance as yf
from tensorflow import keras
from keras import layers

# =============================================================================
# å¼•ç”¨ä¸»ç³»çµ±
# =============================================================================
import ptrl_hybrid_system as core_system

# =============================================================================
# è¨­å®šè·¯å¾‘
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DAILY_RUNS_PATH = os.path.join(PROJECT_PATH, 'daily_runs_v3')  # V3 å°ˆç”¨è¼¸å‡ºç›®éŒ„

# V3 æ¨¡å‹è·¯å¾‘ (å–®ä¸€ç­–ç•¥)
V3_MODEL_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v3')

# LSTM è¨“ç·´è…³æœ¬åç¨±
SCRIPT_5D = "twii_model_registry_5d.py"
SCRIPT_1D = "twii_model_registry_multivariate.py"

# LSTM æ¨¡å‹é è¨­è¼¸å‡ºè·¯å¾‘
DEFAULT_LSTM_5D_DIR = os.path.join(PROJECT_PATH, 'saved_models_5d')
DEFAULT_LSTM_1D_DIR = os.path.join(PROJECT_PATH, 'saved_models_multivariate')


# =============================================================================
# Step 0: å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€
# =============================================================================
def create_daily_workspace(date_str: str) -> dict:
    daily_path = os.path.join(DAILY_RUNS_PATH, date_str)
    paths = {
        'root': daily_path,
        'lstm_models': os.path.join(daily_path, 'lstm_models'),
        'lstm_5d': os.path.join(daily_path, 'lstm_models', 'saved_models_5d'),
        'lstm_1d': os.path.join(daily_path, 'lstm_models', 'saved_models_multivariate'),
        'cache': os.path.join(daily_path, 'cache'),
        'reports': os.path.join(daily_path, 'reports'),
    }
    for key, path in paths.items():
        os.makedirs(path, exist_ok=True)
    print(f"[Workspace] å»ºç«‹ç•¶æ—¥å·¥ä½œå€: {daily_path}")
    return paths


# =============================================================================
# Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜ (å‹•æ…‹å¤©æ•¸ + å…¨é‡å­¸ç¿’)
# =============================================================================
def train_and_archive_lstm(workspace: dict, end_date: str):
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜")
    print("=" * 60)
    
    # å‹•æ…‹è¨ˆç®—èµ·å§‹æ—¥æœŸ
    # T+5 æ¨¡å‹ï¼šä½¿ç”¨éå» 1800 å¤© - æ•æ‰æ›´é•·è¶¨å‹¢
    # T+1 æ¨¡å‹ï¼šä½¿ç”¨éå» 1600 å¤© - å°ˆæ³¨è¿‘æœŸå¸‚å ´
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    start_5d = (end_dt - timedelta(days=2200)).strftime('%Y-%m-%d')
    start_1d = (end_dt - timedelta(days=2000)).strftime('%Y-%m-%d')
    
    # å•Ÿç”¨å…¨é‡å­¸ç¿’ï¼šsplit_ratio = 0.99
    split_ratio = "0.99"
    
    # 1. åŸ·è¡Œ T+5 è¨“ç·´
    print(f"\n[Training] T+5 Model ({start_5d} ~ {end_date}, split={split_ratio})...")
    script_5d_path = os.path.join(PROJECT_PATH, SCRIPT_5D)
    cmd_5d = [sys.executable, script_5d_path, "train", "--start", start_5d, "--end", end_date, "--split_ratio", split_ratio]
    try:
        subprocess.run(cmd_5d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+5 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+5 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_5d_path}")
        return False
    except Exception as e:
        print(f"[Error] åŸ·è¡ŒéŒ¯èª¤: {e}")
        return False

    # 2. åŸ·è¡Œ T+1 è¨“ç·´
    print(f"\n[Training] T+1 Model ({start_1d} ~ {end_date}, split={split_ratio})...")
    script_1d_path = os.path.join(PROJECT_PATH, SCRIPT_1D)
    cmd_1d = [sys.executable, script_1d_path, "train", "--start", start_1d, "--end", end_date, "--split_ratio", split_ratio]
    try:
        subprocess.run(cmd_1d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+1 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+1 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_1d_path}")
        return False

    # 3. å°å­˜æ¨¡å‹
    print("\n[Archive] å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€...")
    
    def archive_dir(src_dir, dest_dir):
        if os.path.exists(src_dir):
            if os.path.exists(dest_dir):
                shutil.rmtree(dest_dir)
            shutil.copytree(src_dir, dest_dir)
            print(f"  âœ… å·²å°å­˜: {os.path.basename(src_dir)} -> {dest_dir}")
        else:
            print(f"  âš ï¸ ä¾†æºç›®éŒ„ä¸å­˜åœ¨: {src_dir}")

    archive_dir(DEFAULT_LSTM_5D_DIR, workspace['lstm_5d'])
    archive_dir(DEFAULT_LSTM_1D_DIR, workspace['lstm_1d'])
    
    return True


# =============================================================================
# Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)
# =============================================================================
def isolated_feature_engineering(workspace: dict, end_date: str) -> pd.DataFrame:
    print("\n" + "=" * 60)
    print("ğŸ”§ Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (æ¨¡å‹æ³¨å…¥)")
    print("=" * 60)
    
    # å¼•ç”¨æ­£ç¢ºçš„ SelfAttention é¡åˆ¥
    try:
        from twii_model_registry_5d import SelfAttention
        print("[System] æˆåŠŸå¼•ç”¨åŸå§‹ SelfAttention é¡åˆ¥")
    except ImportError:
        print("[Error] ç„¡æ³•å¼•ç”¨ twii_model_registry_5dï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨")
        sys.exit(1)

    # è¼”åŠ©å‡½å¼ï¼šè¼‰å…¥æ•´çµ„æ¨¡å‹å…ƒä»¶
    def load_model_components(model_dir):
        keras_files = glob.glob(os.path.join(model_dir, "*.keras"))
        if not keras_files: return None, None, None, None
        
        latest_keras = sorted(keras_files)[-1]
        print(f"  ...Loading {os.path.basename(latest_keras)}")
        
        model = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})

        # è¼‰å…¥ Meta
        meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
        meta = {}
        if os.path.exists(meta_file):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
            except Exception as e:
                print(f"  âš ï¸ è¼‰å…¥ meta å¤±æ•—: {e}")

        # è¼‰å…¥ Feature Scaler
        scaler_feat_file = latest_keras.replace('model_', 'feature_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_feat_file):
             scaler_feat_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
        
        scaler_feat = None
        if os.path.exists(scaler_feat_file):
            with open(scaler_feat_file, 'rb') as f:
                scaler_feat = pickle.load(f)

        # è¼‰å…¥ Target Scaler
        scaler_tgt_file = latest_keras.replace('model_', 'target_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_tgt_file):
             scaler_tgt = scaler_feat
        else:
             with open(scaler_tgt_file, 'rb') as f:
                 scaler_tgt = pickle.load(f)

        return model, scaler_feat, scaler_tgt, meta

    # 1. è¼‰å…¥æ¨¡å‹
    print("\n[Model Injection] è¼‰å…¥ç•¶æ—¥å°å­˜çš„ LSTM æ¨¡å‹...")
    m5d, sf5d, st5d, meta5d = load_model_components(workspace['lstm_5d'])
    m1d, sf1d, st1d, meta1d = load_model_components(workspace['lstm_1d'])
    
    if m5d is None or m1d is None:
        print("[Error] æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œç‰¹å¾µå·¥ç¨‹")
        sys.exit(1)

    # 2. æ³¨å…¥ä¸»ç³»çµ±
    print("\n[Model Injection] æ³¨å…¥ core_system._LSTM_MODELS...")
    if not hasattr(core_system, '_LSTM_MODELS'):
        core_system._LSTM_MODELS = {}
    
    core_system._LSTM_MODELS.update({
        'model_5d': m5d, 'scaler_feat_5d': sf5d, 'scaler_tgt_5d': st5d, 'meta_5d': meta5d,
        'model_1d': m1d, 'scaler_feat_1d': sf1d, 'scaler_tgt_1d': st1d, 'meta_1d': meta1d,
        'loaded': True
    })
    print("  âœ… æ³¨å…¥å®Œæˆ (å« Target Scalers)")

    # 3. ä¸‹è¼‰æ•¸æ“š & è¨ˆç®—ç‰¹å¾µ
    end_dt = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1)
    download_end = end_dt.strftime('%Y-%m-%d')
    print(f"\n[Compute] Loading Data (2020-01-01 ~ {end_date})...")
    # [Modify] ä½¿ç”¨æœ¬åœ°è³‡æ–™è¼‰å…¥å‡½æ•¸
    raw_df = core_system._load_local_twii_data(start_date="2020-01-01")
    
    # ç¯©é¸æ—¥æœŸç¯„åœ (ç¢ºä¿ä¸è¶…é end_date)
    end_dt_ts = pd.Timestamp(end_date)
    raw_df = raw_df[raw_df.index <= end_dt_ts]
    
    actual_last_date = raw_df.index[-1].strftime('%Y-%m-%d')
    print(f"[Data] å¯¦éš›è³‡æ–™æœ€å¾Œæ—¥æœŸ: {actual_last_date}")

    # [Note] Local CSV æˆäº¤é‡å·²æ˜¯æ­£ç¢ºå–®ä½ (å„„å…ƒ) ä¸”å·²è£œå€¼ï¼Œç„¡éœ€é¡å¤–è™•ç†
    
    # [v2.6] åŒ¯å‡ºåŸå§‹æ•¸æ“š CSV
    raw_csv_path = os.path.join(workspace['cache'], 'raw_data.csv')
    raw_df.to_csv(raw_csv_path)
    print(f"[Export] åŸå§‹æ•¸æ“šå·²å­˜æª”: {raw_csv_path}")
    
    print(f"[Compute] è¨ˆç®—ç‰¹å¾µä¸­ (ä½¿ç”¨ç•¶æ—¥æ¨¡å‹ + 30æ¬¡ MC Dropout)...")
    df = core_system.calculate_features(raw_df, raw_df, ticker="^TWII", use_cache=False)
    
    # [v2.6] åŒ¯å‡ºç‰¹å¾µæ•¸æ“š CSV
    features_csv_path = os.path.join(workspace['cache'], 'processed_features.csv')
    df.to_csv(features_csv_path)
    print(f"[Export] ç‰¹å¾µæ•¸æ“šå·²å­˜æª”: {features_csv_path}")
    
    # å­˜å…¥ç•¶æ—¥å¿«å–
    cache_file = os.path.join(workspace['cache'], 'twii_features.pkl')
    with open(cache_file, 'wb') as f:
        pickle.dump(df, f)
    print(f"[Cache] ç‰¹å¾µå·²å­˜æª”: {cache_file}")
    
    return df, actual_last_date


# =============================================================================
# Step 3: V3 å–®ä¸€ç­–ç•¥æ¨è«– (v2.7 - å…¨æ™‚æ¨è«– + æƒ…å¢ƒåˆ†æ)
# =============================================================================
def single_strategy_inference(workspace: dict, df: pd.DataFrame) -> dict:
    print("\n" + "=" * 60)
    print("ğŸ¯ Step 3: V3 ç­–ç•¥æ¨è«–")
    print("=" * 60)
    
    from stable_baselines3 import PPO
    
    FEATURE_COLS = core_system.FEATURE_COLS
    latest = df.iloc[-1]
    
    # ç²å–æ¿¾ç¶²ç‹€æ…‹
    signal_buy_filter = bool(latest.get('Signal_Buy_Filter', False))
    print(f"  [æ¿¾ç¶²] Signal_Buy_Filter = {signal_buy_filter}")
    
    features = []
    for col in FEATURE_COLS:
        val = latest.get(col, 0.0)
        features.append(val)
    features = np.array(features, dtype=np.float32).reshape(1, -1)
    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
    
    result = {'filter_status': signal_buy_filter}
    
    # ä¸‰ç¨®æŒå€‰æƒ…å¢ƒ
    SELL_SCENARIOS = {
        'cost': 1.00,
        'profit': 1.10,
        'loss': 0.95,
    }
    
    buy_path = os.path.join(V3_MODEL_PATH, 'ppo_buy_twii_final.zip')
    sell_path = os.path.join(V3_MODEL_PATH, 'ppo_sell_twii_final.zip')
    
    if not os.path.exists(buy_path):
        print(f"[Error] V3 æ¨¡å‹ä¸å­˜åœ¨: {buy_path}")
        return {'error': 'Model not found', 'filter_status': signal_buy_filter}

    try:
        buy_agent = PPO.load(buy_path)
        sell_agent = PPO.load(sell_path)
        
        # Buy Logic (å…¨æ™‚æ¨è«–)
        b_act, _ = buy_agent.predict(features, deterministic=True)
        b_obs = buy_agent.policy.obs_to_tensor(features)[0]
        b_prob = buy_agent.policy.get_distribution(b_obs).distribution.probs.detach().cpu().numpy()[0]
        
        ai_action = 'BUY' if b_act[0] == 1 else 'WAIT'
        buy_prob = float(b_prob[1]) if b_act[0] == 1 else float(b_prob[0])
        
        if signal_buy_filter:
            buy_signal = ai_action
        else:
            buy_signal = f"FILTERED (AI: {ai_action})"
        
        print(f"  [Buy] {buy_signal} ({buy_prob:.1%})")
        
        # Sell Logic (æƒ…å¢ƒåˆ†æ)
        sell_scenarios = {}
        for scenario_name, return_value in SELL_SCENARIOS.items():
            s_feat = np.concatenate([features[0], [return_value]]).reshape(1, -1)
            s_act, _ = sell_agent.predict(s_feat, deterministic=True)
            sell_scenarios[scenario_name] = 'SELL' if s_act[0] == 1 else 'HOLD'
        
        print(f"  [Sell] æˆæœ¬={sell_scenarios['cost']} | ç²åˆ©={sell_scenarios['profit']} | è™§æ={sell_scenarios['loss']}")
        
        result.update({
            'buy_signal': buy_signal,
            'buy_prob': buy_prob,
            'ai_action': ai_action,
            'sell_scenarios': sell_scenarios,
        })
        
    except Exception as e:
        result['error'] = str(e)
        print(f"  [Error] æ¨è«–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    return result


# =============================================================================
# Step 4: è¼¸å‡ºå ±å‘Š (v2.7)
# =============================================================================
def generate_report(workspace: dict, df: pd.DataFrame, res: dict, date_str: str):
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: æˆ°æƒ…å„€è¡¨æ¿ (V3 v2.7)")
    print("=" * 60)
    
    last = df.iloc[-1]
    filter_status = res.get('filter_status', False)
    
    lines = []
    lines.append("=" * 50)
    lines.append(f"ğŸ“… æ—¥æœŸ: {date_str}")
    lines.append("=" * 50)
    lines.append(f"ğŸ“Š æ”¶ç›¤: {last['Close']:.2f} | é‡: {last['Volume']:.2f} å„„å…ƒ")
    lines.append("-" * 50)
    
    # æ¿¾ç¶²ç‹€æ…‹
    filter_icon = "âœ…" if filter_status else "ğŸš«"
    filter_text = "é€šé (Donchian çªç ´)" if filter_status else "æœªé€šé (éçªç ´æ—¥)"
    lines.append(f"ğŸš¦ [æ¿¾ç¶²ç‹€æ…‹] {filter_icon} {filter_text}")
    lines.append("-" * 50)
    
    # LSTM
    lines.append("ğŸ”® [åˆ†æå¸« LSTM]")
    lines.append(f"   T+1 æ¼²è·Œ: {last.get('LSTM_Pred_1d', 0)*100:+.2f}% (ä¿¡å¿ƒåº¦: {last.get('LSTM_Conf_1d', 0)*100:.1f}%)")
    lines.append(f"   T+5 æ¼²è·Œ: {last.get('LSTM_Pred_5d', 0)*100:+.2f}% (ä¿¡å¿ƒåº¦: {last.get('LSTM_Conf_5d', 0)*100:.1f}%)")
    lines.append("-" * 50)
    
    # V3 ç­–ç•¥
    lines.append("ğŸ¤– [æ“ç›¤æ‰‹ V3]")
    
    if 'error' not in res:
        buy_signal = res['buy_signal']
        buy_prob = res['buy_prob']
        
        if buy_signal == 'BUY':
            buy_icon = "ğŸš€"
        elif buy_signal == 'WAIT':
            buy_icon = "ğŸ’¤"
        elif 'FILTERED' in buy_signal:
            buy_icon = "ğŸš«"
        else:
            buy_icon = "â“"
        
        lines.append(f"   ğŸ›’ è²·å…¥: {buy_icon} {buy_signal} ({buy_prob:.1%})")
        
        # è³£å‡ºæƒ…å¢ƒçŸ©é™£
        ss = res.get('sell_scenarios', {})
        lines.append(f"   ğŸ“¦ è³£å‡º:")
        lines.append(f"      â”œâ”€ æˆæœ¬å€ (0%):  {ss.get('cost', 'N/A')}")
        lines.append(f"      â”œâ”€ ç²åˆ©ä¸­ (+10%): {ss.get('profit', 'N/A')}")
        lines.append(f"      â””â”€ è™§æä¸­ (-5%):  {ss.get('loss', 'N/A')}")
        
        # ç¶œåˆå»ºè­°
        ai_action = res.get('ai_action', 'N/A')
        if not filter_status:
            if ai_action == 'BUY':
                advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: è²·é€² (è¢«æ“‹ä¸‹)"
            else:
                advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: è§€æœ›"
        elif ai_action == 'BUY':
            advice = "â­ V3 è²·é€²"
        else:
            advice = "ğŸ’¤ ç©ºæ‰‹è§€æœ›"
    else:
        lines.append(f"   âŒ éŒ¯èª¤: {res['error']}")
        advice = "â“ æ¨è«–å¤±æ•—"
    
    lines.append("-" * 50)
    lines.append(f"ğŸ’¡ ç¶œåˆå»ºè­°: {advice}")
    lines.append("=" * 50)
    
    report = "\n".join(lines)
    print(report)
    
    # å­˜æª” TXT
    txt_path = os.path.join(workspace['reports'], 'summary.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # å­˜æª” JSON
    json_path = os.path.join(workspace['reports'], 'summary.json')
    json_data = {
        'date': date_str,
        'generated_at': datetime.now().isoformat(),
        'version': 'V3',
        'market': {
            'close': float(last.get('Close', 0)),
            'volume': float(last.get('Volume', 0)),
        },
        'lstm': {
            'pred_1d': float(last.get('LSTM_Pred_1d', 0)),
            'conf_1d': float(last.get('LSTM_Conf_1d', 0)),
            'pred_5d': float(last.get('LSTM_Pred_5d', 0)),
            'conf_5d': float(last.get('LSTM_Conf_5d', 0)),
        },
        'v3_strategy': res,
        'advice': advice,
    }
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n[Report] å·²å„²å­˜: {txt_path}")
    print(f"[Report] å·²å„²å­˜: {json_path}")


# =============================================================================
# Main
# =============================================================================
def main():
    today = datetime.now()
    # è™•ç†é€±æœ«
    if today.weekday() == 5: today -= timedelta(days=1)
    elif today.weekday() == 6: today -= timedelta(days=2)
    
    date_str = today.strftime('%Y-%m-%d')
    print("=" * 60)
    print(f"ğŸš€ V3 æ¯æ—¥ç¶­é‹ç³»çµ± - {date_str}")
    print("=" * 60)
    
    # æª¢æŸ¥ V3 æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(os.path.join(V3_MODEL_PATH, 'ppo_buy_twii_final.zip')):
        print(f"\n[Error] V3 æ¨¡å‹ä¸å­˜åœ¨: {V3_MODEL_PATH}")
        print("è«‹å…ˆåŸ·è¡Œ train_v3_models.py å®Œæˆè¨“ç·´")
        sys.exit(1)
    
    # Step 0
    ws = create_daily_workspace(date_str)
    
    # Step 1
    train_and_archive_lstm(ws, date_str)
    
    # Step 2
    df, actual_date = isolated_feature_engineering(ws, date_str)
    
    if actual_date != date_str:
        print(f"[Warning] é ä¼°æ—¥æœŸ {date_str} èˆ‡å¯¦éš›è³‡æ–™æ—¥æœŸ {actual_date} ä¸åŒ")
        print(f"[Info] å ±å‘Šå°‡ä½¿ç”¨å¯¦éš›è³‡æ–™æ—¥æœŸ: {actual_date}")
    
    # Step 3
    res = single_strategy_inference(ws, df)
    
    # Step 4
    generate_report(ws, df, res, actual_date)
    
    print("\n" + "=" * 60)
    print("âœ… V3 æ¯æ—¥ç¶­é‹å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
