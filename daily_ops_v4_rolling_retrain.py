# -*- coding: utf-8 -*-
"""
================================================================================
Daily Operations for Rolling LSTM V4 Strategy (with Daily LSTM Retraining)
================================================================================
æ¯æ—¥ç¶­é‹è…³æœ¬ - Rolling LSTM V4 + æ¯æ—¥ LSTM é‡è¨“ç‰ˆ

ç‰¹é»ï¼š
1. æ¯æ—¥é‡è¨“ LSTM æ¨¡å‹ (èˆ‡ daily_ops_dual.py ç›¸åŒ)
2. ä½¿ç”¨ Rolling LSTM è¨“ç·´çš„ RL æ¨¡å‹ (models_hybrid_v4_rolling/)
3. é©åˆæ¯”è¼ƒã€Œæ¯æ—¥é‡è¨“ã€vsã€Œå›ºå®šæ¨¡å‹ã€çš„å·®ç•°

èˆ‡ daily_ops_v4_rolling.py å·®ç•°ï¼š
- åŸ·è¡Œ Step 1 (LSTM æ¯æ—¥é‡è¨“)
- åŸ·è¡Œæ™‚é–“ç´„ 15-20 åˆ†é˜ (å›  LSTM è¨“ç·´)

ä½œè€…ï¼šPhil Liang (Generated by Gemini)
æ—¥æœŸï¼š2025-12-12
================================================================================
"""

import os
import sys
import shutil
import pickle
import subprocess
import json
import glob
from datetime import datetime, timedelta

# è¨­å®š UTF-8 è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
from tensorflow import keras

# =============================================================================
# å¼•ç”¨ä¸»ç³»çµ±
# =============================================================================
import ptrl_hybrid_system as core_system

# =============================================================================
# è¨­å®šè·¯å¾‘
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DAILY_RUNS_PATH = os.path.join(PROJECT_PATH, 'daily_runs_rolling_retrain')

# LSTM è¨“ç·´è…³æœ¬
SCRIPT_5D = "twii_model_registry_5d.py"
SCRIPT_1D = "twii_model_registry_multivariate.py"

# LSTM æ¨¡å‹é è¨­è¼¸å‡ºè·¯å¾‘
DEFAULT_LSTM_5D_DIR = os.path.join(PROJECT_PATH, 'saved_models_5d')
DEFAULT_LSTM_1D_DIR = os.path.join(PROJECT_PATH, 'saved_models_multivariate')

# Rolling RL æ¨¡å‹è·¯å¾‘ (ä½¿ç”¨ Rolling LSTM è¨“ç·´çš„æ¨¡å‹)
ROLLING_RL_PATH = os.path.join(PROJECT_PATH, 'models_hybrid_v4_rolling')


# =============================================================================
# Step 0: å»ºç«‹ç•¶æ—¥å°ˆå±¬å·¥ä½œå€
# =============================================================================
def create_daily_workspace(date_str: str) -> dict:
    daily_path = os.path.join(DAILY_RUNS_PATH, date_str)
    paths = {
        'root': daily_path,
        'lstm_models': os.path.join(daily_path, 'lstm_models'),
        'lstm_5d': os.path.join(daily_path, 'lstm_models', 'saved_models_5d'),
        'lstm_1d': os.path.join(daily_path, 'lstm_models', 'saved_models_multivariate'),
        'cache': os.path.join(daily_path, 'cache'),
        'reports': os.path.join(daily_path, 'reports'),
    }
    for key, path in paths.items():
        os.makedirs(path, exist_ok=True)
    print(f"[Workspace] å»ºç«‹ç•¶æ—¥å·¥ä½œå€: {daily_path}")
    return paths


# =============================================================================
# Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜ (èˆ‡ daily_ops_dual.py ç›¸åŒ)
# =============================================================================
def train_and_archive_lstm(workspace: dict, end_date: str):
    print("\n" + "=" * 60)
    print("ğŸ“š Step 1: LSTM å…¨é‡é‡è¨“èˆ‡å°å­˜ (æ¯æ—¥é‡è¨“)")
    print("=" * 60)
    
    # å‹•æ…‹è¨ˆç®—èµ·å§‹æ—¥æœŸ (èˆ‡ daily_ops_dual.py ä¸€è‡´)
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    start_5d = (end_dt - timedelta(days=2200)).strftime('%Y-%m-%d')
    start_1d = (end_dt - timedelta(days=2000)).strftime('%Y-%m-%d')
    
    # å•Ÿç”¨å…¨é‡å­¸ç¿’ï¼šsplit_ratio = 0.99
    split_ratio = "0.99"
    
    # 1. åŸ·è¡Œ T+5 è¨“ç·´
    print(f"\n[Training] T+5 Model ({start_5d} ~ {end_date}, split={split_ratio})...")
    script_5d_path = os.path.join(PROJECT_PATH, SCRIPT_5D)
    cmd_5d = [sys.executable, script_5d_path, "train", "--start", start_5d, "--end", end_date, "--split_ratio", split_ratio]
    try:
        subprocess.run(cmd_5d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+5 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+5 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_5d_path}")
        return False
    except Exception as e:
        print(f"[Error] åŸ·è¡ŒéŒ¯èª¤: {e}")
        return False

    # 2. åŸ·è¡Œ T+1 è¨“ç·´
    print(f"\n[Training] T+1 Model ({start_1d} ~ {end_date}, split={split_ratio})...")
    script_1d_path = os.path.join(PROJECT_PATH, SCRIPT_1D)
    cmd_1d = [sys.executable, script_1d_path, "train", "--start", start_1d, "--end", end_date, "--split_ratio", split_ratio]
    try:
        subprocess.run(cmd_1d, check=True, timeout=1200, cwd=PROJECT_PATH)
        print("[Training] âœ… T+1 è¨“ç·´å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"[Error] T+1 è¨“ç·´å¤±æ•—: {e}")
        return False
    except FileNotFoundError:
        print(f"[Error] æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬: {script_1d_path}")
        return False

    # 3. å°å­˜æ¨¡å‹
    print("\n[Archive] å°å­˜æ¨¡å‹åˆ°ç•¶æ—¥å·¥ä½œå€...")
    
    def archive_dir(src_dir, dest_dir):
        if os.path.exists(src_dir):
            if os.path.exists(dest_dir):
                shutil.rmtree(dest_dir)
            shutil.copytree(src_dir, dest_dir)
            print(f"  âœ… å·²å°å­˜: {os.path.basename(src_dir)} -> {dest_dir}")
        else:
            print(f"  âš ï¸ ä¾†æºç›®éŒ„ä¸å­˜åœ¨: {src_dir}")

    archive_dir(DEFAULT_LSTM_5D_DIR, workspace['lstm_5d'])
    archive_dir(DEFAULT_LSTM_1D_DIR, workspace['lstm_1d'])
    
    return True


# =============================================================================
# Step 2: éš”é›¢å¼ç‰¹å¾µå·¥ç¨‹ (ä½¿ç”¨ç•¶æ—¥é‡è¨“çš„ LSTM)
# =============================================================================
def isolated_feature_engineering(workspace: dict, end_date: str) -> pd.DataFrame:
    print("\n" + "=" * 60)
    print("ğŸ”§ Step 2: ç‰¹å¾µå·¥ç¨‹ (ä½¿ç”¨ç•¶æ—¥é‡è¨“çš„ LSTM)")
    print("=" * 60)
    
    # å¼•ç”¨ SelfAttention é¡åˆ¥
    try:
        from twii_model_registry_5d import SelfAttention
        print("[System] æˆåŠŸå¼•ç”¨åŸå§‹ SelfAttention é¡åˆ¥")
    except ImportError:
        print("[Error] ç„¡æ³•å¼•ç”¨ twii_model_registry_5d")
        sys.exit(1)

    # è¼”åŠ©å‡½å¼ï¼šè¼‰å…¥æ•´çµ„æ¨¡å‹å…ƒä»¶
    def load_model_components(model_dir):
        keras_files = glob.glob(os.path.join(model_dir, "*.keras"))
        if not keras_files: return None, None, None, None
        
        latest_keras = sorted(keras_files)[-1]
        print(f"  ...Loading {os.path.basename(latest_keras)}")
        
        model = keras.models.load_model(latest_keras, custom_objects={'SelfAttention': SelfAttention})

        # è¼‰å…¥ Meta
        meta_file = latest_keras.replace('model_', 'meta_').replace('.keras', '.json')
        meta = {}
        if os.path.exists(meta_file):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
            except Exception as e:
                print(f"  âš ï¸ è¼‰å…¥ meta å¤±æ•—: {e}")

        # è¼‰å…¥ Feature Scaler
        scaler_feat_file = latest_keras.replace('model_', 'feature_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_feat_file):
             scaler_feat_file = latest_keras.replace('model_', 'scaler_').replace('.keras', '.pkl')
        
        scaler_feat = None
        if os.path.exists(scaler_feat_file):
            with open(scaler_feat_file, 'rb') as f:
                scaler_feat = pickle.load(f)

        # è¼‰å…¥ Target Scaler
        scaler_tgt_file = latest_keras.replace('model_', 'target_scaler_').replace('.keras', '.pkl')
        if not os.path.exists(scaler_tgt_file):
             scaler_tgt = scaler_feat
        else:
             with open(scaler_tgt_file, 'rb') as f:
                 scaler_tgt = pickle.load(f)

        return model, scaler_feat, scaler_tgt, meta

    # 1. è¼‰å…¥ç•¶æ—¥é‡è¨“çš„æ¨¡å‹
    print("\n[Model Injection] è¼‰å…¥ç•¶æ—¥é‡è¨“çš„ LSTM æ¨¡å‹...")
    m5d, sf5d, st5d, meta5d = load_model_components(workspace['lstm_5d'])
    m1d, sf1d, st1d, meta1d = load_model_components(workspace['lstm_1d'])
    
    if m5d is None or m1d is None:
        print("[Error] æ¨¡å‹è¼‰å…¥å¤±æ•—")
        sys.exit(1)

    # 2. æ³¨å…¥ä¸»ç³»çµ±
    print("\n[Model Injection] æ³¨å…¥ core_system._LSTM_MODELS...")
    if not hasattr(core_system, '_LSTM_MODELS'):
        core_system._LSTM_MODELS = {}
    
    core_system._LSTM_MODELS.update({
        'model_5d': m5d, 'scaler_feat_5d': sf5d, 'scaler_tgt_5d': st5d, 'meta_5d': meta5d,
        'model_1d': m1d, 'scaler_feat_1d': sf1d, 'scaler_tgt_1d': st1d, 'meta_1d': meta1d,
        'loaded': True
    })
    print("  âœ… ç•¶æ—¥ LSTM æ¨¡å‹æ³¨å…¥å®Œæˆ")

    # 3. ä¸‹è¼‰æ•¸æ“š & è¨ˆç®—ç‰¹å¾µ
    end_dt = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1)
    print(f"\n[Compute] Loading Data (2020-01-01 ~ {end_date})...")
    
    raw_df = core_system._load_local_twii_data(start_date="2020-01-01")
    
    end_dt_ts = pd.Timestamp(end_date)
    raw_df = raw_df[raw_df.index <= end_dt_ts]
    
    actual_last_date = raw_df.index[-1].strftime('%Y-%m-%d')
    print(f"[Data] å¯¦éš›è³‡æ–™æœ€å¾Œæ—¥æœŸ: {actual_last_date}")
    
    # åŒ¯å‡ºåŸå§‹æ•¸æ“š CSV
    raw_csv_path = os.path.join(workspace['cache'], 'raw_data.csv')
    raw_df.to_csv(raw_csv_path)
    print(f"[Export] åŸå§‹æ•¸æ“šå·²å­˜æª”: {raw_csv_path}")
    
    print(f"[Compute] è¨ˆç®—ç‰¹å¾µä¸­ (ä½¿ç”¨ç•¶æ—¥é‡è¨“çš„ LSTM)...")
    df = core_system.calculate_features(raw_df, raw_df, ticker="^TWII", use_cache=False)
    
    # åŒ¯å‡ºè™•ç†å¾Œç‰¹å¾µæ•¸æ“š CSV
    features_csv_path = os.path.join(workspace['cache'], 'processed_features.csv')
    df.to_csv(features_csv_path)
    print(f"[Export] ç‰¹å¾µæ•¸æ“šå·²å­˜æª”: {features_csv_path}")
    
    # å­˜å…¥ç•¶æ—¥å¿«å–
    cache_file = os.path.join(workspace['cache'], 'twii_features.pkl')
    with open(cache_file, 'wb') as f:
        pickle.dump(df, f)
    print(f"[Cache] ç‰¹å¾µå·²å­˜æª”: {cache_file}")
    
    return df, actual_last_date


# =============================================================================
# Step 3: Rolling V4 æ¨¡å‹æ¨è«– (èˆ‡ daily_ops_v4_rolling.py ç›¸åŒ)
# =============================================================================
def rolling_v4_inference(workspace: dict, df: pd.DataFrame) -> dict:
    print("\n" + "=" * 60)
    print("ğŸ¯ Step 3: Rolling V4 æ¨¡å‹æ¨è«– (å«æ¿¾ç¶²èˆ‡æƒ…å¢ƒåˆ†æ)")
    print("=" * 60)
    
    from stable_baselines3 import PPO
    
    # æº–å‚™ç‰¹å¾µ
    FEATURE_COLS = core_system.FEATURE_COLS
    latest = df.iloc[-1]
    
    # ç²å–æ¿¾ç¶²ç‹€æ…‹
    signal_buy_filter = bool(latest.get('Signal_Buy_Filter', False))
    print(f"  [æ¿¾ç¶²] Signal_Buy_Filter = {signal_buy_filter}")
    
    # ç¢ºä¿ç‰¹å¾µæ¬„ä½å°é½Š
    features = []
    for col in FEATURE_COLS:
        val = latest.get(col, 0.0)
        features.append(val)
    features = np.array(features, dtype=np.float32).reshape(1, -1)
    
    # è™•ç† NaN/Inf
    features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
    
    results = {'filter_status': signal_buy_filter}
    
    # ä¸‰ç¨®æŒå€‰æƒ…å¢ƒ
    SELL_SCENARIOS = {
        'cost': 1.00,
        'profit': 1.10,
        'loss': 0.95,
    }
    
    # è¼‰å…¥ Rolling V4 æ¨¡å‹
    buy_path = os.path.join(ROLLING_RL_PATH, 'ppo_buy_twii_final.zip')
    sell_path = os.path.join(ROLLING_RL_PATH, 'ppo_sell_twii_final.zip')
    
    if not os.path.exists(buy_path):
        results['V4_Rolling'] = {'error': 'Model not found'}
        print(f"[Error] Rolling V4 æ¨¡å‹ä¸å­˜åœ¨: {buy_path}")
        return results

    try:
        buy_agent = PPO.load(buy_path)
        sell_agent = PPO.load(sell_path)
        
        # Buy Logic
        b_act, _ = buy_agent.predict(features, deterministic=True)
        b_obs = buy_agent.policy.obs_to_tensor(features)[0]
        b_prob = buy_agent.policy.get_distribution(b_obs).distribution.probs.detach().cpu().numpy()[0]
        
        ai_action = 'BUY' if b_act[0] == 1 else 'WAIT'
        buy_prob = float(b_prob[1]) if b_act[0] == 1 else float(b_prob[0])
        
        if signal_buy_filter:
            buy_signal = ai_action
        else:
            buy_signal = f"FILTERED (AI: {ai_action})"
        
        print(f"  [V4 Rolling] Buy: {buy_signal} ({buy_prob:.1%})")
        
        # Sell Logic
        sell_scenarios = {}
        for scenario_name, return_value in SELL_SCENARIOS.items():
            s_feat = np.concatenate([features[0], [return_value]]).reshape(1, -1)
            s_act, _ = sell_agent.predict(s_feat, deterministic=True)
            sell_scenarios[scenario_name] = 'SELL' if s_act[0] == 1 else 'HOLD'
        
        print(f"  [V4 Rolling] Sell: æˆæœ¬={sell_scenarios['cost']} | ç²åˆ©={sell_scenarios['profit']} | è™§æ={sell_scenarios['loss']}")
        
        results['V4_Rolling'] = {
            'name': 'V4 Rolling LSTM (Daily Retrain)',
            'buy_signal': buy_signal,
            'buy_prob': buy_prob,
            'ai_action': ai_action,
            'sell_scenarios': sell_scenarios,
        }
        
    except Exception as e:
        results['V4_Rolling'] = {'error': str(e)}
        print(f"[Error] V4 Rolling: {e}")
        import traceback
        traceback.print_exc()

    return results


# =============================================================================
# Step 4: è¼¸å‡ºå ±å‘Š
# =============================================================================
def generate_report(workspace: dict, df: pd.DataFrame, res: dict, date_str: str):
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: æˆ°æƒ…å„€è¡¨æ¿ (Rolling V4 + Daily Retrain)")
    print("=" * 60)
    
    last = df.iloc[-1]
    filter_status = res.get('filter_status', False)
    
    lines = []
    lines.append("=" * 50)
    lines.append(f"ğŸ“… æ—¥æœŸ: {date_str}")
    lines.append("=" * 50)
    lines.append(f"ğŸ“Š æ”¶ç›¤: {last['Close']:.2f} | é‡: {last['Volume']:.2f} å„„å…ƒ")
    lines.append("-" * 50)
    
    # æ¨™æ³¨é€™æ˜¯æ¯æ—¥é‡è¨“ç‰ˆ
    lines.append("âš™ï¸ [æ¨¡å¼] æ¯æ—¥ LSTM é‡è¨“ç‰ˆ")
    lines.append("-" * 50)
    
    # æ¿¾ç¶²ç‹€æ…‹
    filter_icon = "âœ…" if filter_status else "ğŸš«"
    filter_text = "é€šé (Donchian çªç ´)" if filter_status else "æœªé€šé (éçªç ´æ—¥)"
    lines.append(f"ğŸš¦ [æ¿¾ç¶²ç‹€æ…‹] {filter_icon} {filter_text}")
    lines.append("-" * 50)
    
    # LSTM
    lines.append("ğŸ”® [åˆ†æå¸« LSTM - ç•¶æ—¥é‡è¨“]")
    lines.append(f"   T+1 æ¼²è·Œ: {last.get('LSTM_Pred_1d', 0)*100:+.2f}% (ä¿¡å¿ƒåº¦: {last.get('LSTM_Conf_1d', 0)*100:.1f}%)")
    lines.append(f"   T+5 æ¼²è·Œ: {last.get('LSTM_Pred_5d', 0)*100:+.2f}% (ä¿¡å¿ƒåº¦: {last.get('LSTM_Conf_5d', 0)*100:.1f}%)")
    lines.append("-" * 50)
    
    # RL ç­–ç•¥
    lines.append("ğŸ¤– [æ“ç›¤æ‰‹ RL - Rolling V4]")
    
    r = res.get('V4_Rolling', {})
    if 'error' in r:
        lines.append(f"   V4 Rolling: âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
    else:
        buy_signal = r['buy_signal']
        buy_prob = r['buy_prob']
        
        if buy_signal == 'BUY':
            buy_icon = "ğŸš€"
        elif buy_signal == 'WAIT':
            buy_icon = "ğŸ’¤"
        elif 'FILTERED' in buy_signal:
            buy_icon = "ğŸš«"
        else:
            buy_icon = "â“"
        
        lines.append(f"   ğŸ›’ è²·å…¥: {buy_icon} {buy_signal} ({buy_prob:.1%})")
        
        ss = r.get('sell_scenarios', {})
        lines.append(f"   ğŸ“¦ è³£å‡º:")
        lines.append(f"      â”œâ”€ æˆæœ¬å€ (0%):  {ss.get('cost', 'N/A')}")
        lines.append(f"      â”œâ”€ ç²åˆ©ä¸­ (+10%): {ss.get('profit', 'N/A')}")
        lines.append(f"      â””â”€ è™§æä¸­ (-5%):  {ss.get('loss', 'N/A')}")
    
    lines.append("-" * 50)
    
    # ç¶œåˆå»ºè­°
    ai_action = res.get('V4_Rolling', {}).get('ai_action', 'N/A')
    
    if not filter_status:
        if ai_action == 'BUY':
            advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: è²·é€² (è¢«æ“‹ä¸‹)"
        else:
            advice = "ğŸš« æ¿¾ç¶²æ””æˆª | AI æ„åœ–: è§€æœ›"
    elif ai_action == 'BUY':
        advice = "â­ V4 Rolling è²·é€² (Buy) â­"
    else:
        advice = "ğŸ’¤ ç©ºæ‰‹è§€æœ› (Wait)"
        
    lines.append(f"ğŸ’¡ ç¶œåˆå»ºè­°: {advice}")
    lines.append("=" * 50)
    
    report = "\n".join(lines)
    print(report)
    
    # å­˜æª” TXT
    txt_path = os.path.join(workspace['reports'], 'summary.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # å­˜æª” JSON
    json_path = os.path.join(workspace['reports'], 'summary.json')
    json_data = {
        'date': date_str,
        'generated_at': datetime.now().isoformat(),
        'mode': 'daily_retrain',
        'filter_status': filter_status,
        'market': {
            'close': float(last.get('Close', 0)),
            'volume': float(last.get('Volume', 0)),
        },
        'lstm': {
            'pred_1d': float(last.get('LSTM_Pred_1d', 0)),
            'conf_1d': float(last.get('LSTM_Conf_1d', 0)),
            'pred_5d': float(last.get('LSTM_Pred_5d', 0)),
            'conf_5d': float(last.get('LSTM_Conf_5d', 0)),
        },
        'strategy': res.get('V4_Rolling', {}),
        'advice': advice,
    }
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n[Report] å·²å„²å­˜: {txt_path}")
    print(f"[Report] å·²å„²å­˜: {json_path}")


# =============================================================================
# Main
# =============================================================================
def main():
    today = datetime.now()
    # è™•ç†é€±æœ«
    if today.weekday() == 5: today -= timedelta(days=1)
    elif today.weekday() == 6: today -= timedelta(days=2)
    
    date_str = today.strftime('%Y-%m-%d')
    print(f"ğŸš€ å•Ÿå‹• Rolling V4 æ¯æ—¥ç¶­é‹ç³»çµ± (æ¯æ—¥é‡è¨“ç‰ˆ) - {date_str}")
    
    # Step 0: å»ºç«‹å·¥ä½œå€
    ws = create_daily_workspace(date_str)
    
    # Step 1: LSTM æ¯æ—¥é‡è¨“ âš ï¸ é€™æ˜¯èˆ‡ daily_ops_v4_rolling.py çš„ä¸»è¦å·®ç•°
    train_and_archive_lstm(ws, date_str)
    
    # Step 2: ç‰¹å¾µå·¥ç¨‹
    df, actual_date = isolated_feature_engineering(ws, date_str)
    
    if actual_date != date_str:
        print(f"[Warning] é ä¼°æ—¥æœŸ {date_str} èˆ‡å¯¦éš›è³‡æ–™æ—¥æœŸ {actual_date} ä¸åŒ")
        print(f"[Info] å ±å‘Šå°‡ä½¿ç”¨å¯¦éš›è³‡æ–™æ—¥æœŸ: {actual_date}")
    
    # Step 3: æ¨¡å‹æ¨è«–
    res = rolling_v4_inference(ws, df)
    
    # Step 4: è¼¸å‡ºå ±å‘Š
    generate_report(ws, df, res, actual_date)
    
    print("\nâœ… Rolling V4 Daily Ops (æ¯æ—¥é‡è¨“ç‰ˆ) åŸ·è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()
