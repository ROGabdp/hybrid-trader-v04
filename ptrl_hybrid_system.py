# -*- coding: utf-8 -*-
"""
Hybrid Trading System for ^TWII (Taiwan Stock Index)
çµåˆ LSTM-SSAM åƒ¹æ ¼é æ¸¬èˆ‡ Pro Trader RL äº¤æ˜“æ±ºç­–

é–‹ç™¼ç­–ç•¥ï¼š
1. æ•¸æ“šæ“´å…… (Data Expansion)ï¼šå¼•å…¥åœ‹éš›æŒ‡æ•¸æ··åˆè¨“ç·´
2. ç‰¹å¾µèåˆ (Feature Fusion)ï¼šæ•´åˆ LSTM T+1/T+5 é æ¸¬èˆ‡ä¿¡å¿ƒåº¦
3. é·ç§»å­¸ç¿’ (Transfer Learning)ï¼šé€šç”¨ Agent â†’ ^TWII Fine-tuning

Phase 1: åŸºç¤è¨­å®šèˆ‡è³‡æ–™ä¸‹è¼‰ âœ…
Phase 2: ç‰¹å¾µå·¥ç¨‹èˆ‡ LSTM æ•´åˆ âœ…
Phase 3: æ··åˆæ•¸æ“šé è¨“ç·´ âœ…
Phase 4: ^TWII å¾®èª¿èˆ‡å›æ¸¬ âœ…
"""

import os
import sys
import pickle
import psutil

# Windows çµ‚ç«¯æ©Ÿ UTF-8 ç·¨ç¢¼è¨­å®šï¼ˆè§£æ±º emoji é¡¯ç¤ºå•é¡Œï¼‰
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import datetime
import numpy as np
import pandas as pd
import yfinance as yf
import ta
from ta.volatility import AverageTrueRange
from ta.momentum import RSIIndicator
from ta.volume import MFIIndicator
import gymnasium as gym
import matplotlib.pyplot as plt
import torch
from tqdm import tqdm
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
import glob
import multiprocessing
import warnings

# æŠ‘åˆ¶ TensorFlow è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore')

# ä¸­æ–‡å­—å‹è¨­å®š
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


# =============================================================================
# è¨­å®š
# =============================================================================
FEATURE_COLS = [
    'Norm_Close', 'Norm_Open', 'Norm_High', 'Norm_Low',
    'Norm_DC_Lower',
    'Norm_HA_Open', 'Norm_HA_High', 'Norm_HA_Low', 'Norm_HA_Close',
    'Norm_SuperTrend_1', 'Norm_SuperTrend_2',
    'Norm_RSI', 'Norm_MFI',
    'Norm_ATR_Change',
    'Norm_RS_Ratio',
    'RS_ROC_5', 'RS_ROC_10', 'RS_ROC_20', 'RS_ROC_60', 'RS_ROC_120',
    'LSTM_Pred_1d', 'LSTM_Conf_1d',  # [v3.0] æ–°å¢ T+1 ä¿¡å¿ƒåº¦
    'LSTM_Pred_5d', 'LSTM_Conf_5d',
]

CACHE_DIR = "data/processed"
SPLIT_DATE = '2023-01-01'  # Fine-tuning / Backtest åˆ‡åˆ†é»


# =============================================================================
# 0. ç’°å¢ƒèˆ‡ GPU è¨­å®š
# =============================================================================
def setup_environment():
    """è¨­å®šåŸ·è¡Œç’°å¢ƒèˆ‡è·¯å¾‘"""
    if torch.cuda.is_available():
        print(f"âœ… CUDA is available! Device: {torch.cuda.get_device_name(0)}")
        device = "cuda"
    else:
        print("âš ï¸ CUDA not available. Using CPU.")
        device = "cpu"

    PROJECT_PATH = os.getcwd()
    MODELS_PATH = os.path.join(PROJECT_PATH, 'models_hybrid')
    RESULTS_PATH = os.path.join(PROJECT_PATH, 'results_hybrid')
    DATA_PATH = os.path.join(PROJECT_PATH, 'data')
    PROCESSED_PATH = os.path.join(DATA_PATH, 'processed')

    for path in [MODELS_PATH, RESULTS_PATH, DATA_PATH, PROCESSED_PATH]:
        if not os.path.exists(path):
            os.makedirs(path)
            
    return PROJECT_PATH, MODELS_PATH, RESULTS_PATH, DATA_PATH, device


# =============================================================================
# 1. è³‡æ–™ä¸‹è¼‰
# =============================================================================
# =============================================================================
# 1. è³‡æ–™ä¸‹è¼‰
# =============================================================================
def _load_local_twii_data(start_date: str = "2000-01-01", end_date: str = None) -> pd.DataFrame:
    """
    è¼‰å…¥æœ¬åœ° TWII CSV è³‡æ–™ (å«è‡ªå‹•æ›´æ–°é‚è¼¯)
    
    Args:
        start_date: è³‡æ–™èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: è³‡æ–™çµæŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œè‹¥ç‚º None å‰‡å–åˆ°æœ€æ–°
    """
    from datetime import date
    import subprocess
    from pathlib import Path
    
    csv_file = Path(__file__).parent / "twii_data_from_2000_01_01.csv"
    
    if not csv_file.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆï¼š{csv_file}")
    
    # 1. è®€å– CSV
    df = pd.read_csv(csv_file)
    df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d')
    df = df.set_index('date')
    
    # 2. è‡ªå‹•æ›´æ–°æª¢æŸ¥
    today = date.today()
    last_date = df.index.max().date()
    
    if last_date < today:
        # æª¢æŸ¥ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼ˆé€±ä¸€è‡³é€±äº”ï¼‰
        if today.weekday() < 5:  # 0-4 æ˜¯å¹³æ—¥
            print(f"[è³‡æ–™æ›´æ–°] TWII è³‡æ–™ ({last_date}) ä¸æ˜¯æœ€æ–°ï¼Œæ­£åœ¨å‘¼å« update_twii_data.py...")
            update_script = Path(__file__).parent / "update_twii_data.py"
            
            if update_script.exists():
                try:
                    result = subprocess.run(
                        [sys.executable, str(update_script)],
                        cwd=Path(__file__).parent,
                        capture_output=True,
                        text=True
                    )
                    if result.returncode == 0:
                        print(f"[è³‡æ–™æ›´æ–°] æ›´æ–°å®Œæˆï¼")
                        # é‡æ–°è®€å–æ›´æ–°å¾Œçš„æª”æ¡ˆ
                        df = pd.read_csv(csv_file)
                        df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d')
                        df = df.set_index('date')
                    else:
                        print(f"[è­¦å‘Š] æ›´æ–°å¤±æ•—ï¼š{result.stderr}")
                except Exception as e:
                    print(f"[è­¦å‘Š] åŸ·è¡Œæ›´æ–°è…³æœ¬éŒ¯èª¤ï¼š{e}")
            else:
                print(f"[è­¦å‘Š] æ‰¾ä¸åˆ°æ›´æ–°è…³æœ¬ï¼š{update_script}")
    
    # 3. æ¬„ä½é‡æ–°å‘½åèˆ‡éæ¿¾
    df = df.rename(columns={
        'open': 'Open',
        'high': 'High',
        'low': 'Low',
        'close': 'Close',
        'volume': 'Volume'  # å–®ä½ï¼šå„„å…ƒ
    })
    
    # ç¯©é¸æ—¥æœŸç¯„åœ
    start_dt = pd.Timestamp(start_date)
    df = df[df.index >= start_dt]
    
    # [æ–°å¢] è‹¥æŒ‡å®š end_dateï¼Œå‰‡éæ¿¾æ‰ä¹‹å¾Œçš„è³‡æ–™
    if end_date is not None:
        end_dt = pd.Timestamp(end_date)
        df = df[df.index < end_dt]
        print(f"  âœ… ^TWII (Local, æˆªæ­¢ {end_date}): {len(df)} ç­† ({df.index[0].date()} ~ {df.index[-1].date()})")
    else:
        print(f"  âœ… ^TWII (Local): {len(df)} ç­† ({df.index[0].date()} ~ {df.index[-1].date()})")
    
    return df


def fetch_index_data(data_path, start_date="2000-01-01", end_date=None):
    """
    ä¸‹è¼‰å¸‚å ´æŒ‡æ•¸è³‡æ–™ (TWII ä½¿ç”¨æœ¬åœ° CSV)
    
    Args:
        data_path: è³‡æ–™å„²å­˜è·¯å¾‘
        start_date: è³‡æ–™èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: è³‡æ–™çµæŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œè‹¥ç‚º None å‰‡å–åˆ°æœ€æ–°
                  [é‡è¦] é è¨“ç·´æ™‚æ‡‰å‚³å…¥ SPLIT_DATE ä»¥é¿å…è³‡æ–™æ´©æ¼
    """
    # TWII ä»¥å¤–çš„åœ‹éš›æŒ‡æ•¸
    foreign_indices = ["^GSPC", "^IXIC", "^SOX", "^DJI"]
    
    print(f"=" * 60)
    print(f"ğŸ“¥ ä¸‹è¼‰/è¼‰å…¥ å¸‚å ´æŒ‡æ•¸è³‡æ–™")
    if end_date:
        print(f"   (è³‡æ–™ç¯„åœ: {start_date} ~ {end_date}ï¼Œé˜²æ­¢è³‡æ–™æ´©æ¼)")
    print(f"=" * 60)
    
    clean_data = {}
    
    # 1. è¼‰å…¥æœ¬åœ° TWII (å‚³å…¥ end_date)
    try:
        clean_data["^TWII"] = _load_local_twii_data(start_date, end_date)
    except Exception as e:
        print(f"  âŒ ^TWII Loading Failed: {e}")
        
    # 2. ä¸‹è¼‰åœ‹éš›æŒ‡æ•¸ (è‹¥æœ‰ end_dateï¼Œå‰‡é™åˆ¶ä¸‹è¼‰ç¯„åœ)
    if foreign_indices:
        print(f"[ä¸‹è¼‰] æ­£åœ¨ç²å–åœ‹éš›æŒ‡æ•¸: {', '.join(foreign_indices)}...")
        download_end = end_date if end_date else None
        data = yf.download(foreign_indices, start=start_date, end=download_end,
                           group_by='ticker', auto_adjust=True, threads=True, progress=False)
        
        for t in foreign_indices:
            try:
                # è™•ç† MultiIndex æˆ– Single Index
                if isinstance(data.columns, pd.MultiIndex):
                     if t in data.columns.levels[0]:
                        df = data[t].copy()
                else:
                    # å¦‚æœåªæœ‰ä¸€å€‹ tickerï¼Œyf å¯èƒ½ä¸æœƒå›å‚³ MultiIndexï¼Œéœ€æª¢æŸ¥
                    # ä½†é€™è£¡æˆ‘å€‘å‚³å…¥äº† listï¼Œé€šå¸¸æœƒæ˜¯ MultiIndex
                    # è‹¥ç‚ºé˜²è¬ä¸€ï¼Œå‡è¨­ data å°±æ˜¯è©² ticker çš„ df (ä¸éé€™è£¡æœ‰å¤šå€‹ ticker)
                    continue

                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = df.columns.get_level_values(0)
                
                df = df.dropna()
                if len(df) > 250:
                    clean_data[t] = df
                    print(f"  âœ… {t}: {len(df)} ç­†")
            except Exception as e:
                print(f"  âš ï¸ {t}: Failed - {e}")
    
    return clean_data


# =============================================================================
# 2. LSTM æ¨¡å‹è¼‰å…¥èˆ‡æ¨è«–
# =============================================================================
_LSTM_MODELS = {
    'model_1d': None, 'scaler_feat_1d': None, 'scaler_tgt_1d': None, 'meta_1d': None,
    'model_5d': None, 'scaler_feat_5d': None, 'scaler_tgt_5d': None, 'meta_5d': None,
    'loaded': False
}

def load_best_lstm_models(target_date=None):
    """
    è¼‰å…¥ LSTM æ¨¡å‹
    
    Args:
        target_date: æŒ‡å®šé¸æ“‡æ¨¡å‹çš„åŸºæº–æ—¥æœŸ (datetime.date æˆ– str)
                     è‹¥ç‚º Noneï¼Œä½¿ç”¨ date.today()
    """
    global _LSTM_MODELS
    if _LSTM_MODELS['loaded']:
        return True
    
    print("\n[System] Loading LSTM T+1 Model (Multivariate)...")
    try:
        import twii_model_registry_multivariate as lstm_1d_module
        import twii_model_registry_5d as lstm_5d_module
        from datetime import date
        
        # è™•ç† target_date åƒæ•¸
        if target_date is None:
            target_date = date.today()
        elif isinstance(target_date, str):
            from datetime import datetime
            target_date = datetime.strptime(target_date, '%Y-%m-%d').date()
        
        meta_1d = lstm_1d_module.select_best_model(target_date)
        if meta_1d is None:
            return False
        model_1d, scaler_feat_1d, scaler_tgt_1d, _ = lstm_1d_module.load_artifacts(
            meta_1d['train_start'], meta_1d['train_end'])
        print(f"  âœ… T+1 Model: {meta_1d['train_start']} ~ {meta_1d['train_end']}")
        
        print("[System] Loading LSTM T+5 Model (5-Day Direct)...")
        meta_5d = lstm_5d_module.select_best_model(target_date)
        if meta_5d is None:
            return False
        model_5d, scaler_feat_5d, scaler_tgt_5d, _ = lstm_5d_module.load_artifacts(
            meta_5d['train_start'], meta_5d['train_end'])
        print(f"  âœ… T+5 Model: {meta_5d['train_start']} ~ {meta_5d['train_end']}")
        
        _LSTM_MODELS.update({
            'model_1d': model_1d, 'scaler_feat_1d': scaler_feat_1d,
            'scaler_tgt_1d': scaler_tgt_1d, 'meta_1d': meta_1d,
            'model_5d': model_5d, 'scaler_feat_5d': scaler_feat_5d,
            'scaler_tgt_5d': scaler_tgt_5d, 'meta_5d': meta_5d, 'loaded': True
        })
        return True
    except Exception as e:
        print(f"[Error] Failed to load LSTM models: {e}")
        return False


def reload_lstm_models(target_date):
    """
    å¼·åˆ¶é‡æ–°è¼‰å…¥æŒ‡å®šæ—¥æœŸçš„ LSTM æ¨¡å‹
    
    Args:
        target_date: æŒ‡å®šé¸æ“‡æ¨¡å‹çš„åŸºæº–æ—¥æœŸ (datetime.date æˆ– str)
    
    Returns:
        æ˜¯å¦æˆåŠŸè¼‰å…¥
    """
    global _LSTM_MODELS
    _LSTM_MODELS['loaded'] = False  # å¼·åˆ¶é‡è¼‰
    return load_best_lstm_models(target_date)


def add_lstm_features(df: pd.DataFrame, ticker: str = "Unknown") -> pd.DataFrame:
    """
    æ–°å¢ LSTM é æ¸¬ç‰¹å¾µ
    
    [v3.0] T+1 ä¹Ÿä½¿ç”¨ MC Dropout è¨ˆç®—ä¿¡å¿ƒåº¦
    - LSTM_Pred_1d: T+1 é æ¸¬æ¼²è·Œå¹…
    - LSTM_Conf_1d: T+1 ä¿¡å¿ƒåº¦ (MC Dropout)
    - LSTM_Pred_5d: T+5 é æ¸¬æ¼²è·Œå¹…  
    - LSTM_Conf_5d: T+5 ä¿¡å¿ƒåº¦ (MC Dropout)
    """
    global _LSTM_MODELS
    
    if not _LSTM_MODELS['loaded']:
        df['LSTM_Pred_1d'] = 0.0
        df['LSTM_Conf_1d'] = 0.5
        df['LSTM_Pred_5d'] = 0.0
        df['LSTM_Conf_5d'] = 0.5
        return df
    
    model_1d, model_5d = _LSTM_MODELS['model_1d'], _LSTM_MODELS['model_5d']
    scaler_feat_1d, scaler_feat_5d = _LSTM_MODELS['scaler_feat_1d'], _LSTM_MODELS['scaler_feat_5d']
    scaler_tgt_1d, scaler_tgt_5d = _LSTM_MODELS['scaler_tgt_1d'], _LSTM_MODELS['scaler_tgt_5d']
    lookback_1d = _LSTM_MODELS['meta_1d'].get('lookback', 10)
    lookback_5d = _LSTM_MODELS['meta_5d'].get('lookback', 30)
    
    if 'Adj Close' not in df.columns:
        df = df.copy()
        df['Adj Close'] = df['Close']
    
    if 'Volume_Log' not in df.columns:
        df = _add_lstm_indicators(df)
    
    feature_cols = ['Adj Close', 'Volume_Log', 'K', 'D', 'MACD_Hist']
    missing = [c for c in feature_cols if c not in df.columns]
    if missing:
        df['LSTM_Pred_1d'] = 0.0
        df['LSTM_Conf_1d'] = 0.5
        df['LSTM_Pred_5d'] = 0.0
        df['LSTM_Conf_5d'] = 0.5
        return df
    
    n = len(df)
    pred_1d, pred_5d = np.zeros(n), np.zeros(n)
    conf_1d = np.full(n, 0.5)
    conf_5d = np.full(n, 0.5)
    
    if n < max(lookback_1d, lookback_5d) + 10:
        df['LSTM_Pred_1d'], df['LSTM_Conf_1d'] = pred_1d, conf_1d
        df['LSTM_Pred_5d'], df['LSTM_Conf_5d'] = pred_5d, conf_5d
        return df
    
    features = df[feature_cols].values
    close_prices = df['Close'].values
    
    # =========================================================================
    # [v3.0] T+1 é æ¸¬ - MC Dropout æ¡æ¨£
    # =========================================================================
    try:
        scaled_1d = scaler_feat_1d.transform(features)
        batch_1d = [scaled_1d[i - lookback_1d:i] for i in range(lookback_1d, n)]
        if batch_1d:
            batch_1d = np.array(batch_1d)
            # MC Dropout æ¡æ¨£ 30 æ¬¡
            mc_results_1d = [scaler_tgt_1d.inverse_transform(
                model_1d(batch_1d, training=True).numpy()).flatten() for _ in range(30)]
            mc_mean_1d, mc_std_1d = np.mean(mc_results_1d, axis=0), np.std(mc_results_1d, axis=0)
            
            for j, idx in enumerate(range(lookback_1d, n)):
                if close_prices[idx] > 0:
                    pred_1d[idx] = (mc_mean_1d[j] - close_prices[idx]) / close_prices[idx]
                    cv = mc_std_1d[j] / mc_mean_1d[j] if mc_mean_1d[j] > 0 else 0.1
                    # [v3.0] T+1 ä¿¡å¿ƒåº¦é–€æª» (æ–¹æ¡ˆ A+ï¼Œèˆ‡ T+5 ç›¸åŒç­–ç•¥)
                    # threshold_high = 0.008 (CV <= 0.8% ç‚ºæ»¿åˆ† 1.0)
                    # threshold_low = 0.040 (CV >= 4.0% ç‚ºé›¶åˆ† 0.0)
                    score = 1.0 - (cv - 0.008) / (0.040 - 0.008)
                    conf_1d[idx] = np.clip(score, 0.0, 1.0)
    except:
        pass
    
    # =========================================================================
    # T+5 é æ¸¬ - MC Dropout æ¡æ¨£ (åŸæœ‰é‚è¼¯)
    # =========================================================================
    try:
        scaled_5d = scaler_feat_5d.transform(features)
        batch_5d = [scaled_5d[i - lookback_5d:i] for i in range(lookback_5d, n)]
        if batch_5d:
            batch_5d = np.array(batch_5d)
            # MC Dropout æ¡æ¨£ 30 æ¬¡
            mc_results = [scaler_tgt_5d.inverse_transform(
                model_5d(batch_5d, training=True).numpy()).flatten() for _ in range(30)]
            mc_mean, mc_std = np.mean(mc_results, axis=0), np.std(mc_results, axis=0)
            for j, idx in enumerate(range(lookback_5d, n)):
                if close_prices[idx] > 0:
                    pred_5d[idx] = (mc_mean[j] - close_prices[idx]) / close_prices[idx]
                    cv = mc_std[j] / mc_mean[j] if mc_mean[j] > 0 else 0.1
                    # [v3.0] T+5 ä¿¡å¿ƒåº¦é–€æª» (ç¶­æŒåŸæœ‰è¼ƒå¯¬é¬†è¨­å®š)
                    # threshold_high = 0.001 (CV <= 0.1% ç‚ºæ»¿åˆ† 1.0)
                    # threshold_low = 0.010 (CV >= 1.0% ç‚ºé›¶åˆ† 0.0)
                    score = 1.0 - (cv - 0.001) / (0.010 - 0.001)
                    conf_5d[idx] = np.clip(score, 0.0, 1.0)
    except:
        pass
    
    df['LSTM_Pred_1d'], df['LSTM_Conf_1d'] = pred_1d, conf_1d
    df['LSTM_Pred_5d'], df['LSTM_Conf_5d'] = pred_5d, conf_5d
    return df


def _add_lstm_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """æ–°å¢ LSTM æŠ€è¡“æŒ‡æ¨™"""
    df = df.copy()
    df['Volume_Log'] = np.log1p(df['Volume'])
    low_min = df['Low'].rolling(9).min()
    high_max = df['High'].rolling(9).max()
    df['K'] = ((df['Close'] - low_min) / (high_max - low_min) * 100).rolling(3).mean()
    df['D'] = df['K'].rolling(3).mean()
    ema_fast = df['Close'].ewm(span=12, adjust=False).mean()
    ema_slow = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD_Hist'] = (ema_fast - ema_slow) - (ema_fast - ema_slow).ewm(span=9, adjust=False).mean()
    if 'Adj Close' not in df.columns:
        df['Adj Close'] = df['Close']
    return df


# =============================================================================
# 3. æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µå·¥ç¨‹
# =============================================================================
def calculate_heikin_ashi(df):
    ha_close = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4
    ha_open = [df['Open'].iloc[0]]
    for i in range(1, len(df)):
        ha_open.append((ha_open[-1] + ha_close.iloc[i-1]) / 2)
    ha_open = pd.Series(ha_open, index=df.index)
    return pd.DataFrame({
        'HA_open': ha_open, 'HA_high': pd.concat([df['High'], ha_open, ha_close], axis=1).max(axis=1),
        'HA_low': pd.concat([df['Low'], ha_open, ha_close], axis=1).min(axis=1), 'HA_close': ha_close
    })


def calculate_supertrend(df, length=14, multiplier=3.0):
    atr = AverageTrueRange(df['High'], df['Low'], df['Close'], window=length).average_true_range().fillna(method='bfill')
    hl2 = (df['High'] + df['Low']) / 2
    basic_upper, basic_lower = hl2 + multiplier * atr, hl2 - multiplier * atr
    final_upper, final_lower = basic_upper.copy(), basic_lower.copy()
    trend = np.zeros(len(df))
    for i in range(1, len(df)):
        final_upper.iloc[i] = basic_upper.iloc[i] if basic_upper.iloc[i] < final_upper.iloc[i-1] or df['Close'].iloc[i-1] > final_upper.iloc[i-1] else final_upper.iloc[i-1]
        final_lower.iloc[i] = basic_lower.iloc[i] if basic_lower.iloc[i] > final_lower.iloc[i-1] or df['Close'].iloc[i-1] < final_lower.iloc[i-1] else final_lower.iloc[i-1]
        trend[i] = 1 if df['Close'].iloc[i] > final_upper.iloc[i-1] else (-1 if df['Close'].iloc[i] < final_lower.iloc[i-1] else trend[i-1])
    return pd.DataFrame({'SUPERT_': np.where(trend == 1, final_lower, final_upper)}, index=df.index)


def calculate_features(df_in: pd.DataFrame, benchmark_df: pd.DataFrame, 
                       ticker: str = "Unknown", use_cache: bool = True) -> pd.DataFrame:
    """è¨ˆç®—å®Œæ•´ç‰¹å¾µ"""
    cache_path = os.path.join(CACHE_DIR, f"{ticker.replace('^', '_').replace('.', '_')}_features.pkl")
    
    if use_cache and os.path.exists(cache_path):
        print(f"[Cache] Loading features for {ticker}...")
        try:
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        except:
            pass
    
    print(f"[Compute] Generating features for {ticker}...")
    df = df_in.copy()
    
    df['DC_Upper'] = df['High'].rolling(20).max().shift(1).fillna(method='bfill')
    df['DC_Lower'] = df['Low'].rolling(20).min().shift(1).fillna(method='bfill')
    df['DC_Upper_10'] = df['High'].rolling(10).max().shift(1).fillna(method='bfill')
    df['ATR'] = AverageTrueRange(df['High'], df['Low'], df['Close'], window=10).average_true_range()
    df['RSI'] = RSIIndicator(df['Close'], window=14).rsi()
    try:
        df['MFI'] = MFIIndicator(df['High'], df['Low'], df['Close'], df['Volume'], window=14).money_flow_index()
    except:
        df['MFI'] = 50.0
    
    ha = calculate_heikin_ashi(df)
    df['HA_Open'], df['HA_High'], df['HA_Low'], df['HA_Close'] = ha['HA_open'], ha['HA_high'], ha['HA_low'], ha['HA_close']
    df['SuperTrend_1'] = calculate_supertrend(df, 14, 2.0).iloc[:, 0]
    df['SuperTrend_2'] = calculate_supertrend(df, 21, 1.0).iloc[:, 0]
    
    base_price = df['DC_Upper'].replace(0, np.nan).fillna(method='bfill')
    for col in ['Close', 'Open', 'High', 'Low', 'DC_Lower', 'HA_Open', 'HA_High', 'HA_Low', 'HA_Close', 'SuperTrend_1', 'SuperTrend_2']:
        df[f'Norm_{col}'] = df[col] / base_price
    df['Norm_RSI'], df['Norm_MFI'] = df['RSI'] / 100.0, df['MFI'] / 100.0
    df['Norm_ATR_Change'] = (df['ATR'] / df['ATR'].shift(1)).fillna(1.0)
    
    if benchmark_df is not None:
        bench_close = benchmark_df['Close'].reindex(df.index).fillna(method='ffill')
        df['RS_Raw'] = df['Close'] / bench_close
        rs_min, rs_max = df['RS_Raw'].rolling(250).min(), df['RS_Raw'].rolling(250).max()
        df['Norm_RS_Ratio'] = ((df['RS_Raw'] - rs_min) / ((rs_max - rs_min).replace(0, np.nan).fillna(1.0) + 1e-9)).fillna(0.5)
        for period in [5, 10, 20, 60, 120]:
            df[f'RS_ROC_{period}'] = df['RS_Raw'].pct_change(period).fillna(0)
    else:
        df['Norm_RS_Ratio'] = 0.5
        for period in [5, 10, 20, 60, 120]:
            df[f'RS_ROC_{period}'] = 0.0
    
    df = add_lstm_features(df, ticker)
    df['Signal_Buy_Filter'] = df['High'] > df['DC_Upper_10']
    df['Next_20d_Max'] = df['High'].shift(-20).rolling(20).max() / df['Close'] - 1
    df = df.dropna(subset=[c for c in df.columns if c != 'Next_20d_Max'])
    
    if use_cache:
        try:
            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
            with open(cache_path, 'wb') as f:
                pickle.dump(df, f)
        except:
            pass
    
    return df


# =============================================================================
# 4. RL ç’°å¢ƒå®šç¾©
# =============================================================================
class BuyEnvHybrid(gym.Env):
    """Buy RL Environment with class balancing"""
    def __init__(self, data_dict, is_training=True):
        super().__init__()
        self.samples, self.pos_samples, self.neg_samples = [], [], []
        
        for t, df in data_dict.items():
            df = df.dropna(subset=['Next_20d_Max'])
            signals = df[df['Signal_Buy_Filter'] == True]
            if len(signals) > 0:
                states = signals[FEATURE_COLS].values.astype(np.float32)
                future_rets = signals['Next_20d_Max'].values.astype(np.float32)
                for i in range(len(signals)):
                    sample = (states[i], future_rets[i])
                    self.samples.append(sample)
                    (self.pos_samples if future_rets[i] >= 0.10 else self.neg_samples).append(sample)
        
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(len(FEATURE_COLS),), dtype=np.float32)
        self.is_training = is_training
        self.idx, self.current_sample = 0, None
    
    def reset(self, seed=None, options=None):
        if self.is_training and np.random.rand() < 0.5 and self.pos_samples:
            self.current_sample = self.pos_samples[np.random.randint(len(self.pos_samples))]
        elif self.neg_samples:
            self.current_sample = self.neg_samples[np.random.randint(len(self.neg_samples))]
        else:
            self.current_sample = self.samples[np.random.randint(len(self.samples))]
        return self.current_sample[0], {}
    
    def step(self, action):
        _, max_ret = self.current_sample
        is_success = max_ret >= 0.10
        reward = 1.0 if (action == 1) == is_success else 0.0
        return self.current_sample[0], reward, True, False, {}


class SellEnvHybrid(gym.Env):
    """Sell RL Environment"""
    def __init__(self, data_dict):
        super().__init__()
        self.episodes = []
        
        for t, df in data_dict.items():
            buy_indices = np.where(df['Signal_Buy_Filter'])[0]
            feature_data = df[FEATURE_COLS].values.astype(np.float32)
            close_prices = df['Close'].values.astype(np.float32)
            
            for idx in buy_indices:
                if idx + 120 < len(df):
                    episode_prices = close_prices[idx:idx+120]
                    self.episodes.append({
                        'features': feature_data[idx:idx+120],
                        'returns': episode_prices / episode_prices[0]
                    })
        
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(len(FEATURE_COLS) + 1,), dtype=np.float32)
    
    def reset(self, seed=None, options=None):
        self.current_episode = self.episodes[np.random.randint(len(self.episodes))]
        self.day = 0
        return np.concatenate([self.current_episode['features'][0], [1.0]]).astype(np.float32), {}
    
    def step(self, action):
        current_return = self.current_episode['returns'][self.day]
        is_profitable = current_return >= 1.10
        
        if action == 1 or self.day >= 119:
            max_ret = np.max(self.current_episode['returns'])
            if is_profitable:
                reward = 1.0 + (current_return - 1.10) / (max_ret - 1.10) if max_ret > 1.10 else 1.0
            else:
                reward = -1.0
            done = True
        else:
            reward = 0.5 if not is_profitable else 0.0
            self.day += 1
            done = False
        
        obs = np.concatenate([self.current_episode['features'][min(self.day, 119)], 
                              [self.current_episode['returns'][min(self.day, 119)]]]).astype(np.float32)
        return obs, reward, done, False, {}


# =============================================================================
# 5. Pre-training æµç¨‹
# =============================================================================
def run_pretraining(train_data: dict, models_path: str, device: str):
    """åŸ·è¡Œé è¨“ç·´ (å« TensorBoard æ—¥èªŒè¨˜éŒ„)"""
    print(f"\n[System] Starting Pre-training with {len(train_data)} indices...")
    
    # å»ºç«‹æ—¥èªŒç›®éŒ„
    tensorboard_log = "./tensorboard_logs/"
    os.makedirs(tensorboard_log, exist_ok=True)
    os.makedirs(os.path.join(models_path, "best_pretrain"), exist_ok=True)
    
    n_envs = min(8, max(1, multiprocessing.cpu_count() - 1))
    print(f"[System] CPU cores: {multiprocessing.cpu_count()}, Using {n_envs} envs")
    
    ppo_params = {
        "learning_rate": 0.0001, 
        "n_steps": max(128, 2048 // n_envs),
        "batch_size": 512, 
        "ent_coef": 0.01, 
        "device": device,
        "policy_kwargs": dict(net_arch=[64, 64, 64]), 
        "verbose": 1,
        "tensorboard_log": tensorboard_log  # å•Ÿç”¨ TensorBoard
    }
    
    # =========================================================================
    # Buy Agent
    # =========================================================================
    print("\nğŸ›’ Training Buy Agent (Base Model)...")
    buy_env = make_vec_env(BuyEnvHybrid, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
                           env_kwargs={'data_dict': train_data, 'is_training': True})
    
    # å»ºç«‹è©•ä¼°ç’°å¢ƒ
    eval_buy_env = make_vec_env(BuyEnvHybrid, n_envs=1, vec_env_cls=DummyVecEnv,
                                env_kwargs={'data_dict': train_data, 'is_training': False})
    
    buy_model = PPO("MlpPolicy", buy_env, **ppo_params)
    
    # Callbacks
    buy_callbacks = CallbackList([
        CheckpointCallback(save_freq=80000, save_path=models_path, name_prefix="ppo_buy_base"),
        EvalCallback(eval_buy_env, best_model_save_path=os.path.join(models_path, "best_pretrain", "buy"),
                     log_path="./logs/", eval_freq=10000, n_eval_episodes=50, 
                     deterministic=True)
    ])
    
    buy_model.learn(total_timesteps=1_000_000, callback=buy_callbacks, tb_log_name="buy_pretrain")
    buy_model.save(os.path.join(models_path, "ppo_buy_base"))
    buy_env.close()
    eval_buy_env.close()
    
    # =========================================================================
    # Sell Agent
    # =========================================================================
    print("\nğŸ’° Training Sell Agent (Base Model)...")
    sell_env = make_vec_env(SellEnvHybrid, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
                            env_kwargs={'data_dict': train_data})
    
    # å»ºç«‹è©•ä¼°ç’°å¢ƒ
    eval_sell_env = make_vec_env(SellEnvHybrid, n_envs=1, vec_env_cls=DummyVecEnv,
                                 env_kwargs={'data_dict': train_data})
    
    sell_model = PPO("MlpPolicy", sell_env, **ppo_params)
    
    # Callbacks
    sell_callbacks = CallbackList([
        CheckpointCallback(save_freq=80000, save_path=models_path, name_prefix="ppo_sell_base"),
        EvalCallback(eval_sell_env, best_model_save_path=os.path.join(models_path, "best_pretrain"),
                     log_path="./logs/", eval_freq=10000, n_eval_episodes=50, 
                     deterministic=True)
    ])
    
    sell_model.learn(total_timesteps=500_000, callback=sell_callbacks, tb_log_name="sell_pretrain")
    sell_model.save(os.path.join(models_path, "ppo_sell_base"))
    sell_env.close()
    eval_sell_env.close()
    
    print("[System] Pre-training Completed.")
    return buy_model, sell_model


# =============================================================================
# 6. Fine-tuning æµç¨‹ (Transfer Learning)
# =============================================================================
def run_finetuning(twii_finetune_data: dict, twii_eval_data: dict, models_path: str, device: str,
                   finetune_buy_steps: int = 1_000_000, finetune_sell_steps: int = 300_000):
    """
    é‡å° ^TWII é€²è¡Œå¾®èª¿ (å« TensorBoard æ—¥èªŒè¨˜éŒ„)
    - è¼‰å…¥é è¨“ç·´æ¬Šé‡
    - ä½¿ç”¨è¼ƒä½çš„ Learning Rate (1e-5)
    - å¯è‡ªè¨‚è¨“ç·´æ­¥æ•¸
    - EvalCallback ç›£æ§é©—è­‰é›†è¡¨ç¾
    
    Args:
        finetune_buy_steps: Buy Agent å¾®èª¿æ­¥æ•¸ (default: 1,000,000)
        finetune_sell_steps: Sell Agent å¾®èª¿æ­¥æ•¸ (default: 300,000)
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ Phase 4: Fine-tuning for ^TWII (with TensorBoard)")
    print("=" * 60)
    
    # å»ºç«‹æ—¥èªŒç›®éŒ„
    tensorboard_log = "./tensorboard_logs/"
    os.makedirs(tensorboard_log, exist_ok=True)
    os.makedirs(os.path.join(models_path, "best_tuned"), exist_ok=True)
    os.makedirs("./logs/", exist_ok=True)
    
    n_envs = min(4, max(1, multiprocessing.cpu_count() - 1))
    
    # Fine-tuning åƒæ•¸ï¼ˆTransfer Learning é—œéµï¼‰
    finetune_params = {
        "learning_rate": 1e-5,  # åŸæœ¬çš„ 1/10
        "n_steps": 256,
        "batch_size": 128,
        "ent_coef": 0.005,
        "device": device,
        "verbose": 1
    }
    
    # =========================================================================
    # Fine-tune Buy Agent
    # =========================================================================
    print("\n[Fine-tune] Loading ppo_buy_base and fine-tuning for ^TWII...")
    
    buy_base_path = os.path.join(models_path, "ppo_buy_base.zip")
    if not os.path.exists(buy_base_path):
        print(f"[Error] Base model not found: {buy_base_path}")
        return None, None
    
    buy_env = make_vec_env(BuyEnvHybrid, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
                           env_kwargs={'data_dict': twii_finetune_data, 'is_training': True})
    
    # å»ºç«‹è©•ä¼°ç’°å¢ƒ (ä½¿ç”¨ Backtest æ•¸æ“šå­é›†)
    eval_buy_env = make_vec_env(BuyEnvHybrid, n_envs=1, vec_env_cls=DummyVecEnv,
                                env_kwargs={'data_dict': twii_eval_data, 'is_training': False})
    
    buy_model = PPO.load(buy_base_path, env=buy_env, device=device,
                         tensorboard_log=tensorboard_log)
    buy_model.learning_rate = finetune_params["learning_rate"]
    buy_model.ent_coef = finetune_params["ent_coef"]
    
    # Callbacks
    buy_callbacks = CallbackList([
        CheckpointCallback(save_freq=100000, save_path=models_path, name_prefix="ppo_buy_finetune"),
        EvalCallback(eval_buy_env, best_model_save_path=os.path.join(models_path, "best_tuned", "buy"),
                     log_path="./logs/", eval_freq=10000, n_eval_episodes=30, 
                     deterministic=True)
    ])
    
    print(f"[Fine-tune] Training Buy Agent for {finetune_buy_steps:,} steps (LR: {finetune_params['learning_rate']})")
    buy_model.learn(total_timesteps=finetune_buy_steps, callback=buy_callbacks, 
                    tb_log_name="buy_finetune", reset_num_timesteps=False)
    
    buy_final_path = os.path.join(models_path, "ppo_buy_twii_final")
    buy_model.save(buy_final_path)
    print(f"[Fine-tune] Buy Agent saved to: {buy_final_path}")
    buy_env.close()
    eval_buy_env.close()
    
    # =========================================================================
    # Fine-tune Sell Agent
    # =========================================================================
    print("\n[Fine-tune] Loading ppo_sell_base and fine-tuning for ^TWII...")
    
    sell_base_path = os.path.join(models_path, "ppo_sell_base.zip")
    sell_env = make_vec_env(SellEnvHybrid, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
                            env_kwargs={'data_dict': twii_finetune_data})
    
    # å»ºç«‹è©•ä¼°ç’°å¢ƒ (ä½¿ç”¨ Backtest æ•¸æ“šå­é›†)
    eval_sell_env = make_vec_env(SellEnvHybrid, n_envs=1, vec_env_cls=DummyVecEnv,
                                 env_kwargs={'data_dict': twii_eval_data})
    
    sell_model = PPO.load(sell_base_path, env=sell_env, device=device,
                          tensorboard_log=tensorboard_log)
    sell_model.learning_rate = finetune_params["learning_rate"]
    sell_model.ent_coef = finetune_params["ent_coef"]
    
    # Callbacks
    sell_callbacks = CallbackList([
        CheckpointCallback(save_freq=50000, save_path=models_path, name_prefix="ppo_sell_finetune"),
        EvalCallback(eval_sell_env, best_model_save_path=os.path.join(models_path, "best_tuned", "sell"),
                     log_path="./logs/", eval_freq=10000, n_eval_episodes=30, 
                     deterministic=True)
    ])
    
    print(f"[Fine-tune] Training Sell Agent for {finetune_sell_steps:,} steps (LR: {finetune_params['learning_rate']})")
    sell_model.learn(total_timesteps=finetune_sell_steps, callback=sell_callbacks, 
                     tb_log_name="sell_finetune", reset_num_timesteps=False)
    
    sell_final_path = os.path.join(models_path, "ppo_sell_twii_final")
    sell_model.save(sell_final_path)
    print(f"[Fine-tune] Sell Agent saved to: {sell_final_path}")
    sell_env.close()
    eval_sell_env.close()
    
    print("\n[System] Fine-tuning Completed!")
    return buy_model, sell_model


# =============================================================================
# 7. Backtesting æµç¨‹
# =============================================================================
class HybridBacktester:
    """Hybrid Trading System Backtester"""
    
    def __init__(self, buy_model, sell_model, initial_capital=1_000_000):
        self.buy_model = buy_model
        self.sell_model = sell_model
        self.initial_capital = initial_capital
        
        # äº¤æ˜“è¨˜éŒ„
        self.trades = []
        self.equity_curve = []
        self.buy_signals = []  # (date, price)
        self.sell_signals = [] # (date, price)
    
    def run(self, df: pd.DataFrame) -> dict:
        """
        åŸ·è¡Œå›æ¸¬
        
        Args:
            df: åŒ…å«ç‰¹å¾µçš„ DataFrame
        
        Returns:
            ç¸¾æ•ˆæŒ‡æ¨™å­—å…¸
        """
        capital = self.initial_capital
        position = None  # {'shares': int, 'buy_price': float, 'buy_date': date}
        
        dates = df.index.tolist()
        closes = df['Close'].values
        
        # å»ºç«‹è§€å¯Ÿè³‡æ–™
        features = df[FEATURE_COLS].values.astype(np.float32)
        buy_signals_mask = df['Signal_Buy_Filter'].values
        
        for i in tqdm(range(len(df)), desc="Backtesting"):
            date = dates[i]
            price = closes[i]
            
            # è¨˜éŒ„ç•¶æ—¥æ·¨å€¼
            if position:
                current_value = capital + position['shares'] * price
            else:
                current_value = capital
            self.equity_curve.append({'date': date, 'value': current_value})
            
            # æŒæœ‰ä¸­ï¼šæª¢æŸ¥è³£å‡º
            if position is not None:
                hold_days = i - position['buy_idx']
                current_return = price / position['buy_price']
                
                # æº–å‚™ Sell Agent è§€å¯Ÿ
                sell_obs = np.concatenate([features[i], [current_return]]).astype(np.float32)
                
                # é æ¸¬
                action, _ = self.sell_model.predict(sell_obs.reshape(1, -1), deterministic=True)
                
                # åœææ¢ä»¶ æˆ– AI æ±ºå®šè³£å‡º æˆ– æŒæœ‰è¶…é 120 å¤©
                stop_loss = current_return < 0.92  # -8% åœæ
                should_sell = action[0] == 1 or stop_loss or hold_days >= 120
                
                if should_sell:
                    # åŸ·è¡Œè³£å‡º
                    sell_value = position['shares'] * price
                    profit = sell_value - position['shares'] * position['buy_price']
                    capital += sell_value
                    
                    self.trades.append({
                        'buy_date': position['buy_date'],
                        'buy_price': position['buy_price'],
                        'sell_date': date,
                        'sell_price': price,
                        'return': current_return - 1,
                        'profit': profit
                    })
                    self.sell_signals.append((date, price))
                    
                    position = None
            
            # ç©ºæ‰‹ï¼šæª¢æŸ¥è²·å…¥
            elif buy_signals_mask[i]:
                # æº–å‚™ Buy Agent è§€å¯Ÿ
                buy_obs = features[i].reshape(1, -1)
                action, _ = self.buy_model.predict(buy_obs, deterministic=True)
                
                if action[0] == 1:  # Buy
                    # åŸ·è¡Œè²·å…¥ï¼ˆä½¿ç”¨ 90% è³‡é‡‘ï¼‰
                    invest_amount = capital * 0.9
                    shares = int(invest_amount / price)
                    
                    if shares > 0:
                        cost = shares * price
                        capital -= cost
                        
                        position = {
                            'shares': shares,
                            'buy_price': price,
                            'buy_date': date,
                            'buy_idx': i
                        }
                        self.buy_signals.append((date, price))
        
        # è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™
        return self._calculate_metrics(df)
    
    def _calculate_metrics(self, df: pd.DataFrame) -> dict:
        """è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™"""
        if not self.equity_curve:
            return {}
        
        equity_df = pd.DataFrame(self.equity_curve)
        equity_df['date'] = pd.to_datetime(equity_df['date'])
        equity_df.set_index('date', inplace=True)
        
        # ç¸½å ±é…¬ç‡
        initial = self.initial_capital
        final = equity_df['value'].iloc[-1]
        total_return = (final - initial) / initial
        
        # å¹´åŒ–å ±é…¬ç‡
        days = (equity_df.index[-1] - equity_df.index[0]).days
        years = days / 365.0
        annualized_return = (1 + total_return) ** (1 / years) - 1 if years > 0 else 0
        
        # å¤æ™®å€¼ (å‡è¨­ç„¡é¢¨éšªåˆ©ç‡ = 2%)
        daily_returns = equity_df['value'].pct_change().dropna()
        if len(daily_returns) > 0 and daily_returns.std() > 0:
            sharpe = (daily_returns.mean() * 252 - 0.02) / (daily_returns.std() * np.sqrt(252))
        else:
            sharpe = 0
        
        # æœ€å¤§å›æ’¤
        rolling_max = equity_df['value'].cummax()
        drawdown = (equity_df['value'] - rolling_max) / rolling_max
        max_drawdown = drawdown.min()
        
        # å‹ç‡
        if self.trades:
            wins = sum(1 for t in self.trades if t['return'] > 0)
            win_rate = wins / len(self.trades)
        else:
            win_rate = 0
        
        return {
            'initial_capital': initial,
            'final_value': final,
            'total_return': total_return,
            'annualized_return': annualized_return,
            'sharpe_ratio': sharpe,
            'max_drawdown': max_drawdown,
            'total_trades': len(self.trades),
            'win_rate': win_rate,
            'equity_df': equity_df
        }


def run_backtesting(twii_backtest_df: pd.DataFrame, buy_model, sell_model, 
                    results_path: str, benchmark_df: pd.DataFrame) -> dict:
    """
    åŸ·è¡Œå›æ¸¬ä¸¦è¦–è¦ºåŒ–çµæœ
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š Phase 4: Backtesting (2023-Present)")
    print("=" * 60)
    
    # åŸ·è¡Œå›æ¸¬
    backtester = HybridBacktester(buy_model, sell_model, initial_capital=1_000_000)
    metrics = backtester.run(twii_backtest_df)
    
    if not metrics:
        print("[Error] Backtesting failed!")
        return {}
    
    # å°å‡ºç¸¾æ•ˆ
    print("\n" + "-" * 60)
    print("ğŸ“ˆ Performance Summary")
    print("-" * 60)
    print(f"  åˆå§‹è³‡é‡‘: ${metrics['initial_capital']:,.0f}")
    print(f"  æœ€çµ‚æ·¨å€¼: ${metrics['final_value']:,.0f}")
    print(f"  ç¸½å ±é…¬ç‡: {metrics['total_return']*100:.2f}%")
    print(f"  å¹´åŒ–å ±é…¬: {metrics['annualized_return']*100:.2f}%")
    print(f"  å¤æ™®å€¼:   {metrics['sharpe_ratio']:.2f}")
    print(f"  æœ€å¤§å›æ’¤: {metrics['max_drawdown']*100:.2f}%")
    print(f"  äº¤æ˜“æ¬¡æ•¸: {metrics['total_trades']}")
    print(f"  å‹ç‡:     {metrics['win_rate']*100:.1f}%")
    print("-" * 60)
    
    # ==========================================================================
    # è¦–è¦ºåŒ–
    # ==========================================================================
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    # å­åœ– 1: Portfolio Value vs Benchmark
    ax1 = axes[0]
    
    equity_df = metrics['equity_df']
    
    # Portfolio
    ax1.plot(equity_df.index, equity_df['value'], label='Hybrid System', 
             color='blue', linewidth=2)
    
    # Benchmark (Buy & Hold)
    bench_slice = benchmark_df.loc[equity_df.index[0]:equity_df.index[-1]]['Close']
    bench_normalized = bench_slice / bench_slice.iloc[0] * metrics['initial_capital']
    ax1.plot(bench_normalized.index, bench_normalized.values, 
             label='^TWII Buy & Hold', color='gray', linewidth=1.5, alpha=0.7)
    
    ax1.set_title('Portfolio Value vs Benchmark (2023-Present)', fontsize=14)
    ax1.set_ylabel('Portfolio Value ($)')
    ax1.legend(loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    # å­åœ– 2: Price with Buy/Sell Signals
    ax2 = axes[1]
    
    price_slice = twii_backtest_df['Close']
    ax2.plot(price_slice.index, price_slice.values, label='^TWII Close', 
             color='black', linewidth=1)
    
    # Buy signals (ç´…è‰²ä¸‰è§’å½¢)
    if backtester.buy_signals:
        buy_dates, buy_prices = zip(*backtester.buy_signals)
        ax2.scatter(buy_dates, buy_prices, marker='^', color='red', s=100, 
                    label='Buy Signal', zorder=5)
    
    # Sell signals (ç¶ è‰²ä¸‰è§’å½¢)
    if backtester.sell_signals:
        sell_dates, sell_prices = zip(*backtester.sell_signals)
        ax2.scatter(sell_dates, sell_prices, marker='v', color='green', s=100, 
                    label='Sell Signal', zorder=5)
    
    ax2.set_title('^TWII Price with Trading Signals', fontsize=14)
    ax2.set_ylabel('Price')
    ax2.set_xlabel('Date')
    ax2.legend(loc='upper left')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # å„²å­˜åœ–è¡¨
    save_path = os.path.join(results_path, 'final_performance.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\n[System] Chart saved to: {save_path}")
    
    plt.close()
    
    return metrics


# =============================================================================
# Main Execution
# =============================================================================
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("  ğŸš€ Hybrid Trading System - Full Pipeline")
    print("=" * 70)
    
    # =========================================================================
    # Phase 0: Setup
    # =========================================================================
    PROJECT_PATH, MODELS_PATH, RESULTS_PATH, DATA_PATH, device = setup_environment()
    
    # Check if base models exist
    buy_base_exists = os.path.exists(os.path.join(MODELS_PATH, "ppo_buy_base.zip"))
    sell_base_exists = os.path.exists(os.path.join(MODELS_PATH, "ppo_sell_base.zip"))
    
    # =========================================================================
    # Phase 1-3: Pre-training (if needed)
    # =========================================================================
    if not buy_base_exists or not sell_base_exists:
        print("\n[System] Base models not found. Running Phase 1-3...")
        
        # Load LSTM
        load_best_lstm_models()
        
        # Download data
        raw_data = fetch_index_data(DATA_PATH, start_date="2000-01-01")
        
        # Calculate features
        train_data = {}
        benchmark_df = raw_data.get("^TWII")
        for ticker, df in raw_data.items():
            try:
                processed = calculate_features(df, benchmark_df, ticker, use_cache=True)
                if len(processed) > 100:
                    train_data[ticker] = processed
            except Exception as e:
                print(f"  Error: {ticker} - {e}")
        
        # Pre-training
        run_pretraining(train_data, MODELS_PATH, device)
    else:
        print("\n[System] Base models found. Skipping Phase 1-3.")
    
    # =========================================================================
    # Phase 4: Fine-tuning & Backtesting
    # =========================================================================
    print("\n" + "=" * 70)
    print("  ğŸ“Œ Phase 4: Fine-tuning & Backtesting for ^TWII")
    print("=" * 70)
    
    # Load LSTM models (if not already loaded)
    load_best_lstm_models()
    
    # Load ^TWII data with features
    print("\n[System] Loading ^TWII data...")
    cache_path = os.path.join(CACHE_DIR, "_TWII_features.pkl")
    
    if os.path.exists(cache_path):
        print(f"[Cache] Loading ^TWII features...")
        with open(cache_path, 'rb') as f:
            twii_full_df = pickle.load(f)
    else:
        print("[Compute] Downloading and processing ^TWII...")
        twii_raw = yf.download("^TWII", start="2000-01-01", auto_adjust=True, progress=False)
        twii_full_df = calculate_features(twii_raw, twii_raw, ticker="^TWII", use_cache=True)
    
    print(f"[System] ^TWII data: {len(twii_full_df)} rows")
    print(f"[System] Date range: {twii_full_df.index[0].strftime('%Y-%m-%d')} ~ {twii_full_df.index[-1].strftime('%Y-%m-%d')}")
    
    # =========================================================================
    # Split data
    # =========================================================================
    print(f"\n[System] Splitting data at {SPLIT_DATE}...")
    
    split_date = pd.Timestamp(SPLIT_DATE)
    twii_finetune_df = twii_full_df[twii_full_df.index < split_date]
    twii_backtest_df = twii_full_df[twii_full_df.index >= split_date]
    
    print(f"  Fine-tuning set: {len(twii_finetune_df)} rows (< {SPLIT_DATE})")
    print(f"  Backtest set:    {len(twii_backtest_df)} rows (>= {SPLIT_DATE})")
    
    # =========================================================================
    # Fine-tuning
    # =========================================================================
    finetune_data = {'^TWII': twii_finetune_df}
    eval_data = {'^TWII': twii_backtest_df}  # ä½¿ç”¨ Backtest æ•¸æ“šä½œç‚ºé©—è­‰é›†
    buy_model, sell_model = run_finetuning(finetune_data, eval_data, MODELS_PATH, device)
    
    if buy_model is None:
        print("[Error] Fine-tuning failed!")
        sys.exit(1)
    
    # =========================================================================
    # Backtesting
    # =========================================================================
    # è¼‰å…¥ Fine-tuned æ¨¡å‹
    buy_final = PPO.load(os.path.join(MODELS_PATH, "ppo_buy_twii_final.zip"))
    sell_final = PPO.load(os.path.join(MODELS_PATH, "ppo_sell_twii_final.zip"))
    
    metrics = run_backtesting(twii_backtest_df, buy_final, sell_final, 
                               RESULTS_PATH, twii_full_df)
    
    # =========================================================================
    # Final Summary
    # =========================================================================
    print("\n" + "=" * 70)
    print("  âœ… [System] All Phases Completed.")
    print("=" * 70)
    print(f"""
    ğŸ“Š Final Results:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ç¸½å ±é…¬ç‡:   {metrics.get('total_return', 0)*100:.2f}%
    å¹´åŒ–å ±é…¬:   {metrics.get('annualized_return', 0)*100:.2f}%
    å¤æ™®å€¼:     {metrics.get('sharpe_ratio', 0):.2f}
    æœ€å¤§å›æ’¤:   {metrics.get('max_drawdown', 0)*100:.2f}%
    äº¤æ˜“æ¬¡æ•¸:   {metrics.get('total_trades', 0)}
    å‹ç‡:       {metrics.get('win_rate', 0)*100:.1f}%
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    ğŸ“ Output Files:
    - models_hybrid/ppo_buy_twii_final.zip
    - models_hybrid/ppo_sell_twii_final.zip
    - results_hybrid/final_performance.png
    
    ğŸ“ˆ TensorBoard è¨“ç·´ç›£æ§ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤é–‹å•Ÿ TensorBoardï¼š
    
        tensorboard --logdir ./tensorboard_logs/
    
    é–‹å•Ÿç€è¦½å™¨å‰å¾€ http://localhost:6006
    æŸ¥çœ‹ Loss, Entropy, Reward ç­‰è¨“ç·´æ›²ç·šã€‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)
