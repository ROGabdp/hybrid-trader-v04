# -*- coding: utf-8 -*-
"""
TWII æŠ•è³‡é¡§å•æ©Ÿå™¨äºº (Trade Advisor)
æ™ºæ…§æ•´åˆå¤šæ¨¡å‹ï¼Œç”¢ç”Ÿå‹•æ…‹å®šæœŸå®šé¡æ“ä½œå»ºè­°

åŠŸèƒ½ï¼š
- æ™ºæ…§æ¨¡å‹é¸æ“‡ï¼šè‡ªå‹•æƒæä¸¦é¸æ“‡æœ€ä½³çš„ 1æ—¥/5æ—¥ é æ¸¬æ¨¡å‹
- é›™æ¨¡å‹æ¨è«–ï¼šåŒæ™‚å–å¾—çŸ­æœŸ(T+1)èˆ‡æ³¢æ®µ(T+5)é æ¸¬
- ä¿¡å¿ƒåº¦è©•ä¼°ï¼šMC Dropout (5æ—¥) + RMSE å€é–“åˆ¤æ–· (1æ—¥)
- æŠ•è³‡å»ºè­°ç”¢ç”Ÿï¼šæ ¹æ“šé æ¸¬æ¼²å¹…èˆ‡ä¿¡å¿ƒåº¦æä¾›è³‡é‡‘æ§ç®¡èˆ‡é€²å ´æ™‚æ©Ÿå»ºè­°

ä½¿ç”¨æ–¹å¼ï¼š
  python trade_advisor.py

è¼¸å…¥ä¾†æºï¼š
  - çŸ­æœŸè¨Šè™Ÿ (T+1): saved_models_multivariate/
  - æ³¢æ®µè¶¨å‹¢ (T+5): saved_models_5d/
"""

import json
import pickle
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Optional, Tuple, Dict, Any

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# =============================================================================
# è¨­å®š
# =============================================================================
BASE_DIR = Path(__file__).parent

# æ¨¡å‹ç›®éŒ„
MODELS_DIR_1D = BASE_DIR / "saved_models_multivariate"  # T+1 çŸ­æœŸæ¨¡å‹
MODELS_DIR_5D = BASE_DIR / "saved_models_5d"            # T+5 æ³¢æ®µæ¨¡å‹

# æ¨¡å‹ç¯©é¸æ¢ä»¶
MIN_TRAIN_DAYS = 1460  # æœ€ä½è¨“ç·´å¤©æ•¸ï¼ˆ4 å¹´ï¼‰

# æŠ€è¡“æŒ‡æ¨™åƒæ•¸
KD_PARAMS = (9, 3, 3)
MACD_PARAMS = (12, 26, 9)

# æŠ•è³‡å»ºè­°é–¾å€¼
TREND_BULLISH_THRESHOLD = 0.02    # 5æ—¥æ¼²å¹… > 2% ç‚ºå¤§æ™´å¤©
TREND_BEARISH_THRESHOLD = -0.02   # 5æ—¥æ¼²å¹… < -2% ç‚ºæš´é¢¨é›¨

# MC Dropout è¨­å®š
MC_DROPOUT_ITERATIONS = 30        # MC Dropout é æ¸¬è¿­ä»£æ¬¡æ•¸

# ä¿¡å¿ƒåº¦é–¾å€¼
CV_HIGH_CONFIDENCE = 0.005        # CV < 0.5% ç‚ºé«˜ä¿¡å¿ƒåº¦
CV_LOW_CONFIDENCE = 0.01          # CV > 1% ç‚ºä½ä¿¡å¿ƒåº¦


# =============================================================================
# è‡ªè¨‚ Self-Attention Layerï¼ˆè¼‰å…¥æ¨¡å‹éœ€è¦ï¼‰
# =============================================================================
class SelfAttention(layers.Layer):
    """Sequential Self-Attention Layer"""
    
    def __init__(self, **kwargs):
        super(SelfAttention, self).__init__(**kwargs)
    
    def build(self, input_shape):
        self.units = input_shape[-1]
        
        self.W_q = self.add_weight(
            name='W_query',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        self.W_k = self.add_weight(
            name='W_key',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        self.W_v = self.add_weight(
            name='W_value',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        super(SelfAttention, self).build(input_shape)
    
    def call(self, inputs):
        Q = tf.matmul(inputs, self.W_q)
        K = tf.matmul(inputs, self.W_k)
        V = tf.matmul(inputs, self.W_v)
        
        attention_scores = tf.matmul(Q, K, transpose_b=True)
        d_k = tf.cast(self.units, tf.float32)
        attention_scores = attention_scores / tf.math.sqrt(d_k)
        attention_weights = tf.nn.softmax(attention_scores, axis=-1)
        output = tf.matmul(attention_weights, V)
        
        return output
    
    def get_config(self):
        config = super(SelfAttention, self).get_config()
        return config


# =============================================================================
# ç‰¹å¾µå·¥ç¨‹ï¼ˆèˆ‡è¨“ç·´æ™‚å®Œå…¨ç›¸åŒï¼‰
# =============================================================================
def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ–°å¢æŠ€è¡“æŒ‡æ¨™åˆ° DataFrameï¼ˆèˆ‡è¨“ç·´æ™‚å®Œå…¨ç›¸åŒï¼‰
    
    æ–°å¢æ¬„ä½ï¼š
    - Volume_Log: æˆäº¤é‡ï¼ˆLog è½‰æ›ï¼‰
    - K: KD æŒ‡æ¨™çš„ K å€¼ (9, 3, 3)
    - D: KD æŒ‡æ¨™çš„ D å€¼
    - MACD_Hist: MACD æŸ±ç‹€åœ– (12, 26, 9)
    """
    df = df.copy()
    
    # Volume Log è½‰æ›
    df['Volume_Log'] = np.log1p(df['Volume'])
    
    # KD æŒ‡æ¨™
    k_period, k_smooth, d_smooth = KD_PARAMS
    low_min = df['Low'].rolling(window=k_period).min()
    high_max = df['High'].rolling(window=k_period).max()
    raw_k = (df['Close'] - low_min) / (high_max - low_min) * 100
    df['K'] = raw_k.rolling(window=k_smooth).mean()
    df['D'] = df['K'].rolling(window=d_smooth).mean()
    
    # MACD æŒ‡æ¨™
    fast_period, slow_period, signal_period = MACD_PARAMS
    ema_fast = df['Close'].ewm(span=fast_period, adjust=False).mean()
    ema_slow = df['Close'].ewm(span=slow_period, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()
    df['MACD_Hist'] = macd_line - signal_line
    
    # ç§»é™¤ NaN
    df = df.dropna()
    
    return df


def get_feature_columns() -> list:
    """å–å¾—ç‰¹å¾µæ¬„ä½åç¨±"""
    return ['Adj Close', 'Volume_Log', 'K', 'D', 'MACD_Hist']


# =============================================================================
# æ™ºæ…§æ¨¡å‹é¸æ“‡æ©Ÿåˆ¶
# =============================================================================
def select_best_model(model_dir: Path) -> Optional[Dict[str, Any]]:
    """
    æ™ºæ…§é¸æ“‡æœ€ä½³æ¨¡å‹
    
    é¸æ“‡é‚è¼¯ï¼š
    1. è¨“ç·´æœŸé–“éæ¿¾ï¼štrain_end - train_start >= 1460 å¤© (4å¹´)
    2. é¿å…æœªä¾†æ•¸æ“šï¼štrain_end <= today
    3. æ’åºï¼ˆé™å†ªï¼‰ï¼štrain_end -> r2 -> train_start
    """
    if not model_dir.exists():
        print(f"[éŒ¯èª¤] æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨ï¼š{model_dir}")
        return None
    
    meta_files = list(model_dir.glob("meta_*.json"))
    if not meta_files:
        print(f"[éŒ¯èª¤] åœ¨ {model_dir} æ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹æª”æ¡ˆ")
        return None
    
    today = date.today()
    candidates = []
    
    for meta_file in meta_files:
        try:
            with open(meta_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            train_start = datetime.strptime(metadata['train_start'], '%Y-%m-%d').date()
            train_end = datetime.strptime(metadata['train_end'], '%Y-%m-%d').date()
            
            duration_days = (train_end - train_start).days
            
            if duration_days < MIN_TRAIN_DAYS:
                continue
            
            if train_end > today:
                continue
            
            r2 = metadata.get('metrics', {}).get('r2', 0.0) or 0.0
            
            candidates.append({
                'metadata': metadata,
                'meta_file': meta_file,
                'train_start': train_start,
                'train_end': train_end,
                'duration_days': duration_days,
                'r2': r2
            })
            
        except Exception as e:
            print(f"[è­¦å‘Š] ç„¡æ³•è§£æ {meta_file}: {e}")
            continue
    
    if not candidates:
        print(f"[éŒ¯èª¤] åœ¨ {model_dir} æ‰¾ä¸åˆ°ç¬¦åˆæ¢ä»¶çš„æ¨¡å‹ï¼ˆè¨“ç·´ >= 4 å¹´ï¼‰")
        return None
    
    candidates.sort(
        key=lambda x: (x['train_end'], x['r2'], x['train_start']),
        reverse=True
    )
    
    return candidates[0]['metadata']


# =============================================================================
# æ¨¡å‹è¼‰å…¥
# =============================================================================
def load_model_artifacts(model_dir: Path, metadata: Dict[str, Any]) -> Tuple:
    """
    è¼‰å…¥æ¨¡å‹åŠç›¸é—œè³‡æº
    
    æ”¯æ´å…©ç¨® Scaler å‘½åæ ¼å¼ï¼š
    - æ ¼å¼ Aï¼šfeature_scaler_YYYY-MM-DD_YYYY-MM-DD.pklï¼ˆæ–°æ ¼å¼ï¼‰
    - æ ¼å¼ Bï¼šscaler_YYYY-MM-DD_YYYY-MM-DD.pklï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰
    
    Returns:
        (model, feature_scaler, target_scaler, metadata)
    """
    train_start = metadata['train_start']
    train_end = metadata['train_end']
    
    model_path = model_dir / f"model_{train_start}_{train_end}.keras"
    
    feature_scaler_path = model_dir / f"feature_scaler_{train_start}_{train_end}.pkl"
    target_scaler_path = model_dir / f"target_scaler_{train_start}_{train_end}.pkl"
    
    if not feature_scaler_path.exists():
        legacy_scaler_path = model_dir / f"scaler_{train_start}_{train_end}.pkl"
        if legacy_scaler_path.exists():
            feature_scaler_path = legacy_scaler_path
            target_scaler_path = legacy_scaler_path
            print(f"  [æ³¨æ„] ä½¿ç”¨èˆŠç‰ˆ Scaler æ ¼å¼ï¼š{legacy_scaler_path.name}")
    
    model = keras.models.load_model(
        model_path,
        custom_objects={'SelfAttention': SelfAttention}
    )
    
    with open(feature_scaler_path, 'rb') as f:
        feature_scaler = pickle.load(f)
    
    with open(target_scaler_path, 'rb') as f:
        target_scaler = pickle.load(f)
    
    return model, feature_scaler, target_scaler, metadata


# =============================================================================
# è³‡æ–™ç²å–
# =============================================================================
def download_market_data(lookback_1d: int, lookback_5d: int) -> pd.DataFrame:
    """ä¸‹è¼‰å¸‚å ´è³‡æ–™"""
    required_days = max(lookback_1d, lookback_5d) + 50
    
    print(f"[è³‡æ–™ç²å–] æ­£åœ¨ä¸‹è¼‰ ^TWII æœ€è¿‘ {required_days} å¤©è³‡æ–™...")
    
    ticker = yf.Ticker("^TWII")
    df = ticker.history(period=f"{required_days}d")
    
    if df.empty:
        raise ValueError("ç„¡æ³•å–å¾— ^TWII è³‡æ–™")
    
    print(f"[è³‡æ–™ç²å–] æˆåŠŸä¸‹è¼‰ {len(df)} ç­†è³‡æ–™")
    
    return df


# =============================================================================
# MC Dropout ä¸ç¢ºå®šæ€§è©•ä¼°ï¼ˆé‡å° 5 æ—¥æ¨¡å‹ï¼‰
# =============================================================================
def predict_with_uncertainty(
    model,
    X: np.ndarray,
    target_scaler: MinMaxScaler,
    n_iter: int = MC_DROPOUT_ITERATIONS
) -> Tuple[float, float, str]:
    """
    ä½¿ç”¨ MC Dropout é€²è¡Œä¸ç¢ºå®šæ€§è©•ä¼°
    
    åŸç†ï¼š
    - å¼·åˆ¶é–‹å•Ÿ Dropout æ¨¡å¼ï¼ˆtraining=Trueï¼‰ï¼Œé‡è¤‡é æ¸¬ n_iter æ¬¡
    - è¨ˆç®—é æ¸¬çµæœçš„å¹³å‡å€¼ä½œç‚ºæœ€çµ‚é æ¸¬ï¼Œæ¨™æº–å·®ä½œç‚ºä¸ç¢ºå®šæ€§
    
    Args:
        model: Keras æ¨¡å‹ï¼ˆå¿…é ˆåŒ…å« Dropout å±¤ï¼‰
        X: è¼¸å…¥ç‰¹å¾µ (shape: 1, lookback, n_features)
        target_scaler: ç›®æ¨™è®Šæ•¸ç¸®æ”¾å™¨
        n_iter: MC Dropout è¿­ä»£æ¬¡æ•¸
    
    Returns:
        (mean_price, std_price, confidence_level)
        - mean_price: é æ¸¬åƒ¹æ ¼å¹³å‡å€¼
        - std_price: é æ¸¬åƒ¹æ ¼æ¨™æº–å·®ï¼ˆé¢¨éšªæ³¢å‹•ï¼‰
        - confidence_level: ä¿¡å¿ƒåº¦ç­‰ç´š ('é«˜', 'ä¸­', 'ä½')
    """
    predictions = []
    
    for _ in range(n_iter):
        # å¼·åˆ¶é–‹å•Ÿ Dropout æ¨¡å¼
        y_pred_scaled = model(X, training=True)
        y_pred = target_scaler.inverse_transform(y_pred_scaled.numpy())[0, 0]
        predictions.append(y_pred)
    
    predictions = np.array(predictions)
    
    # è¨ˆç®—çµ±è¨ˆé‡
    mean_price = np.mean(predictions)
    std_price = np.std(predictions)
    
    # è¨ˆç®—è®Šç•°ä¿‚æ•¸ (CV = Std / Mean)
    cv = std_price / mean_price if mean_price != 0 else 0
    
    # åˆ¤æ–·ä¿¡å¿ƒåº¦
    if cv < CV_HIGH_CONFIDENCE:
        confidence_level = "é«˜"
    elif cv > CV_LOW_CONFIDENCE:
        confidence_level = "ä½"
    else:
        confidence_level = "ä¸­"
    
    return mean_price, std_price, confidence_level


# =============================================================================
# RMSE å€é–“ä¿¡å¿ƒåº¦è©•ä¼°ï¼ˆé‡å° 1 æ—¥æ¨¡å‹ï¼‰- å¯¬é¬†é–€æª»ç‰ˆæœ¬
# =============================================================================
def evaluate_1d_confidence(
    pred_price: float,
    current_price: float,
    rmse: float
) -> str:
    """
    æ ¹æ“š RMSE è©•ä¼° 1 æ—¥æ¨¡å‹çš„ä¿¡å¿ƒåº¦ï¼ˆå¯¬é¬†é–€æª»ï¼‰
    
    é‚è¼¯ï¼ˆå¯¬é¬†ç‰ˆï¼‰ï¼š
    - é æœŸç²åˆ©é»æ•¸ = abs(é æ¸¬åƒ¹ - ç¾åƒ¹)
    - è‹¥ é æœŸç²åˆ© > 0.8 * RMSE -> ä¿¡å¿ƒåº¦ï¼šé«˜
    - è‹¥ é æœŸç²åˆ© > 0.4 * RMSE -> ä¿¡å¿ƒåº¦ï¼šä¸­
    - è‹¥ é æœŸç²åˆ© < 0.4 * RMSE -> ä¿¡å¿ƒåº¦ï¼šä½ï¼ˆå¯èƒ½åªæ˜¯é›œè¨Šï¼‰
    
    Args:
        pred_price: é æ¸¬åƒ¹æ ¼
        current_price: ç•¶å‰åƒ¹æ ¼
        rmse: æ¨¡å‹ RMSE
    
    Returns:
        confidence_level: ä¿¡å¿ƒåº¦ç­‰ç´š ('é«˜', 'ä¸­', 'ä½')
    """
    expected_profit = abs(pred_price - current_price)
    
    # å¯¬é¬†é–€æª»ï¼š0.8x å’Œ 0.4x RMSE
    if expected_profit > 0.8 * rmse:
        return "é«˜"
    elif expected_profit > 0.4 * rmse:
        return "ä¸­"
    else:
        return "ä½"


# =============================================================================
# è¶¨å‹¢å…±æŒ¯ (Trend Alignment) åŠ åˆ†æ©Ÿåˆ¶
# =============================================================================
def apply_trend_alignment(
    confidence_1d: str,
    change_5d: float,
    confidence_5d: str
) -> tuple:
    """
    æ ¹æ“š T+5 è¶¨å‹¢å° T+1 ä¿¡å¿ƒåº¦é€²è¡ŒåŠ åˆ†èª¿æ•´
    
    é‚è¼¯ï¼š
    - è‹¥ T+5 çœ‹æ¼² (change > 0) ä¸”ä¿¡å¿ƒåº¦ç‚ºé«˜/ä¸­ï¼š
      â†’ T+1 ä¿¡å¿ƒåº¦å‡ä¸€ç´šï¼ˆä½â†’ä¸­ï¼Œä¸­â†’é«˜ï¼‰
    - è‹¥ T+5 çœ‹è·Œï¼š
      â†’ T+1 ç¶­æŒåŸåˆ¤ï¼ˆé€†å‹¢éœ€é«˜æ¨™æº–ï¼‰
    
    Args:
        confidence_1d: åŸå§‹ T+1 ä¿¡å¿ƒåº¦
        change_5d: T+5 é æœŸæ¼²è·Œå¹…
        confidence_5d: T+5 ä¿¡å¿ƒåº¦
    
    Returns:
        (adjusted_confidence, upgraded): èª¿æ•´å¾Œä¿¡å¿ƒåº¦, æ˜¯å¦æœ‰å‡ç´š
    """
    # æª¢æŸ¥æ˜¯å¦ç¬¦åˆé †å‹¢åŠ åˆ†æ¢ä»¶
    is_t5_bullish = change_5d > 0
    is_t5_confident = confidence_5d in ["é«˜", "ä¸­"]
    
    if is_t5_bullish and is_t5_confident:
        # é †å‹¢äº¤æ˜“ï¼Œä¿¡å¿ƒåº¦å‡ä¸€ç´š
        if confidence_1d == "ä½":
            return "ä¸­", True
        elif confidence_1d == "ä¸­":
            return "é«˜", True
        else:
            return "é«˜", False  # å·²ç¶“æ˜¯é«˜ï¼Œä¸è®Š
    else:
        # é€†å‹¢æˆ– T+5 ä¿¡å¿ƒä¸è¶³ï¼Œç¶­æŒåŸåˆ¤
        return confidence_1d, False


# =============================================================================
# æ¨¡å‹æ¨è«–ï¼ˆæ¨™æº–ç‰ˆï¼‰
# =============================================================================
def prepare_input_data(
    df_processed: pd.DataFrame,
    feature_scaler: MinMaxScaler,
    lookback: int
) -> np.ndarray:
    """æº–å‚™æ¨¡å‹è¼¸å…¥è³‡æ–™"""
    feature_columns = get_feature_columns()
    
    if 'Adj Close' not in df_processed.columns:
        df_processed = df_processed.copy()
        df_processed['Adj Close'] = df_processed['Close']
    
    features = df_processed[feature_columns].values
    scaled_features = feature_scaler.transform(features)
    
    if len(scaled_features) < lookback:
        raise ValueError(f"è³‡æ–™ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {lookback} ç­†")
    
    X = scaled_features[-lookback:].reshape(1, lookback, len(feature_columns))
    
    return X


def run_inference(
    model,
    feature_scaler: MinMaxScaler,
    target_scaler: MinMaxScaler,
    df_processed: pd.DataFrame,
    lookback: int
) -> float:
    """åŸ·è¡Œæ¨™æº–æ¨¡å‹æ¨è«–"""
    X = prepare_input_data(df_processed, feature_scaler, lookback)
    y_pred_scaled = model.predict(X, verbose=0)
    predicted_price = target_scaler.inverse_transform(y_pred_scaled)[0, 0]
    
    return predicted_price


# =============================================================================
# æŠ•è³‡å»ºè­°ç”¢ç”Ÿï¼ˆå«ä¿¡å¿ƒåº¦è€ƒé‡ï¼‰
# =============================================================================
def generate_advice(
    change_1d: float,
    change_5d: float,
    confidence_1d: str,
    confidence_5d: str
) -> Dict[str, str]:
    """
    æ ¹æ“šé æ¸¬æ¼²å¹…èˆ‡ä¿¡å¿ƒåº¦ç”¢ç”ŸæŠ•è³‡å»ºè­°
    
    ä¿¡å¿ƒåº¦èª¿æ•´é‚è¼¯ï¼š
    - 5æ—¥ç­–ç•¥ï¼šå¤§æ™´å¤© + ä½ä¿¡å¿ƒåº¦ -> é™ç´šç‚ºç¶­æŒæ¨™æº–æ‰£æ¬¾
    - 1æ—¥ç­–ç•¥ï¼šç¶ ç‡ˆ + ä½ä¿¡å¿ƒåº¦ -> é™ç´šç‚ºè§€æœ›
    """
    # ==========================================================================
    # è³‡é‡‘æ§ç®¡å»ºè­°ï¼ˆ5æ—¥æ¨¡å‹ + ä¿¡å¿ƒåº¦èª¿æ•´ï¼‰
    # ==========================================================================
    if change_5d > TREND_BULLISH_THRESHOLD:
        if confidence_5d == "ä½":
            # å¤§æ™´å¤©ä½†ä¿¡å¿ƒåº¦ä½ï¼Œé™ç´šè™•ç†
            trend_emoji = "ğŸŒ¤ï¸"
            trend_status = "æ™´æ™‚å¤šé›²"
            trend_advice = "è¶¨å‹¢æ¨‚è§€ä½†ä¿¡å¿ƒä¸è¶³ï¼Œå»ºè­°ç¶­æŒæ¨™æº–æ‰£æ¬¾"
        else:
            trend_emoji = "ğŸŒ"
            trend_status = "å¤§æ™´å¤©"
            trend_advice = "å¸‚å ´æ¨‚è§€ï¼Œå»ºè­°åŠ ç¢¼æ‰£æ¬¾ 1.5~2 å€"
    elif change_5d < TREND_BEARISH_THRESHOLD:
        trend_emoji = "â›ˆï¸"
        trend_status = "æš´é¢¨é›¨"
        trend_advice = "å¸‚å ´æ‚²è§€ï¼Œå»ºè­°æš«åœæ‰£æ¬¾æˆ–æ¸›ç¢¼ 50%"
    else:
        trend_emoji = "â˜ï¸"
        trend_status = "å¤šé›²ç›¤æ•´"
        trend_advice = "å¸‚å ´ä¸­æ€§ï¼Œç¶­æŒæ¨™æº–æ‰£æ¬¾é‡‘é¡"
    
    # ==========================================================================
    # é€²å ´æ™‚æ©Ÿå»ºè­°ï¼ˆ1æ—¥æ¨¡å‹ + ä¿¡å¿ƒåº¦èª¿æ•´ï¼‰
    # ==========================================================================
    if change_1d > 0:
        if confidence_1d == "ä½":
            # ç¶ ç‡ˆä½†ä¿¡å¿ƒåº¦ä½ï¼Œé™ç´šè™•ç†
            timing_emoji = "ğŸŸ¡"
            timing_status = "é»ƒç‡ˆè¬¹æ…"
            timing_advice = "çŸ­æœŸå¾®æ¼²ä½†ä¿¡å¿ƒä¸è¶³ï¼Œå»ºè­°è§€æœ›"
        else:
            timing_emoji = "âœ…"
            timing_status = "ç¶ ç‡ˆé€šè¡Œ"
            timing_advice = "çŸ­æœŸçœ‹æ¼²ï¼Œå»ºè­°ä»Šæ—¥é€²å ´æ‰£æ¬¾"
    else:
        timing_emoji = "ğŸ›‘"
        timing_status = "ç´…ç‡ˆåœçœ‹è½"
        timing_advice = "çŸ­æœŸçœ‹è·Œï¼Œå»ºè­°è§€æœ›ç­‰å¾…æ›´å¥½æ™‚æ©Ÿ"
    
    return {
        'trend_emoji': trend_emoji,
        'trend_status': trend_status,
        'trend_advice': trend_advice,
        'timing_emoji': timing_emoji,
        'timing_status': timing_status,
        'timing_advice': timing_advice
    }


# =============================================================================
# è¨ˆç®—æœªä¾†äº¤æ˜“æ—¥
# =============================================================================
def get_future_trading_date(start_date: date, trading_days: int) -> date:
    """è¨ˆç®—æœªä¾†ç¬¬ N å€‹äº¤æ˜“æ—¥çš„æ—¥æœŸï¼ˆè·³éé€±æœ«ï¼‰"""
    current_date = start_date
    days_counted = 0
    
    while days_counted < trading_days:
        current_date += timedelta(days=1)
        if current_date.weekday() < 5:
            days_counted += 1
    
    return current_date


# =============================================================================
# ä¿¡å¿ƒåº¦ Emoji
# =============================================================================
def get_confidence_emoji(level: str) -> str:
    """æ ¹æ“šä¿¡å¿ƒåº¦ç­‰ç´šè¿”å› Emoji"""
    if level == "é«˜":
        return "ğŸŸ¢"
    elif level == "ä¸­":
        return "ğŸŸ¡"
    else:
        return "ğŸ”´"


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================
def main():
    print("\n" + "=" * 70)
    print("  ğŸ¤– TWII æŠ•è³‡é¡§å•æ©Ÿå™¨äºº (Trade Advisor)")
    print("  æ™ºæ…§æ•´åˆå¤šæ¨¡å‹ï¼Œç”¢ç”Ÿå‹•æ…‹å®šæœŸå®šé¡æ“ä½œå»ºè­°")
    print("  v2.1 - MC Dropout + è¶¨å‹¢å…±æŒ¯åŠ åˆ†æ©Ÿåˆ¶")
    print("=" * 70)
    
    today = date.today()
    print(f"\nğŸ“… ä»Šæ—¥æ—¥æœŸï¼š{today}")
    
    # =========================================================================
    # 1. æ™ºæ…§é¸æ“‡æœ€ä½³æ¨¡å‹
    # =========================================================================
    print("\n" + "-" * 50)
    print("ğŸ“Š æ¨¡å‹é¸æ“‡")
    print("-" * 50)
    
    # é¸æ“‡ T+1 æ¨¡å‹
    print(f"\n[T+1 æ¨¡å‹] æƒæ {MODELS_DIR_1D}...")
    metadata_1d = select_best_model(MODELS_DIR_1D)
    
    if metadata_1d is None:
        print("\nâŒ ç„¡æ³•è¼‰å…¥ T+1 æ¨¡å‹ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
        print("   è«‹å…ˆåŸ·è¡Œï¼špython twii_model_registry_multivariate.py train ...")
        return
    
    # é¸æ“‡ T+5 æ¨¡å‹
    print(f"\n[T+5 æ¨¡å‹] æƒæ {MODELS_DIR_5D}...")
    metadata_5d = select_best_model(MODELS_DIR_5D)
    
    if metadata_5d is None:
        print("\nâŒ ç„¡æ³•è¼‰å…¥ T+5 æ¨¡å‹ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
        print("   è«‹å…ˆåŸ·è¡Œï¼špython twii_model_registry_5d.py train ...")
        return
    
    # å–å¾—åƒæ•¸
    lookback_1d = metadata_1d.get('lookback', 10)
    lookback_5d = metadata_5d.get('hyperparameters', {}).get('lookback', 
                  metadata_5d.get('lookback', 30))
    
    # å–å¾— RMSE ç”¨æ–¼ 1D ä¿¡å¿ƒåº¦è©•ä¼°
    rmse_1d = metadata_1d.get('metrics', {}).get('rmse', 100.0) or 100.0
    
    r2_1d = metadata_1d.get('metrics', {}).get('r2', 'N/A')
    r2_5d = metadata_5d.get('metrics', {}).get('r2', 'N/A')
    
    print(f"\nâœ… å·²é¸æ“‡æ¨¡å‹ï¼š")
    print(f"  [T+1] {metadata_1d['train_start']} ~ {metadata_1d['train_end']} (RÂ²: {r2_1d}, RMSE: {rmse_1d:.2f})")
    print(f"  [T+5] {metadata_5d['train_start']} ~ {metadata_5d['train_end']} (RÂ²: {r2_5d}, Lookback: {lookback_5d})")
    
    # =========================================================================
    # 2. è¼‰å…¥æ¨¡å‹
    # =========================================================================
    print("\n" + "-" * 50)
    print("ğŸ”§ è¼‰å…¥æ¨¡å‹")
    print("-" * 50)
    
    try:
        model_1d, scaler_feat_1d, scaler_tgt_1d, _ = load_model_artifacts(MODELS_DIR_1D, metadata_1d)
        print(f"  [T+1] æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        
        model_5d, scaler_feat_5d, scaler_tgt_5d, _ = load_model_artifacts(MODELS_DIR_5D, metadata_5d)
        print(f"  [T+5] æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ˆå« Dropout å±¤ï¼‰")
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
        return
    
    # =========================================================================
    # 3. ä¸‹è¼‰ä¸¦è™•ç†å¸‚å ´è³‡æ–™
    # =========================================================================
    print("\n" + "-" * 50)
    print("ğŸ“ˆ å¸‚å ´è³‡æ–™")
    print("-" * 50)
    
    try:
        df_raw = download_market_data(lookback_1d, lookback_5d)
        df_processed = add_technical_indicators(df_raw)
        
        if 'Adj Close' not in df_processed.columns:
            df_processed['Adj Close'] = df_processed['Close']
        
        current_price = df_processed['Adj Close'].iloc[-1]
        last_date = df_processed.index[-1].date()
        
        print(f"  æœ€è¿‘äº¤æ˜“æ—¥ï¼š{last_date}")
        print(f"  ç›®å‰æ”¶ç›¤åƒ¹ï¼š{current_price:.2f}")
    except Exception as e:
        print(f"\nâŒ è³‡æ–™ç²å–å¤±æ•—ï¼š{e}")
        return
    
    # =========================================================================
    # 4. åŸ·è¡Œé æ¸¬ï¼ˆå«ä¿¡å¿ƒåº¦è©•ä¼°ï¼‰
    # =========================================================================
    print("\n" + "-" * 50)
    print("ğŸ”® æ¨¡å‹é æ¸¬ + ä¿¡å¿ƒåº¦è©•ä¼°")
    print("-" * 50)
    
    try:
        # ---------------------------------------------------------------------
        # T+1 é æ¸¬ï¼ˆæ¨™æº–æ¨è«– + RMSE ä¿¡å¿ƒåº¦ï¼‰
        # ---------------------------------------------------------------------
        pred_1d = run_inference(model_1d, scaler_feat_1d, scaler_tgt_1d, df_processed, lookback_1d)
        change_1d = (pred_1d - current_price) / current_price
        date_1d = get_future_trading_date(last_date, 1)
        
        # è©•ä¼° 1D åŸå§‹ä¿¡å¿ƒåº¦
        raw_confidence_1d = evaluate_1d_confidence(pred_1d, current_price, rmse_1d)
        
        # æš«å­˜ï¼Œç¨å¾Œåœ¨ T+5 é æ¸¬å®Œæˆå¾Œé€²è¡Œè¶¨å‹¢å…±æŒ¯èª¿æ•´
        print(f"  [T+1] é æ¸¬ {date_1d}ï¼š{pred_1d:.2f} ({change_1d:+.2%}) | (ä¿¡å¿ƒåº¦ç¨å¾Œè©•ä¼°)")
        
        # ---------------------------------------------------------------------
        # T+5 é æ¸¬ï¼ˆMC Dropout ä¸ç¢ºå®šæ€§è©•ä¼°ï¼‰
        # ---------------------------------------------------------------------
        print(f"  [T+5] åŸ·è¡Œ MC Dropout ({MC_DROPOUT_ITERATIONS} æ¬¡è¿­ä»£)...")
        
        X_5d = prepare_input_data(df_processed, scaler_feat_5d, lookback_5d)
        pred_5d, std_5d, confidence_5d = predict_with_uncertainty(
            model_5d, X_5d, scaler_tgt_5d, MC_DROPOUT_ITERATIONS
        )
        
        change_5d = (pred_5d - current_price) / current_price
        date_5d = get_future_trading_date(last_date, 5)
        conf_emoji_5d = get_confidence_emoji(confidence_5d)
        
        print(f"  [T+5] é æ¸¬ {date_5d}ï¼š{pred_5d:.2f} ({change_5d:+.2%}) | ä¿¡å¿ƒåº¦: {conf_emoji_5d} {confidence_5d}")
        
        # ---------------------------------------------------------------------
        # è¶¨å‹¢å…±æŒ¯ (Trend Alignment) èª¿æ•´ T+1 ä¿¡å¿ƒåº¦
        # ---------------------------------------------------------------------
        confidence_1d, trend_aligned = apply_trend_alignment(
            raw_confidence_1d, change_5d, confidence_5d
        )
        conf_emoji_1d = get_confidence_emoji(confidence_1d)
        
        # ç”¢ç”ŸåŠ åˆ†å‚™è¨»
        trend_bonus_text = " (é †å‹¢åŠ åˆ†)" if trend_aligned else ""
        
        print(f"  [T+1] ä¿¡å¿ƒåº¦è©•ä¼°ï¼š{conf_emoji_1d} {confidence_1d}{trend_bonus_text}")
        print(f"         é¢¨éšªæ³¢å‹• (Std): Â±{std_5d:.2f} é»")
        
    except Exception as e:
        print(f"\nâŒ é æ¸¬å¤±æ•—ï¼š{e}")
        return
    
    # =========================================================================
    # 5. ç”¢ç”ŸæŠ•è³‡å»ºè­°ï¼ˆå«ä¿¡å¿ƒåº¦èª¿æ•´ï¼‰
    # =========================================================================
    advice = generate_advice(change_1d, change_5d, confidence_1d, confidence_5d)
    
    # =========================================================================
    # 6. è¼¸å‡ºå ±è¡¨
    # =========================================================================
    print("\n" + "=" * 70)
    print("  ğŸ“‹ æŠ•è³‡é¡§å•å ±å‘Š")
    print("=" * 70)
    
    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ğŸ“Š æ¨¡å‹å±¥æ­·                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  çŸ­æœŸæ¨¡å‹ (T+1)ï¼š{metadata_1d['train_start']} ~ {metadata_1d['train_end']}                     â”‚
â”‚                  RÂ² = {r2_1d}  |  RMSE = {rmse_1d:<6.2f}                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ³¢æ®µæ¨¡å‹ (T+5)ï¼š{metadata_5d['train_start']} ~ {metadata_5d['train_end']}                     â”‚
â”‚                  RÂ² = {r2_5d}  |  Lookback = {lookback_5d} å¤©                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ğŸ”® é æ¸¬æ•¸æ“š (å«ä¿¡å¿ƒåº¦)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç›®å‰åƒ¹æ ¼ ({last_date})              ï¼š{current_price:>10.2f}                     â”‚
â”‚  T+1 é æ¸¬ ({date_1d})              ï¼š{pred_1d:>10.2f}  ({change_1d:>+6.2%}) {conf_emoji_1d} {confidence_1d}{trend_bonus_text}  â”‚
â”‚  T+5 é æ¸¬ ({date_5d})              ï¼š{pred_5d:>10.2f}  ({change_5d:>+6.2%}) {conf_emoji_5d} {confidence_5d}    â”‚
â”‚      â†’ é¢¨éšªæ³¢å‹• (Std)              ï¼š   Â±{std_5d:<6.2f} é»                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ğŸ’¡ æ“ä½œå»ºè­°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  {advice['trend_emoji']} è³‡é‡‘æ§ç®¡ (5æ—¥è¶¨å‹¢)ï¼š{advice['trend_status']}                              â”‚
â”‚     â†’ {advice['trend_advice']}                       â”‚
â”‚                                                                     â”‚
â”‚  {advice['timing_emoji']} é€²å ´æ™‚æ©Ÿ (1æ—¥è¨Šè™Ÿ)ï¼š{advice['timing_status']}                            â”‚
â”‚     â†’ {advice['timing_advice']}                       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    # ç¶œåˆå»ºè­°ï¼ˆè€ƒé‡ä¿¡å¿ƒåº¦ï¼‰
    print("=" * 70)
    
    # é«˜ä¿¡å¿ƒåº¦æ¢ä»¶ä¸‹çš„ç¶œåˆå»ºè­°
    if change_5d > TREND_BULLISH_THRESHOLD and change_1d > 0:
        if confidence_1d == "ä½" or confidence_5d == "ä½":
            print("  ğŸ¯ ç¶œåˆå»ºè­°ï¼šè¶¨å‹¢æ¨‚è§€ä½†ä¿¡å¿ƒä¸è¶³ï¼Œå»ºè­°ã€Œè¬¹æ…è§€å¯Ÿã€")
        else:
            print("  ğŸ¯ ç¶œåˆå»ºè­°ï¼šå¸‚å ´çŸ­æœŸçœ‹æ¼²ã€ä¸­æœŸæ¨‚è§€ï¼Œå»ºè­°ã€ŒåŠ ç¢¼é€²å ´ã€")
    elif change_5d < TREND_BEARISH_THRESHOLD and change_1d < 0:
        print("  ğŸ¯ ç¶œåˆå»ºè­°ï¼šå¸‚å ´çŸ­æœŸçœ‹è·Œã€ä¸­æœŸæ‚²è§€ï¼Œå»ºè­°ã€Œæš«åœè§€æœ›ã€")
    elif change_5d > TREND_BULLISH_THRESHOLD and change_1d < 0:
        print("  ğŸ¯ ç¶œåˆå»ºè­°ï¼šä¸­æœŸæ¨‚è§€ä½†çŸ­æœŸå›æª”ï¼Œå»ºè­°ã€Œç­‰å¾…ä½æ¥ã€")
    elif change_5d < TREND_BEARISH_THRESHOLD and change_1d > 0:
        print("  ğŸ¯ ç¶œåˆå»ºè­°ï¼šä¸­æœŸæ‚²è§€ä½†çŸ­æœŸåå½ˆï¼Œå»ºè­°ã€Œé€¢é«˜æ¸›ç¢¼ã€")
    else:
        print("  ğŸ¯ ç¶œåˆå»ºè­°ï¼šå¸‚å ´ç›¤æ•´ä¸­ï¼Œå»ºè­°ã€Œç¶­æŒæ¨™æº–å®šæœŸå®šé¡ã€")
    
    print("=" * 70)
    
    print("\nâš ï¸  å…è²¬è²æ˜ï¼šæœ¬å ±å‘Šåƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæŠ•è³‡å»ºè­°ã€‚æŠ•è³‡æœ‰é¢¨éšªï¼Œè«‹è¬¹æ…æ±ºç­–ã€‚\n")


if __name__ == "__main__":
    main()
