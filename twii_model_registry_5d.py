# -*- coding: utf-8 -*-
"""
TWII 5 æ—¥é æ¸¬æ¨¡å‹è¨»å†Šç³»çµ± (5-Day Forecast Model Registry System)
ç‰ˆæœ¬ç®¡ç†èˆ‡è‡ªå‹•æ¨¡å‹é¸æ“‡

åŠŸèƒ½ï¼š
- train æ¨¡å¼ï¼šä½¿ç”¨å¤šè®Šé‡è¼¸å…¥è¨“ç·´ LSTM-SSAM æ¨¡å‹ï¼Œç›´æ¥é æ¸¬ 5 å€‹äº¤æ˜“æ—¥å¾Œçš„æ”¶ç›¤åƒ¹
- predict æ¨¡å¼ï¼šæ™ºæ…§é¸æ“‡åˆé©æ¨¡å‹é€²è¡Œ 5 æ—¥å¾Œé æ¸¬

é æ¸¬ç­–ç•¥ï¼š
- ä½¿ç”¨ Direct Strategyï¼ˆç›´æ¥é æ¸¬æ³•ï¼‰ï¼Œä¸ä½¿ç”¨éè¿´é æ¸¬
- æ¨¡å‹è¼¸å…¥ï¼šéå» 30 å¤©çš„ç‰¹å¾µè³‡æ–™
- æ¨¡å‹è¼¸å‡ºï¼šç¬¬ 5 å€‹äº¤æ˜“æ—¥å¾Œçš„ Adj Close

è¼¸å…¥ç‰¹å¾µ (Features)ï¼š
- Adj Close: èª¿æ•´å¾Œæ”¶ç›¤åƒ¹
- Volume (Log): æˆäº¤é‡ï¼ˆLog è½‰æ›ï¼‰
- K, D: KD æŒ‡æ¨™ï¼ˆ9, 3, 3ï¼‰
- MACD_Hist: MACD æŸ±ç‹€åœ–ï¼ˆ12, 26, 9ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
  è¨“ç·´ï¼špython twii_model_registry_5d.py train --start 2020-01-01 --end 2025-12-05
  é æ¸¬ï¼špython twii_model_registry_5d.py predict
"""

import argparse
import json
import pickle
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Optional, Tuple, Dict, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model

# =============================================================================
# è¨­å®š
# =============================================================================
MODELS_DIR = Path(__file__).parent / "saved_models_5d"  # 5 æ—¥é æ¸¬å°ˆç”¨ç›®éŒ„
LOOKBACK = 30  # å›çœ‹å¤©æ•¸ï¼ˆå¢åŠ ä»¥æ•æ‰æ›´é•·è¶¨å‹¢ï¼‰
FORECAST_HORIZON = 5  # é æ¸¬æœªä¾†ç¬¬ 5 å€‹äº¤æ˜“æ—¥
LSTM_UNITS = 256
DROPOUT_RATE = 0.05  # Dropout æ¯”ç‡ï¼ˆé˜²æ­¢éæ“¬åˆï¼‰
EPOCHS = 50
BATCH_SIZE = 12
TRAIN_RATIO = 0.9
MODEL_STALE_DAYS = 180  # æ¨¡å‹éæœŸè­¦å‘Šé–¾å€¼ï¼ˆå¤©ï¼‰
MIN_TRAIN_DAYS = 1460   # æœ€ä½è¨“ç·´å¤©æ•¸ï¼ˆ4 å¹´ = 4 Ã— 365 = 1460 å¤©ï¼‰

# æŠ€è¡“æŒ‡æ¨™åƒæ•¸
KD_PARAMS = (9, 3, 3)  # (K period, K smooth, D smooth)
MACD_PARAMS = (12, 26, 9)  # (å¿«ç·š, æ…¢ç·š, è¨Šè™Ÿç·š)

# æŠ€è¡“æŒ‡æ¨™è¨ˆç®—æ‰€éœ€çš„æœ€å°è³‡æ–™ç­†æ•¸
MIN_INDICATOR_DAYS = 50  # ä¿å®ˆä¼°è¨ˆï¼Œç¢ºä¿æŒ‡æ¨™ç©©å®š

# ä¸­æ–‡å­—å‹è¨­å®š
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


# =============================================================================
# è‡ªè¨‚ Self-Attention Layer
# =============================================================================
class SelfAttention(layers.Layer):
    """
    Sequential Self-Attention Layer (è«–æ–‡ SSAM æ¶æ§‹)
    """
    
    def __init__(self, **kwargs):
        super(SelfAttention, self).__init__(**kwargs)
    
    def build(self, input_shape):
        self.units = input_shape[-1]
        
        self.W_q = self.add_weight(
            name='W_query',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        self.W_k = self.add_weight(
            name='W_key',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        self.W_v = self.add_weight(
            name='W_value',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        super(SelfAttention, self).build(input_shape)
    
    def call(self, inputs):
        Q = tf.matmul(inputs, self.W_q)
        K = tf.matmul(inputs, self.W_k)
        V = tf.matmul(inputs, self.W_v)
        
        attention_scores = tf.matmul(Q, K, transpose_b=True)
        d_k = tf.cast(self.units, tf.float32)
        attention_scores = attention_scores / tf.math.sqrt(d_k)
        attention_weights = tf.nn.softmax(attention_scores, axis=-1)
        output = tf.matmul(attention_weights, V)
        
        return output
    
    def get_config(self):
        config = super(SelfAttention, self).get_config()
        return config


# =============================================================================
# ç‰¹å¾µå·¥ç¨‹ (Feature Engineering)
# =============================================================================
def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ–°å¢æŠ€è¡“æŒ‡æ¨™åˆ° DataFrame
    
    æ–°å¢æ¬„ä½ï¼š
    - Volume_Log: æˆäº¤é‡ï¼ˆLog è½‰æ›ï¼‰
    - K: KD æŒ‡æ¨™çš„ K å€¼
    - D: KD æŒ‡æ¨™çš„ D å€¼
    - MACD_Hist: MACD æŸ±ç‹€åœ–
    
    Args:
        df: åŸå§‹ OHLCV è³‡æ–™
    
    Returns:
        åŒ…å«æŠ€è¡“æŒ‡æ¨™çš„ DataFrameï¼ˆå·²ç§»é™¤ NaNï¼‰
    """
    df = df.copy()
    
    # -------------------------------------------------------------------------
    # 1. Volume Log è½‰æ›
    # -------------------------------------------------------------------------
    df['Volume_Log'] = np.log1p(df['Volume'])
    
    # -------------------------------------------------------------------------
    # 2. KD æŒ‡æ¨™ (Stochastic Oscillator)
    # åƒæ•¸ï¼š(K period, K smooth, D smooth) = (9, 3, 3)
    # -------------------------------------------------------------------------
    k_period, k_smooth, d_smooth = KD_PARAMS
    
    low_min = df['Low'].rolling(window=k_period).min()
    high_max = df['High'].rolling(window=k_period).max()
    
    raw_k = (df['Close'] - low_min) / (high_max - low_min) * 100
    df['K'] = raw_k.rolling(window=k_smooth).mean()
    df['D'] = df['K'].rolling(window=d_smooth).mean()
    
    # -------------------------------------------------------------------------
    # 3. MACD æŒ‡æ¨™
    # åƒæ•¸ï¼š(å¿«ç·šæœŸæ•¸, æ…¢ç·šæœŸæ•¸, è¨Šè™Ÿç·šæœŸæ•¸) = (12, 26, 9)
    # -------------------------------------------------------------------------
    fast_period, slow_period, signal_period = MACD_PARAMS
    
    ema_fast = df['Close'].ewm(span=fast_period, adjust=False).mean()
    ema_slow = df['Close'].ewm(span=slow_period, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()
    df['MACD_Hist'] = macd_line - signal_line
    
    # -------------------------------------------------------------------------
    # 4. ç§»é™¤ NaN
    # -------------------------------------------------------------------------
    original_len = len(df)
    df = df.dropna()
    removed_len = original_len - len(df)
    
    if removed_len > 0:
        print(f"[ç‰¹å¾µå·¥ç¨‹] å·²ç§»é™¤ {removed_len} ç­†å« NaN çš„è³‡æ–™ï¼ˆæŠ€è¡“æŒ‡æ¨™æš–æ©ŸæœŸï¼‰")
    
    print(f"[ç‰¹å¾µå·¥ç¨‹] æ–°å¢ç‰¹å¾µï¼šVolume_Log, K, D, MACD_Hist")
    print(f"[ç‰¹å¾µå·¥ç¨‹] æœ€çµ‚è³‡æ–™ç­†æ•¸ï¼š{len(df)}")
    
    return df


# =============================================================================
# æ¨¡å‹æ¶æ§‹
# =============================================================================
def build_lstm_ssam_model(
    time_steps: int = LOOKBACK, 
    n_features: int = 5, 
    lstm_units: int = LSTM_UNITS,
    dropout_rate: float = DROPOUT_RATE
):
    """
    å»ºç«‹ LSTM + Dropout + Self-Attention æ··åˆæ¨¡å‹ï¼ˆ5 æ—¥é æ¸¬ç‰ˆæœ¬ï¼‰
    
    æ¶æ§‹ï¼šInput -> LSTM -> Dropout -> Self-Attention -> Flatten -> Dense(1)
    
    Args:
        time_steps: å›çœ‹å¤©æ•¸ï¼ˆé è¨­ 30ï¼‰
        n_features: è¼¸å…¥ç‰¹å¾µæ•¸é‡ï¼ˆé è¨­ 5ï¼‰
        lstm_units: LSTM éš±è—å±¤å–®å…ƒæ•¸
        dropout_rate: Dropout æ¯”ç‡ï¼ˆé˜²æ­¢éæ“¬åˆï¼‰
    
    Returns:
        ç·¨è­¯å¥½çš„ Keras æ¨¡å‹
    """
    inputs = layers.Input(shape=(time_steps, n_features), name='input_layer')
    
    # LSTM å±¤
    lstm_out = layers.LSTM(units=lstm_units, return_sequences=True, name='lstm_layer')(inputs)
    
    # Dropout å±¤ï¼ˆé˜²æ­¢éæ“¬åˆï¼‰
    dropout_out = layers.Dropout(rate=dropout_rate, name='dropout_layer')(lstm_out)
    
    # Self-Attention å±¤
    attention_out = SelfAttention(name='self_attention')(dropout_out)
    
    # è¼¸å‡ºå±¤
    flatten_out = layers.Flatten(name='flatten_layer')(attention_out)
    outputs = layers.Dense(units=1, activation='linear', name='output_layer')(flatten_out)
    
    model = Model(inputs=inputs, outputs=outputs, name='LSTM_SSAM_5Day_Model')
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    
    return model


# =============================================================================
# è³‡æ–™è™•ç†
# =============================================================================

# æœ¬åœ° CSV æª”æ¡ˆè·¯å¾‘
# æ³¨æ„ï¼šCSV ä¸­çš„ volume æ¬„ä½å–®ä½æ˜¯ã€Œå„„å…ƒã€ï¼ˆæˆäº¤é‡‘é¡ / 1e8ï¼‰
CSV_FILE = Path(__file__).parent / "twii_data_from_2000_01_01.csv"


def _load_csv_data() -> pd.DataFrame:
    """
    è¼‰å…¥æœ¬åœ° CSV è³‡æ–™
    
    æ³¨æ„ï¼šCSV ä¸­çš„ volume æ¬„ä½å–®ä½æ˜¯ã€Œå„„å…ƒã€
    
    Returns:
        DataFrame with DatetimeIndex and columns: Open, High, Low, Close, Volume
    """
    if not CSV_FILE.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆï¼š{CSV_FILE}")
    
    df = pd.read_csv(CSV_FILE)
    
    # è½‰æ›æ—¥æœŸæ ¼å¼ (ä¾‹å¦‚: "2025/12/9" -> Timestamp)
    df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d')
    df = df.set_index('date')
    
    # é‡æ–°å‘½åæ¬„ä½ä»¥ç¬¦åˆ yfinance æ ¼å¼
    df = df.rename(columns={
        'open': 'Open',
        'high': 'High',
        'low': 'Low',
        'close': 'Close',
        'volume': 'Volume'  # å–®ä½ï¼šå„„å…ƒ
    })
    
    return df


def _ensure_data_updated() -> None:
    """
    ç¢ºä¿ CSV è³‡æ–™å·²æ›´æ–°åˆ°ä»Šå¤©
    
    é‚è¼¯ï¼š
    1. è®€å– CSV çš„æœ€æ–°æ—¥æœŸ
    2. è‹¥æœ€æ–°æ—¥æœŸ < ä»Šå¤©ï¼Œå‰‡å‘¼å« update_twii_data.py æ›´æ–°
    """
    import subprocess
    import sys
    
    today = date.today()
    
    # è®€å– CSV æœ€æ–°æ—¥æœŸ
    df = _load_csv_data()
    last_date = df.index.max().date()
    
    print(f"[è³‡æ–™æª¢æŸ¥] ä»Šå¤©æ—¥æœŸï¼š{today}")
    print(f"[è³‡æ–™æª¢æŸ¥] CSV æœ€æ–°æ—¥æœŸï¼š{last_date}")
    
    if last_date < today:
        # æª¢æŸ¥ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼ˆé€±ä¸€è‡³é€±äº”ï¼‰
        if today.weekday() >= 5:  # é€±å…­=5, é€±æ—¥=6
            print(f"[è³‡æ–™æª¢æŸ¥] ä»Šå¤©æ˜¯é€±æœ«ï¼Œç„¡éœ€æ›´æ–°")
            return
        
        print(f"[è³‡æ–™æ›´æ–°] CSV è³‡æ–™ä¸æ˜¯æœ€æ–°ï¼Œæ­£åœ¨å‘¼å« update_twii_data.py...")
        
        update_script = Path(__file__).parent / "update_twii_data.py"
        if not update_script.exists():
            print(f"[è­¦å‘Š] æ‰¾ä¸åˆ°æ›´æ–°è…³æœ¬ï¼š{update_script}")
            print(f"[è­¦å‘Š] å°‡ä½¿ç”¨ç¾æœ‰è³‡æ–™ç¹¼çºŒ...")
            return
        
        result = subprocess.run(
            [sys.executable, str(update_script)],
            cwd=Path(__file__).parent,
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print(f"[è³‡æ–™æ›´æ–°] æ›´æ–°å®Œæˆï¼")
        else:
            print(f"[è­¦å‘Š] æ›´æ–°è…³æœ¬åŸ·è¡Œå¤±æ•—ï¼š{result.stderr}")
            print(f"[è­¦å‘Š] å°‡ä½¿ç”¨ç¾æœ‰è³‡æ–™ç¹¼çºŒ...")
    else:
        print(f"[è³‡æ–™æª¢æŸ¥] CSV è³‡æ–™å·²æ˜¯æœ€æ–°")


def load_data_by_date_range(start_date: str, end_date: str) -> pd.DataFrame:
    """
    å¾æœ¬åœ° CSV è¼‰å…¥æŒ‡å®šæ—¥æœŸç¯„åœçš„ TWII è³‡æ–™
    
    æ³¨æ„ï¼šä¸å†å¾ yfinance ä¸‹è¼‰ï¼Œæ”¹ç”¨æœ¬åœ° CSV æª”æ¡ˆ
    volume æ¬„ä½å–®ä½ç‚ºã€Œå„„å…ƒã€
    
    Args:
        start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        DataFrame with OHLCV data
    """
    print(f"[è³‡æ–™ç²å–] å¾æœ¬åœ° CSV è¼‰å…¥ TWII è³‡æ–™ ({start_date} ~ {end_date})...")
    
    # ç¢ºä¿è³‡æ–™å·²æ›´æ–°
    _ensure_data_updated()
    
    # é‡æ–°è¼‰å…¥ CSVï¼ˆå¯èƒ½å·²è¢«æ›´æ–°ï¼‰
    df = _load_csv_data()
    
    # éæ¿¾æ—¥æœŸç¯„åœ
    start_dt = pd.Timestamp(start_date)
    end_dt = pd.Timestamp(end_date)
    
    df = df[(df.index >= start_dt) & (df.index <= end_dt)]
    
    if df.empty:
        raise ValueError(f"åœ¨ CSV ä¸­æ‰¾ä¸åˆ° {start_date} ~ {end_date} ç¯„åœçš„è³‡æ–™")
    
    print(f"[è³‡æ–™ç²å–] æˆåŠŸè¼‰å…¥ {len(df)} ç­†è³‡æ–™")
    print(f"[è³‡æ–™ç²å–] å¯¦éš›æœŸé–“ï¼š{df.index[0].strftime('%Y-%m-%d')} ~ {df.index[-1].strftime('%Y-%m-%d')}")
    
    return df


def load_recent_data(lookback_days: int = 60) -> pd.DataFrame:
    """
    å¾æœ¬åœ° CSV è¼‰å…¥æœ€è¿‘çš„è³‡æ–™ç”¨æ–¼é æ¸¬
    
    æ³¨æ„ï¼šä¸å†å¾ yfinance ä¸‹è¼‰ï¼Œæ”¹ç”¨æœ¬åœ° CSV æª”æ¡ˆ
    volume æ¬„ä½å–®ä½ç‚ºã€Œå„„å…ƒã€
    
    Args:
        lookback_days: éœ€è¦çš„å›çœ‹å¤©æ•¸
    
    Returns:
        DataFrame with OHLCV data
    """
    # è¨ˆç®—éœ€è¦çš„ç¸½è³‡æ–™é‡ï¼šlookback + æŠ€è¡“æŒ‡æ¨™æš–æ©ŸæœŸ
    required_days = lookback_days + MIN_INDICATOR_DAYS
    
    print(f"[è³‡æ–™ç²å–] å¾æœ¬åœ° CSV è¼‰å…¥æœ€è¿‘ {required_days} å¤©çš„è³‡æ–™ï¼ˆå«æŠ€è¡“æŒ‡æ¨™æš–æ©ŸæœŸï¼‰...")
    
    # ç¢ºä¿è³‡æ–™å·²æ›´æ–°
    _ensure_data_updated()
    
    # é‡æ–°è¼‰å…¥ CSVï¼ˆå¯èƒ½å·²è¢«æ›´æ–°ï¼‰
    df = _load_csv_data()
    
    # å–æœ€å¾Œ required_days ç­†è³‡æ–™
    df = df.tail(required_days)
    
    if len(df) < required_days:
        print(f"[è­¦å‘Š] CSV è³‡æ–™ä¸è¶³ {required_days} ç­†ï¼Œå¯¦éš›å–å¾— {len(df)} ç­†")
    
    print(f"[è³‡æ–™ç²å–] æˆåŠŸè¼‰å…¥ {len(df)} ç­†è³‡æ–™")
    
    return df


# ç‚ºäº†å‘å¾Œç›¸å®¹ï¼Œä¿ç•™èˆŠå‡½æ•¸åç¨±ï¼ˆä½†æ¨™è¨˜ç‚ºæ£„ç”¨ï¼‰
def download_data_by_date_range(start_date: str, end_date: str) -> pd.DataFrame:
    """[å·²æ£„ç”¨] è«‹ä½¿ç”¨ load_data_by_date_range()"""
    return load_data_by_date_range(start_date, end_date)


def download_recent_data(lookback_days: int = 60) -> pd.DataFrame:
    """[å·²æ£„ç”¨] è«‹ä½¿ç”¨ load_recent_data()"""
    return load_recent_data(lookback_days)


def get_feature_columns() -> list:
    """å–å¾—ç‰¹å¾µæ¬„ä½åç¨±"""
    return ['Adj Close', 'Volume_Log', 'K', 'D', 'MACD_Hist']


def preprocess_for_training(df: pd.DataFrame, lookback: int = LOOKBACK, forecast_horizon: int = FORECAST_HORIZON, train_ratio: float = TRAIN_RATIO):
    """
    è¨“ç·´ç”¨è³‡æ–™é è™•ç†ï¼ˆ5 æ—¥é æ¸¬ç‰ˆæœ¬ - Direct Strategyï¼‰
    
    [ä¿®æ­£] é˜²æ­¢è³‡æ–™æ´©æ¼ (Data Leakage Fix):
    - èˆŠé‚è¼¯ï¼šå…ˆå…¨éƒ¨è³‡æ–™ fit_transform -> å†åˆ‡åˆ† (æ¨¡å‹å·çœ‹æœªä¾†é«˜ä½é»)
    - æ–°é‚è¼¯ï¼šå…ˆåˆ‡åˆ† -> åªç”¨ Train Set fit -> å† transform å…¨é«”
    
    è³‡æ–™å°é½Šé‚è¼¯ï¼ˆDirect Strategyï¼‰ï¼š
    - Input: [X_t-lookback, ..., X_t]
    - Target: Y_{t+forecast_horizon} (ä¾‹å¦‚ 5 å¤©å¾Œçš„æ”¶ç›¤åƒ¹)
    
    æ³¨æ„ï¼šé€™æœƒå°è‡´æœ€å¾Œ forecast_horizon å¤©æ²’æœ‰å°æ‡‰çš„ targetï¼Œéœ€æ¨æ£„
    
    Returns:
        X_train, y_train, X_test, y_test, feature_scaler, target_scaler, price_min, price_max, n_features
    """
    # 1. æ–°å¢æŠ€è¡“æŒ‡æ¨™
    df = add_technical_indicators(df)
    
    # 2. ç¢ºä¿æœ‰ Adj Close æ¬„ä½
    if 'Adj Close' not in df.columns:
        df['Adj Close'] = df['Close']
        print("[é è™•ç†] ä½¿ç”¨ Close æ¬„ä½ä½œç‚º Adj Close")
    
    # 3. æº–å‚™ç‰¹å¾µæ¬„ä½
    feature_columns = get_feature_columns()
    
    for col in feature_columns:
        if col not in df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{col}")
            
    # å–å‡ºåŸå§‹æ•¸æ“š
    raw_features = df[feature_columns].values
    raw_target = df['Adj Close'].values.reshape(-1, 1)
    
    n_features = len(feature_columns)
    total_len = len(df)
    
    # -------------------------------------------------------------------------
    # [é—œéµä¿®æ­£] æ­£ç¢ºçš„åˆ‡åˆ†é»è¨ˆç®—
    # ç”±æ–¼ sequences æ˜¯å¾ lookback é–‹å§‹ï¼Œä¸”å—é™äº forecast_horizon
    # æˆ‘å€‘éœ€è¦åœ¨ã€Œæ™‚é–“è»¸ã€ä¸Šåˆ‡åˆ†ï¼Œç¢ºä¿ scaler åªçœ‹åˆ°éå»çš„æ•¸æ“š
    # -------------------------------------------------------------------------
    
    # è¨ˆç®—è¨“ç·´é›†å¤§å°ï¼ˆåŸºæ–¼åŸå§‹æ•¸æ“šé•·åº¦ï¼Œæœªè€ƒæ…®åºåˆ—åŒ–æå¤±ï¼‰
    split_idx = int(total_len * train_ratio)
    
    # å¦‚æœåˆ‡åˆ†é»å¤ªå°å°è‡´ç„¡æ³•å½¢æˆè¶³å¤  lookbackï¼Œå‰‡å¼·åˆ¶èª¿æ•´
    if split_idx <= lookback + forecast_horizon:
        raise ValueError(f"è³‡æ–™é‡ä¸è¶³æˆ– train_ratio å¤ªå°ï¼Œç„¡æ³•å»ºç«‹è¨“ç·´é›†")
    
    print(f"[é è™•ç†] è³‡æ–™åˆ‡åˆ†é»ï¼šIndex {split_idx} (Date: {df.index[split_idx].date()})")
    
    # 4. å»ºç«‹é›™ç¸®æ”¾å™¨ (åªç”¨è¨“ç·´é›† Fit)
    train_features = raw_features[:split_idx]
    train_target = raw_target[:split_idx]
    
    feature_scaler = MinMaxScaler(feature_range=(0, 1))
    target_scaler = MinMaxScaler(feature_range=(0, 1))
    
    # Fit (å­¸ç¿’è¨“ç·´é›†çš„ Min/Max)
    feature_scaler.fit(train_features)
    target_scaler.fit(train_target)
    
    # Transform (è½‰æ›æ•´ä»½è³‡æ–™)
    scaled_features = feature_scaler.transform(raw_features)
    scaled_target = target_scaler.transform(raw_target)
    
    # 5. å»ºç«‹æ™‚åºè³‡æ–™é›† (Direct Strategy)
    X, y = [], []
    
    # é€™è£¡çš„ç¯„åœéœ€è¦èª¿æ•´ï¼Œç¢ºä¿ t+forecast_horizon ä¸æœƒè¶Šç•Œ
    # æ¨£æœ¬ i ä»£è¡¨æ™‚é–“ t
    # Input: features[i-lookback : i]
    # Target: target[i + forecast_horizon] 
    # (è¨»ï¼šå¦‚æœæ˜¯é æ¸¬ç¬¬5å¤©ï¼Œindex offset æ‡‰ç‚º 5ï¼Œå‡è¨­ t=i æ˜¯ç¬¬0å¤©ï¼Œå‰‡ t+5 æ˜¯ i+5)
    
    # ä¿®æ­£è¿´åœˆç¯„åœï¼š
    # start: lookback (ç¬¬ä¸€ç­†è¼¸å…¥éœ€è¦å‰ lookback å¤©)
    # end: len - forecast_horizon (ç¢ºä¿æœ‰æœªä¾†çš„ç›®æ¨™å€¼)
    for i in range(lookback, len(scaled_features) - forecast_horizon):
        # X: éå» lookback å¤©çš„ç‰¹å¾µ
        X.append(scaled_features[i - lookback:i])
        
        # y: æœªä¾†ç¬¬ forecast_horizon å¤©çš„ç›®æ¨™åƒ¹
        # ä¾‹å¦‚ i=30, forecast=5, target=raw_target[35] (ä»£è¡¨ç¬¬35å¤©çš„åƒ¹æ ¼)
        y.append(scaled_target[i + forecast_horizon, 0])
    
    X, y = np.array(X), np.array(y)
    
    # 6. åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›† (åŸºæ–¼ç”Ÿæˆçš„ X åºåˆ—)
    # ç”±æ–¼ sequence ç”Ÿæˆé€ æˆäº† shiftï¼Œæˆ‘å€‘éœ€è¦é‡æ–°è¨ˆç®— split point
    # åŸå§‹è³‡æ–™ split_idx ä»£è¡¨è¨“ç·´è³‡æ–™çµæŸçš„æ™‚é–“é» t
    # å°æ‡‰çš„åºåˆ— Index æ‡‰è©²æ˜¯ split_idx - lookback
    
    train_size = split_idx - lookback
    
    # å®‰å…¨æª¢æŸ¥ç¢ºä¿ train_size åˆç†
    if train_size >= len(X):
        train_size = int(len(X) * 0.8)
        print(f"[è­¦å‘Š] è¨ˆç®—å‡ºçš„ train_size è¶…å‡ºç¯„åœï¼Œå›é€€è‡³æ¨™æº– 80% åˆ‡åˆ†")
    
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # 7. è¨˜éŒ„åƒ¹æ ¼ç¯„åœï¼ˆåªè¨˜éŒ„è¨“ç·´é›†çš„ç¯„åœï¼‰
    price_min = float(df['Adj Close'].iloc[:split_idx].min())
    price_max = float(df['Adj Close'].iloc[:split_idx].max())
    
    print(f"[é è™•ç†] è¼¸å…¥å½¢ç‹€ï¼š{X_train.shape} (samples, time_steps, n_features)")
    print(f"[é è™•ç†] ç‰¹å¾µæ•¸é‡ï¼š{n_features} ({', '.join(feature_columns)})")
    print(f"[é è™•ç†] é æ¸¬ç›®æ¨™ï¼šæœªä¾†ç¬¬ {forecast_horizon} å€‹äº¤æ˜“æ—¥")
    print(f"[é è™•ç†] è¨“ç·´é›†ï¼š{len(X_train)} ç­† | æ¸¬è©¦é›†ï¼š{len(X_test)} ç­†")
    print(f"[é è™•ç†] è¨“ç·´é›†åƒ¹æ ¼ç¯„åœï¼š{price_min:.2f} ~ {price_max:.2f}")
    
    return X_train, y_train, X_test, y_test, feature_scaler, target_scaler, price_min, price_max, n_features


def preprocess_for_prediction(
    df: pd.DataFrame, 
    feature_scaler: MinMaxScaler, 
    lookback: int = LOOKBACK
) -> Tuple[np.ndarray, pd.DataFrame]:
    """
    é æ¸¬ç”¨è³‡æ–™é è™•ç†
    
    Returns:
        X: æ¨¡å‹è¼¸å…¥ shape (1, lookback, n_features)
        df_processed: è™•ç†å¾Œçš„ DataFrame
    """
    df_processed = add_technical_indicators(df)
    
    if 'Adj Close' not in df_processed.columns:
        df_processed['Adj Close'] = df_processed['Close']
    
    feature_columns = get_feature_columns()
    features = df_processed[feature_columns].values
    
    scaled_features = feature_scaler.transform(features)
    
    if len(scaled_features) < lookback:
        raise ValueError(f"è³‡æ–™ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {lookback} ç­†è³‡æ–™ï¼Œç›®å‰åªæœ‰ {len(scaled_features)} ç­†")
    
    X = scaled_features[-lookback:].reshape(1, lookback, len(feature_columns))
    
    return X, df_processed


# =============================================================================
# è¨“ç·´çµæœè¦–è¦ºåŒ–
# =============================================================================
def plot_training_results(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    start_date: str,
    end_date: str,
    rmse: float,
    r2: float
) -> Path:
    """ç¹ªè£½è¨“ç·´çµæœè¦–è¦ºåŒ–åœ–è¡¨"""
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    
    fig, ax = plt.subplots(figsize=(14, 6))
    
    x_axis = range(len(y_true))
    ax.plot(x_axis, y_true, label='Actual (T+5)', color='blue', linewidth=1.5, alpha=0.8)
    ax.plot(x_axis, y_pred, label='Predicted (T+5)', color='red', linewidth=1.5, alpha=0.8)
    
    title = f"TWII 5-Day Forecast ({start_date} ~ {end_date}) | RÂ²: {r2:.4f} | RMSE: {rmse:.2f}"
    ax.set_title(title, fontsize=14, fontweight='bold')
    
    ax.set_xlabel('æ¸¬è©¦é›†æ¨£æœ¬ç´¢å¼•', fontsize=12)
    ax.set_ylabel('åƒ¹æ ¼ (Price)', fontsize=12)
    ax.legend(loc='upper left', fontsize=11)
    ax.grid(True, alpha=0.3)
    
    textstr = f'RÂ² = {r2:.4f}\nRMSE = {rmse:.2f}\n5æ—¥ç›´æ¥é æ¸¬'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
    ax.text(0.97, 0.05, textstr, transform=ax.transAxes, fontsize=11,
            verticalalignment='bottom', horizontalalignment='right', bbox=props)
    
    plt.tight_layout()
    
    plot_path = MODELS_DIR / f"plot_{start_date}_{end_date}.png"
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    plt.close(fig)
    
    print(f"[è¦–è¦ºåŒ–] è¨“ç·´çµæœåœ–è¡¨å·²å„²å­˜è‡³ï¼š{plot_path}")
    
    return plot_path


# =============================================================================
# æ¨¡å‹æˆå“ç®¡ç†
# =============================================================================
def get_artifact_paths(start_date: str, end_date: str) -> Tuple[Path, Path, Path, Path]:
    """å–å¾—æ¨¡å‹æˆå“è·¯å¾‘"""
    model_path = MODELS_DIR / f"model_{start_date}_{end_date}.keras"
    feature_scaler_path = MODELS_DIR / f"feature_scaler_{start_date}_{end_date}.pkl"
    target_scaler_path = MODELS_DIR / f"target_scaler_{start_date}_{end_date}.pkl"
    meta_path = MODELS_DIR / f"meta_{start_date}_{end_date}.json"
    return model_path, feature_scaler_path, target_scaler_path, meta_path


def save_artifacts(
    model,
    feature_scaler: MinMaxScaler,
    target_scaler: MinMaxScaler,
    start_date: str,
    end_date: str,
    price_min: float,
    price_max: float,
    n_features: int,
    rmse: float = None,
    r2: float = None,
    lookback: int = LOOKBACK,
    forecast_horizon: int = FORECAST_HORIZON
):
    """å„²å­˜æ¨¡å‹æˆå“"""
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    
    model_path, feature_scaler_path, target_scaler_path, meta_path = get_artifact_paths(start_date, end_date)
    
    model.save(model_path)
    print(f"[å„²å­˜] æ¨¡å‹å·²å„²å­˜è‡³ï¼š{model_path}")
    
    with open(feature_scaler_path, 'wb') as f:
        pickle.dump(feature_scaler, f)
    print(f"[å„²å­˜] ç‰¹å¾µç¸®æ”¾å™¨å·²å„²å­˜è‡³ï¼š{feature_scaler_path}")
    
    with open(target_scaler_path, 'wb') as f:
        pickle.dump(target_scaler, f)
    print(f"[å„²å­˜] ç›®æ¨™ç¸®æ”¾å™¨å·²å„²å­˜è‡³ï¼š{target_scaler_path}")
    
    metadata = {
        "model_type": "5day_direct",
        "train_start": start_date,
        "train_end": end_date,
        "lookback": lookback,
        "forecast_horizon": forecast_horizon,
        "n_features": n_features,
        "feature_columns": get_feature_columns(),
        "price_min": price_min,
        "price_max": price_max,
        "dropout_rate": DROPOUT_RATE,
        "lstm_units": LSTM_UNITS,
        "batch_size": BATCH_SIZE,
        "training_timestamp": datetime.now().isoformat(),
        "technical_indicators": {
            "kd_params": list(KD_PARAMS),
            "macd_params": list(MACD_PARAMS)
        },
        "metrics": {
            "rmse": round(rmse, 2) if rmse is not None else None,
            "r2": round(r2, 4) if r2 is not None else None
        }
    }
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"[å„²å­˜] å…ƒè³‡æ–™å·²å„²å­˜è‡³ï¼š{meta_path}")


def load_artifacts(start_date: str, end_date: str) -> Tuple[Model, MinMaxScaler, MinMaxScaler, Dict[str, Any]]:
    """è¼‰å…¥æ¨¡å‹æˆå“"""
    model_path, feature_scaler_path, target_scaler_path, meta_path = get_artifact_paths(start_date, end_date)
    
    model = keras.models.load_model(
        model_path,
        custom_objects={'SelfAttention': SelfAttention}
    )
    
    with open(feature_scaler_path, 'rb') as f:
        feature_scaler = pickle.load(f)
    
    with open(target_scaler_path, 'rb') as f:
        target_scaler = pickle.load(f)
    
    with open(meta_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    return model, feature_scaler, target_scaler, metadata


# =============================================================================
# æ™ºæ…§æ¨¡å‹é¸æ“‡
# =============================================================================
def select_best_model(target_date: date) -> Optional[Dict[str, Any]]:
    """æ™ºæ…§é¸æ“‡æœ€é©åˆçš„æ¨¡å‹"""
    if not MODELS_DIR.exists():
        print("[æœå°‹] æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨")
        return None
    
    meta_files = list(MODELS_DIR.glob("meta_*.json"))
    if not meta_files:
        print("[æœå°‹] æ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹æª”æ¡ˆ")
        return None
    
    candidates = []
    
    for meta_file in meta_files:
        try:
            with open(meta_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            train_start = datetime.strptime(metadata['train_start'], '%Y-%m-%d').date()
            train_end = datetime.strptime(metadata['train_end'], '%Y-%m-%d').date()
            
            duration_days = (train_end - train_start).days
            model_name = f"model_{metadata['train_start']}_{metadata['train_end']}"
            
            if duration_days < MIN_TRAIN_DAYS:
                print(f"[ç•¥é] æ¨¡å‹ {model_name} è¨“ç·´å¤©æ•¸ {duration_days} å¤©ä¸è¶³ 4 å¹´ ({MIN_TRAIN_DAYS} å¤©)")
                continue
            
            if train_end < target_date:
                candidates.append({
                    'metadata': metadata,
                    'train_start': train_start,
                    'train_end': train_end,
                    'duration_days': duration_days,
                    'gap_days': (target_date - train_end).days,
                    'model_name': model_name,
                    'r2': metadata.get('metrics', {}).get('r2', 0.0) or 0.0
                })
        except Exception as e:
            print(f"[è­¦å‘Š] ç„¡æ³•è§£æ {meta_file}: {e}")
            continue
    
    if not candidates:
        print(f"[æœå°‹] æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ¨¡å‹ï¼ˆtrain_end < {target_date}ï¼‰")
        return None
    
    candidates.sort(key=lambda x: (x['train_end'], x['r2'], x['train_start']), reverse=True)
    
    print(f"\n[æœå°‹] æ‰¾åˆ° {len(candidates)} å€‹å¯ç”¨æ¨¡å‹ï¼š")
    for i, c in enumerate(candidates):
        r2_display = f"RÂ²: {c['r2']:.4f}" if c['r2'] else "RÂ²: N/A"
        status = "Selected (Best Match)" if i == 0 else "Backup"
        print(f"  {i+1}. {c['model_name']} ({r2_display}) -> {status}")
    
    return candidates[0]['metadata']


def validate_model(metadata: Dict[str, Any], target_date: date, current_price: Optional[float] = None):
    """é©—è­‰æ¨¡å‹ä¸¦ç™¼å‡ºè­¦å‘Š"""
    train_end = datetime.strptime(metadata['train_end'], '%Y-%m-%d').date()
    gap_days = (target_date - train_end).days
    
    if gap_days > MODEL_STALE_DAYS:
        print(f"\nâš ï¸ è­¦å‘Šï¼šé¸æ“‡çš„æ¨¡å‹å·²è¨“ç·´è¶…é {MODEL_STALE_DAYS} å¤©ï¼ˆè·ä»Š {gap_days} å¤©ï¼‰ï¼Œå»ºè­°é‡æ–°è¨“ç·´ã€‚")
    
    if current_price is not None:
        price_min = metadata.get('price_min', 0)
        price_max = metadata.get('price_max', float('inf'))
        
        if current_price < price_min or current_price > price_max:
            print(f"\nâš ï¸ è­¦å‘Šï¼šç•¶å‰åƒ¹æ ¼ {current_price:.2f} è¶…å‡ºè¨“ç·´æ™‚çš„åƒ¹æ ¼ç¯„åœ [{price_min:.2f}, {price_max:.2f}]")


# =============================================================================
# è¨ˆç®—æœªä¾†äº¤æ˜“æ—¥
# =============================================================================
def get_future_trading_date(start_date: date, trading_days: int) -> date:
    """
    è¨ˆç®—æœªä¾†ç¬¬ N å€‹äº¤æ˜“æ—¥çš„æ—¥æœŸï¼ˆè·³éé€±æœ«ï¼‰
    
    Args:
        start_date: èµ·å§‹æ—¥æœŸ
        trading_days: è¦å‰é€²çš„äº¤æ˜“æ—¥æ•¸
    
    Returns:
        æœªä¾†ç¬¬ N å€‹äº¤æ˜“æ—¥çš„æ—¥æœŸ
    """
    current_date = start_date
    days_counted = 0
    
    while days_counted < trading_days:
        current_date += timedelta(days=1)
        # é€±ä¸€åˆ°é€±äº”æ‰ç®—äº¤æ˜“æ—¥
        if current_date.weekday() < 5:
            days_counted += 1
    
    return current_date


# =============================================================================
# è¨“ç·´æ¨¡å¼
# =============================================================================
def train_mode(args):
    """è¨“ç·´æ¨¡å¼"""
    print("\n" + "=" * 60)
    print("  TWII 5 æ—¥é æ¸¬æ¨¡å‹è¨»å†Šç³»çµ± - è¨“ç·´æ¨¡å¼")
    print("  (Direct Strategy - ç›´æ¥é æ¸¬æ³•)")
    print("=" * 60)
    
    start_date = args.start
    end_date = args.end
    
    print(f"\n[è¨­å®š] è¨“ç·´æœŸé–“ï¼š{start_date} ~ {end_date}")
    print(f"[è¨­å®š] Lookback: {LOOKBACK} | Forecast Horizon: {FORECAST_HORIZON}")
    print(f"[è¨­å®š] LSTM Units: {LSTM_UNITS} | Epochs: {EPOCHS} | Batch Size: {BATCH_SIZE}")
    print(f"[è¨­å®š] KD åƒæ•¸: {KD_PARAMS} | MACD åƒæ•¸: {MACD_PARAMS}")
    print(f"[è¨­å®š] Split Ratio: {args.split_ratio} (è¨“ç·´é›†æ¯”ä¾‹)")
    
    np.random.seed(42)
    tf.random.set_seed(42)
    
    # 1. ä¸‹è¼‰è³‡æ–™
    df = download_data_by_date_range(start_date, end_date)
    
    # 2. é è™•ç†ï¼ˆDirect Strategyï¼‰- ä½¿ç”¨æŒ‡å®šçš„ split_ratio
    X_train, y_train, X_test, y_test, feature_scaler, target_scaler, price_min, price_max, n_features = preprocess_for_training(df, train_ratio=args.split_ratio)
    
    # 3. å»ºç«‹æ¨¡å‹
    print("\n[æ¨¡å‹] å»ºç«‹ LSTM-SSAM 5 æ—¥é æ¸¬æ¨¡å‹...")
    model = build_lstm_ssam_model(time_steps=LOOKBACK, n_features=n_features)
    model.summary()
    
    # 4. è¨“ç·´
    print(f"\n[è¨“ç·´] é–‹å§‹è¨“ç·´...")
    early_stop = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
    
    model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        callbacks=[early_stop],
        verbose=1
    )
    
    # 5. è©•ä¼°
    print("\n[è©•ä¼°] è¨ˆç®—æ¸¬è©¦é›†æŒ‡æ¨™...")
    y_pred_scaled = model.predict(X_test, verbose=0)
    
    y_actual = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
    y_predicted = target_scaler.inverse_transform(y_pred_scaled).flatten()
    
    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))
    r2 = r2_score(y_actual, y_predicted)
    
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¨¡å‹è©•ä¼°çµæœ (5 æ—¥ç›´æ¥é æ¸¬)")
    print("=" * 50)
    print(f"  RMSE (å‡æ–¹æ ¹èª¤å·®)  : {rmse:.2f} é»")
    print(f"  RÂ² Score (æ±ºå®šä¿‚æ•¸): {r2:.4f}")
    print(f"  é æ¸¬ç›®æ¨™           : æœªä¾†ç¬¬ {FORECAST_HORIZON} å€‹äº¤æ˜“æ—¥")
    print("=" * 50)
    
    # 6. å„²å­˜æˆå“
    save_artifacts(
        model, feature_scaler, target_scaler, 
        start_date, end_date, 
        price_min, price_max, n_features,
        rmse=rmse, r2=r2
    )
    
    # 7. ç¹ªè£½è¨“ç·´çµæœåœ–è¡¨
    plot_training_results(y_actual, y_predicted, start_date, end_date, rmse, r2)
    
    print("\nâœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹æˆå“å·²å„²å­˜è‡³ saved_models_5d/ ç›®éŒ„")


# =============================================================================
# é æ¸¬æ¨¡å¼
# =============================================================================
def predict_mode(args):
    """é æ¸¬æ¨¡å¼ - ç›´æ¥é æ¸¬ 5 å€‹äº¤æ˜“æ—¥å¾Œçš„æ”¶ç›¤åƒ¹"""
    print("\n" + "=" * 60)
    print("  TWII 5 æ—¥é æ¸¬æ¨¡å‹è¨»å†Šç³»çµ± - é æ¸¬æ¨¡å¼")
    print("  (Direct Strategy - ç›´æ¥é æ¸¬æ³•)")
    print("=" * 60)
    
    # è¨ˆç®—ä»Šå¤©å’Œ 5 å€‹äº¤æ˜“æ—¥å¾Œçš„æ—¥æœŸ
    today = date.today()
    target_date = get_future_trading_date(today, FORECAST_HORIZON)
    
    print(f"\n[è¨­å®š] ä»Šæ—¥æ—¥æœŸï¼š{today}")
    print(f"[è¨­å®š] é æ¸¬ç›®æ¨™ï¼šæœªä¾†ç¬¬ {FORECAST_HORIZON} å€‹äº¤æ˜“æ—¥ ({target_date})")
    
    # é¸æ“‡æœ€ä½³æ¨¡å‹ï¼ˆåŸºæ–¼ä»Šå¤©çš„æ—¥æœŸï¼‰
    print("\n[æœå°‹] æ­£åœ¨æœå°‹åˆé©çš„æ¨¡å‹...")
    metadata = select_best_model(today)
    
    if metadata is None:
        print(f"\nâŒ æ‰¾ä¸åˆ°é©åˆçš„æ­·å²æ¨¡å‹ã€‚")
        print("   è«‹å…ˆè¨“ç·´æ¨¡å‹ã€‚")
        print(f"   ç¯„ä¾‹ï¼špython {Path(__file__).name} train --start 2020-01-01 --end {today - timedelta(days=1)}")
        return
    
    train_start = metadata['train_start']
    train_end = metadata['train_end']
    lookback = metadata.get('lookback', LOOKBACK)
    forecast_horizon = metadata.get('forecast_horizon', FORECAST_HORIZON)
    
    print(f"\nâœ… ä½¿ç”¨æ¨¡å‹ç‰ˆæœ¬ï¼šè¨“ç·´æœŸé–“ {train_start} è‡³ {train_end}")
    print(f"   æ¨¡å‹é¡å‹ï¼š{forecast_horizon} æ—¥ç›´æ¥é æ¸¬")
    print(f"   ç‰¹å¾µæ¬„ä½ï¼š{metadata.get('feature_columns', get_feature_columns())}")
    
    # è¼‰å…¥æ¨¡å‹æˆå“
    print("\n[è¼‰å…¥] æ­£åœ¨è¼‰å…¥æ¨¡å‹å’Œç¸®æ”¾å™¨...")
    model, feature_scaler, target_scaler, metadata = load_artifacts(train_start, train_end)
    
    # ä¸‹è¼‰æœ€è¿‘è³‡æ–™
    df = download_recent_data(lookback_days=lookback + 20)
    
    # é è™•ç†
    X, df_processed = preprocess_for_prediction(df, feature_scaler, lookback)
    
    current_price = df_processed['Adj Close'].iloc[-1]
    last_data_date = df_processed.index[-1].date()
    
    # é©—è­‰æ¨¡å‹
    validate_model(metadata, today, current_price)
    
    # ç›´æ¥é æ¸¬ï¼ˆç„¡éœ€éè¿´è¿´åœˆï¼‰
    print(f"\n[é æ¸¬] æœ€è¿‘è³‡æ–™æ—¥æœŸï¼š{last_data_date}")
    print(f"[é æ¸¬] ä½¿ç”¨éå» {lookback} å¤©è³‡æ–™é€²è¡Œå–®æ¬¡é æ¸¬")
    
    y_pred_scaled = model.predict(X, verbose=0)
    predicted_price = target_scaler.inverse_transform(y_pred_scaled)[0, 0]
    
    # è¨ˆç®—é æ¸¬ç›®æ¨™æ—¥æœŸï¼ˆå¾æœ€å¾Œè³‡æ–™æ—¥é–‹å§‹ç®— 5 å€‹äº¤æ˜“æ—¥ï¼‰
    predicted_date = get_future_trading_date(last_data_date, forecast_horizon)
    
    # è¨ˆç®—æ¼²è·Œå¹…
    price_change = predicted_price - current_price
    price_change_pct = (price_change / current_price) * 100
    trend = "ğŸ“ˆ çœ‹æ¼²" if price_change > 0 else "ğŸ“‰ çœ‹è·Œ"
    
    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 55)
    print(f"ğŸ”® TWII 5 æ—¥é æ¸¬çµæœ")
    print("=" * 55)
    print(f"  æœ€è¿‘æ”¶ç›¤åƒ¹ ({last_data_date})     : {current_price:.2f}")
    print(f"  é æ¸¬åƒ¹æ ¼   ({predicted_date}) : {predicted_price:.2f}")
    print(f"  é æœŸè®ŠåŒ–                        : {price_change:+.2f} ({price_change_pct:+.2f}%)")
    print(f"  è¶¨å‹¢åˆ¤æ–·                        : {trend}")
    print("=" * 55)
    print(f"  é æ¸¬ç­–ç•¥   : Direct Strategyï¼ˆç›´æ¥é æ¸¬ï¼‰")
    print(f"  é æ¸¬ç¯„åœ   : æœªä¾†ç¬¬ {forecast_horizon} å€‹äº¤æ˜“æ—¥")
    print(f"  ä½¿ç”¨æ¨¡å‹   : {train_start} ~ {train_end}")
    print(f"  å›çœ‹å¤©æ•¸   : {lookback} å¤©")
    print("=" * 55)


# =============================================================================
# CLI å…¥å£
# =============================================================================
def main():
    parser = argparse.ArgumentParser(
        description='TWII 5 æ—¥é æ¸¬æ¨¡å‹è¨»å†Šç³»çµ± - ç›´æ¥é æ¸¬æ³•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  è¨“ç·´æ¨¡å‹ï¼š
    python twii_model_registry_5d.py train --start 2020-01-01 --end 2024-01-01
  
  é æ¸¬ 5 å€‹äº¤æ˜“æ—¥å¾Œï¼š
    python twii_model_registry_5d.py predict

é æ¸¬ç­–ç•¥ï¼š
  - ä½¿ç”¨ Direct Strategyï¼ˆç›´æ¥é æ¸¬æ³•ï¼‰
  - æ¨¡å‹è¼¸å…¥ï¼šéå» 30 å¤©çš„å¤šè®Šé‡ç‰¹å¾µ
  - æ¨¡å‹è¼¸å‡ºï¼šç¬¬ 5 å€‹äº¤æ˜“æ—¥å¾Œçš„ Adj Close

è¼¸å…¥ç‰¹å¾µï¼ˆå¤šè®Šé‡ï¼‰ï¼š
  - Adj Close: èª¿æ•´å¾Œæ”¶ç›¤åƒ¹
  - Volume_Log: æˆäº¤é‡ï¼ˆLog è½‰æ›ï¼‰
  - K, D: KD æŒ‡æ¨™ï¼ˆ9, 3, 3ï¼‰
  - MACD_Hist: MACD æŸ±ç‹€åœ–ï¼ˆ12, 26, 9ï¼‰
        """
    )
    
    subparsers = parser.add_subparsers(dest='mode', help='é‹ä½œæ¨¡å¼')
    
    # train å­å‘½ä»¤
    train_parser = subparsers.add_parser('train', help='è¨“ç·´æ–°æ¨¡å‹')
    train_parser.add_argument(
        '--start',
        type=str,
        required=True,
        help='è¨“ç·´è³‡æ–™èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)'
    )
    train_parser.add_argument(
        '--end',
        type=str,
        required=True,
        help='è¨“ç·´è³‡æ–™çµæŸæ—¥æœŸ (YYYY-MM-DD)'
    )
    train_parser.add_argument(
        '--split_ratio',
        type=float,
        default=0.9,
        help='è¨“ç·´é›†æ¯”ä¾‹ (é è¨­ 0.9ï¼Œæ¯æ—¥ç¶­é‹å»ºè­°ä½¿ç”¨ 0.99 ä»¥å­¸ç¿’æœ€æ–°æ•¸æ“š)'
    )
    
    # predict å­å‘½ä»¤
    predict_parser = subparsers.add_parser('predict', help='é æ¸¬ 5 å€‹äº¤æ˜“æ—¥å¾Œçš„åƒ¹æ ¼')
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        train_mode(args)
    elif args.mode == 'predict':
        predict_mode(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
