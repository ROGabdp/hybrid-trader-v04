# -*- coding: utf-8 -*-
"""
TWII å¤šè®Šé‡æ¨¡å‹è¨»å†Šç³»çµ± (Multivariate Model Registry System)
ç‰ˆæœ¬ç®¡ç†èˆ‡è‡ªå‹•æ¨¡å‹é¸æ“‡

åŠŸèƒ½ï¼š
- train æ¨¡å¼ï¼šä½¿ç”¨å¤šè®Šé‡è¼¸å…¥ï¼ˆæŠ€è¡“æŒ‡æ¨™ï¼‰è¨“ç·´ LSTM-SSAM æ¨¡å‹ä¸¦å„²å­˜æˆå“
- predict æ¨¡å¼ï¼šæ™ºæ…§é¸æ“‡åˆé©æ¨¡å‹é€²è¡Œé æ¸¬

è¼¸å…¥ç‰¹å¾µ (Features)ï¼š
- Adj Close: èª¿æ•´å¾Œæ”¶ç›¤åƒ¹
- Volume (Log): æˆäº¤é‡ï¼ˆLog è½‰æ›ï¼‰
- K, D: KD æŒ‡æ¨™ï¼ˆ9, 3, 3ï¼‰
- MACD_Hist: MACD æŸ±ç‹€åœ–ï¼ˆ12, 26, 9ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
  è¨“ç·´ï¼špython twii_model_registry_multivariate.py train --start 2020-07-01 --end 2025-12-05
  é æ¸¬ï¼špython twii_model_registry_multivariate.py predict --target_date 2024-12-10
  é æ¸¬ï¼ˆæ˜å¤©ï¼‰ï¼špython twii_model_registry_multivariate.py predict
"""

import argparse
import json
import pickle
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Optional, Tuple, Dict, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model

# =============================================================================
# è¨­å®š
# =============================================================================
MODELS_DIR = Path(__file__).parent / "saved_models_multivariate"
LOOKBACK = 10  # å›çœ‹å¤©æ•¸ï¼ˆè«–æ–‡è¦æ ¼ï¼‰
LSTM_UNITS = 50
DROPOUT_RATE = 0.05  # [v2.0] Dropout æ¯”ç‡ï¼ˆç”¨æ–¼ MC Dropout ä¿¡å¿ƒåº¦è¨ˆç®—ï¼‰
EPOCHS = 50
BATCH_SIZE = 10
TRAIN_RATIO = 0.9
MODEL_STALE_DAYS = 180  # æ¨¡å‹éæœŸè­¦å‘Šé–¾å€¼ï¼ˆå¤©ï¼‰
MIN_TRAIN_DAYS = 1460   # æœ€ä½è¨“ç·´å¤©æ•¸ï¼ˆ4 å¹´ = 4 Ã— 365 = 1460 å¤©ï¼‰

# æŠ€è¡“æŒ‡æ¨™åƒæ•¸
KD_PARAMS = (9, 3, 3)  # (K period, K smooth, D smooth)
MACD_PARAMS = (12, 26, 9)  # (å¿«ç·š, æ…¢ç·š, è¨Šè™Ÿç·š)

# æŠ€è¡“æŒ‡æ¨™è¨ˆç®—æ‰€éœ€çš„æœ€å°è³‡æ–™ç­†æ•¸
# MACD éœ€è¦ 26 æ—¥æ…¢ç·š + 9 æ—¥è¨Šè™Ÿç·š = è‡³å°‘ 35 å¤©
# KD éœ€è¦ 9 + 3 + 3 = 15 å¤©
MIN_INDICATOR_DAYS = 50  # ä¿å®ˆä¼°è¨ˆï¼Œç¢ºä¿æŒ‡æ¨™ç©©å®š

# ä¸­æ–‡å­—å‹è¨­å®š
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


# =============================================================================
# è‡ªè¨‚ Self-Attention Layer
# =============================================================================
class SelfAttention(layers.Layer):
    """
    Sequential Self-Attention Layer (è«–æ–‡ SSAM æ¶æ§‹)
    """
    
    def __init__(self, **kwargs):
        super(SelfAttention, self).__init__(**kwargs)
    
    def build(self, input_shape):
        self.units = input_shape[-1]
        
        self.W_q = self.add_weight(
            name='W_query',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        self.W_k = self.add_weight(
            name='W_key',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        self.W_v = self.add_weight(
            name='W_value',
            shape=(self.units, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        super(SelfAttention, self).build(input_shape)
    
    def call(self, inputs):
        Q = tf.matmul(inputs, self.W_q)
        K = tf.matmul(inputs, self.W_k)
        V = tf.matmul(inputs, self.W_v)
        
        attention_scores = tf.matmul(Q, K, transpose_b=True)
        d_k = tf.cast(self.units, tf.float32)
        attention_scores = attention_scores / tf.math.sqrt(d_k)
        attention_weights = tf.nn.softmax(attention_scores, axis=-1)
        output = tf.matmul(attention_weights, V)
        
        return output
    
    def get_config(self):
        config = super(SelfAttention, self).get_config()
        return config


# =============================================================================
# ç‰¹å¾µå·¥ç¨‹ (Feature Engineering)
# =============================================================================
def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ–°å¢æŠ€è¡“æŒ‡æ¨™åˆ° DataFrame
    
    æ–°å¢æ¬„ä½ï¼š
    - Volume_Log: æˆäº¤é‡ï¼ˆLog è½‰æ›ï¼‰
    - K: KD æŒ‡æ¨™çš„ K å€¼
    - D: KD æŒ‡æ¨™çš„ D å€¼
    - MACD_Hist: MACD æŸ±ç‹€åœ–
    
    Args:
        df: åŸå§‹ OHLCV è³‡æ–™
    
    Returns:
        åŒ…å«æŠ€è¡“æŒ‡æ¨™çš„ DataFrameï¼ˆå·²ç§»é™¤ NaNï¼‰
    """
    df = df.copy()
    
    # -------------------------------------------------------------------------
    # 1. Volume Log è½‰æ›
    # -------------------------------------------------------------------------
    # ä½¿ç”¨ log1p é¿å… log(0) çš„å•é¡Œ
    df['Volume_Log'] = np.log1p(df['Volume'])
    
    # -------------------------------------------------------------------------
    # 2. KD æŒ‡æ¨™ (Stochastic Oscillator)
    # åƒæ•¸ï¼š(K period, K smooth, D smooth) = (9, 3, 3)
    # -------------------------------------------------------------------------
    k_period, k_smooth, d_smooth = KD_PARAMS
    
    # è¨ˆç®—æœ€ä½åƒ¹å’Œæœ€é«˜åƒ¹çš„æ»¾å‹•çª—å£
    low_min = df['Low'].rolling(window=k_period).min()
    high_max = df['High'].rolling(window=k_period).max()
    
    # è¨ˆç®— Raw Stochastic (%K åŸå§‹å€¼)
    # %K = (Close - Lowest Low) / (Highest High - Lowest Low) * 100
    raw_k = (df['Close'] - low_min) / (high_max - low_min) * 100
    
    # å¹³æ»‘ K å€¼ï¼ˆä½¿ç”¨ SMAï¼‰
    df['K'] = raw_k.rolling(window=k_smooth).mean()
    
    # D å€¼æ˜¯ K å€¼çš„ SMA
    df['D'] = df['K'].rolling(window=d_smooth).mean()
    
    # -------------------------------------------------------------------------
    # 3. MACD æŒ‡æ¨™
    # åƒæ•¸ï¼š(å¿«ç·šæœŸæ•¸, æ…¢ç·šæœŸæ•¸, è¨Šè™Ÿç·šæœŸæ•¸) = (12, 26, 9)
    # -------------------------------------------------------------------------
    fast_period, slow_period, signal_period = MACD_PARAMS
    
    # è¨ˆç®— EMAï¼ˆæŒ‡æ•¸ç§»å‹•å¹³å‡ï¼‰
    ema_fast = df['Close'].ewm(span=fast_period, adjust=False).mean()
    ema_slow = df['Close'].ewm(span=slow_period, adjust=False).mean()
    
    # MACD ç·š = å¿«ç·š EMA - æ…¢ç·š EMA
    macd_line = ema_fast - ema_slow
    
    # è¨Šè™Ÿç·š = MACD ç·šçš„ EMA
    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()
    
    # MACD æŸ±ç‹€åœ– = MACD ç·š - è¨Šè™Ÿç·š
    df['MACD_Hist'] = macd_line - signal_line
    
    # -------------------------------------------------------------------------
    # 4. ç§»é™¤ NaNï¼ˆæŠ€è¡“æŒ‡æ¨™è¨ˆç®—åˆæœŸæœƒç”¢ç”Ÿç©ºå€¼ï¼‰
    # -------------------------------------------------------------------------
    original_len = len(df)
    df = df.dropna()
    removed_len = original_len - len(df)
    
    if removed_len > 0:
        print(f"[ç‰¹å¾µå·¥ç¨‹] å·²ç§»é™¤ {removed_len} ç­†å« NaN çš„è³‡æ–™ï¼ˆæŠ€è¡“æŒ‡æ¨™æš–æ©ŸæœŸï¼‰")
    
    print(f"[ç‰¹å¾µå·¥ç¨‹] æ–°å¢ç‰¹å¾µï¼šVolume_Log, K, D, MACD_Hist")
    print(f"[ç‰¹å¾µå·¥ç¨‹] æœ€çµ‚è³‡æ–™ç­†æ•¸ï¼š{len(df)}")
    
    return df


# =============================================================================
# æ¨¡å‹æ¶æ§‹
# =============================================================================
def build_lstm_ssam_model(
    time_steps: int = LOOKBACK, 
    n_features: int = 5, 
    lstm_units: int = LSTM_UNITS,
    dropout_rate: float = DROPOUT_RATE
):
    """
    å»ºç«‹ LSTM + Dropout + Self-Attention æ··åˆæ¨¡å‹ï¼ˆå¤šè®Šé‡ç‰ˆæœ¬ v2.0ï¼‰
    
    [v2.0] æ–°å¢ Dropout å±¤ä»¥æ”¯æ´ MC Dropout ä¿¡å¿ƒåº¦è¨ˆç®—
    æ¶æ§‹ï¼šInput -> LSTM -> Dropout -> Self-Attention -> Flatten -> Dense(1)
    
    Args:
        time_steps: å›çœ‹å¤©æ•¸
        n_features: è¼¸å…¥ç‰¹å¾µæ•¸é‡ï¼ˆé è¨­ 5ï¼šAdj Close, Volume_Log, K, D, MACD_Histï¼‰
        lstm_units: LSTM éš±è—å±¤å–®å…ƒæ•¸
        dropout_rate: Dropout æ¯”ç‡ï¼ˆé è¨­ 0.05ï¼‰
    
    Returns:
        ç·¨è­¯å¥½çš„ Keras æ¨¡å‹
    """
    inputs = layers.Input(shape=(time_steps, n_features), name='input_layer')
    
    # LSTM å±¤
    lstm_out = layers.LSTM(units=lstm_units, return_sequences=True, name='lstm_layer')(inputs)
    
    # [v2.0] Dropout å±¤ï¼ˆç”¨æ–¼ MC Dropout ä¿¡å¿ƒåº¦è¨ˆç®—ï¼‰
    dropout_out = layers.Dropout(rate=dropout_rate, name='dropout_layer')(lstm_out)
    
    # Self-Attention å±¤
    attention_out = SelfAttention(name='self_attention')(dropout_out)
    
    # è¼¸å‡ºå±¤
    flatten_out = layers.Flatten(name='flatten_layer')(attention_out)
    outputs = layers.Dense(units=1, activation='linear', name='output_layer')(flatten_out)
    
    model = Model(inputs=inputs, outputs=outputs, name='LSTM_SSAM_Multivariate_Model_v2')
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    
    return model


# =============================================================================
# è³‡æ–™è™•ç†
# =============================================================================

# æœ¬åœ° CSV æª”æ¡ˆè·¯å¾‘
# æ³¨æ„ï¼šCSV ä¸­çš„ volume æ¬„ä½å–®ä½æ˜¯ã€Œå„„å…ƒã€ï¼ˆæˆäº¤é‡‘é¡ / 1e8ï¼‰
CSV_FILE = Path(__file__).parent / "twii_data_from_2000_01_01.csv"


def _load_csv_data() -> pd.DataFrame:
    """
    è¼‰å…¥æœ¬åœ° CSV è³‡æ–™
    
    æ³¨æ„ï¼šCSV ä¸­çš„ volume æ¬„ä½å–®ä½æ˜¯ã€Œå„„å…ƒã€
    
    Returns:
        DataFrame with DatetimeIndex and columns: Open, High, Low, Close, Volume
    """
    if not CSV_FILE.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆï¼š{CSV_FILE}")
    
    df = pd.read_csv(CSV_FILE)
    
    # è½‰æ›æ—¥æœŸæ ¼å¼ (ä¾‹å¦‚: "2025/12/9" -> Timestamp)
    df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d')
    df = df.set_index('date')
    
    # é‡æ–°å‘½åæ¬„ä½ä»¥ç¬¦åˆ yfinance æ ¼å¼
    df = df.rename(columns={
        'open': 'Open',
        'high': 'High',
        'low': 'Low',
        'close': 'Close',
        'volume': 'Volume'  # å–®ä½ï¼šå„„å…ƒ
    })
    
    return df


def _ensure_data_updated() -> None:
    """
    ç¢ºä¿ CSV è³‡æ–™å·²æ›´æ–°åˆ°ä»Šå¤©
    
    é‚è¼¯ï¼š
    1. è®€å– CSV çš„æœ€æ–°æ—¥æœŸ
    2. è‹¥æœ€æ–°æ—¥æœŸ < ä»Šå¤©ï¼Œå‰‡å‘¼å« update_twii_data.py æ›´æ–°
    """
    import subprocess
    import sys
    
    today = date.today()
    
    # è®€å– CSV æœ€æ–°æ—¥æœŸ
    df = _load_csv_data()
    last_date = df.index.max().date()
    
    print(f"[è³‡æ–™æª¢æŸ¥] ä»Šå¤©æ—¥æœŸï¼š{today}")
    print(f"[è³‡æ–™æª¢æŸ¥] CSV æœ€æ–°æ—¥æœŸï¼š{last_date}")
    
    if last_date < today:
        # æª¢æŸ¥ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼ˆé€±ä¸€è‡³é€±äº”ï¼‰
        if today.weekday() >= 5:  # é€±å…­=5, é€±æ—¥=6
            print(f"[è³‡æ–™æª¢æŸ¥] ä»Šå¤©æ˜¯é€±æœ«ï¼Œç„¡éœ€æ›´æ–°")
            return
        
        print(f"[è³‡æ–™æ›´æ–°] CSV è³‡æ–™ä¸æ˜¯æœ€æ–°ï¼Œæ­£åœ¨å‘¼å« update_twii_data.py...")
        
        update_script = Path(__file__).parent / "update_twii_data.py"
        if not update_script.exists():
            print(f"[è­¦å‘Š] æ‰¾ä¸åˆ°æ›´æ–°è…³æœ¬ï¼š{update_script}")
            print(f"[è­¦å‘Š] å°‡ä½¿ç”¨ç¾æœ‰è³‡æ–™ç¹¼çºŒ...")
            return
        
        result = subprocess.run(
            [sys.executable, str(update_script)],
            cwd=Path(__file__).parent,
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print(f"[è³‡æ–™æ›´æ–°] æ›´æ–°å®Œæˆï¼")
        else:
            print(f"[è­¦å‘Š] æ›´æ–°è…³æœ¬åŸ·è¡Œå¤±æ•—ï¼š{result.stderr}")
            print(f"[è­¦å‘Š] å°‡ä½¿ç”¨ç¾æœ‰è³‡æ–™ç¹¼çºŒ...")
    else:
        print(f"[è³‡æ–™æª¢æŸ¥] CSV è³‡æ–™å·²æ˜¯æœ€æ–°")


def load_data_by_date_range(start_date: str, end_date: str) -> pd.DataFrame:
    """
    å¾æœ¬åœ° CSV è¼‰å…¥æŒ‡å®šæ—¥æœŸç¯„åœçš„ TWII è³‡æ–™
    
    æ³¨æ„ï¼šä¸å†å¾ yfinance ä¸‹è¼‰ï¼Œæ”¹ç”¨æœ¬åœ° CSV æª”æ¡ˆ
    volume æ¬„ä½å–®ä½ç‚ºã€Œå„„å…ƒã€
    
    Args:
        start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        DataFrame with OHLCV data
    """
    print(f"[è³‡æ–™ç²å–] å¾æœ¬åœ° CSV è¼‰å…¥ TWII è³‡æ–™ ({start_date} ~ {end_date})...")
    
    # ç¢ºä¿è³‡æ–™å·²æ›´æ–°
    _ensure_data_updated()
    
    # é‡æ–°è¼‰å…¥ CSVï¼ˆå¯èƒ½å·²è¢«æ›´æ–°ï¼‰
    df = _load_csv_data()
    
    # éæ¿¾æ—¥æœŸç¯„åœ
    start_dt = pd.Timestamp(start_date)
    end_dt = pd.Timestamp(end_date)
    
    df = df[(df.index >= start_dt) & (df.index <= end_dt)]
    
    if df.empty:
        raise ValueError(f"åœ¨ CSV ä¸­æ‰¾ä¸åˆ° {start_date} ~ {end_date} ç¯„åœçš„è³‡æ–™")
    
    print(f"[è³‡æ–™ç²å–] æˆåŠŸè¼‰å…¥ {len(df)} ç­†è³‡æ–™")
    print(f"[è³‡æ–™ç²å–] å¯¦éš›æœŸé–“ï¼š{df.index[0].strftime('%Y-%m-%d')} ~ {df.index[-1].strftime('%Y-%m-%d')}")
    
    return df


def load_recent_data(lookback_days: int = 30) -> pd.DataFrame:
    """
    å¾æœ¬åœ° CSV è¼‰å…¥æœ€è¿‘çš„è³‡æ–™ç”¨æ–¼é æ¸¬
    
    æ³¨æ„ï¼šä¸å†å¾ yfinance ä¸‹è¼‰ï¼Œæ”¹ç”¨æœ¬åœ° CSV æª”æ¡ˆ
    volume æ¬„ä½å–®ä½ç‚ºã€Œå„„å…ƒã€
    
    Args:
        lookback_days: éœ€è¦çš„å›çœ‹å¤©æ•¸
    
    Returns:
        DataFrame with OHLCV data
    """
    # è¨ˆç®—éœ€è¦çš„ç¸½è³‡æ–™é‡ï¼šlookback + æŠ€è¡“æŒ‡æ¨™æš–æ©ŸæœŸ
    required_days = lookback_days + MIN_INDICATOR_DAYS
    
    print(f"[è³‡æ–™ç²å–] å¾æœ¬åœ° CSV è¼‰å…¥æœ€è¿‘ {required_days} å¤©çš„è³‡æ–™ï¼ˆå«æŠ€è¡“æŒ‡æ¨™æš–æ©ŸæœŸï¼‰...")
    
    # ç¢ºä¿è³‡æ–™å·²æ›´æ–°
    _ensure_data_updated()
    
    # é‡æ–°è¼‰å…¥ CSVï¼ˆå¯èƒ½å·²è¢«æ›´æ–°ï¼‰
    df = _load_csv_data()
    
    # å–æœ€å¾Œ required_days ç­†è³‡æ–™
    df = df.tail(required_days)
    
    if len(df) < required_days:
        print(f"[è­¦å‘Š] CSV è³‡æ–™ä¸è¶³ {required_days} ç­†ï¼Œå¯¦éš›å–å¾— {len(df)} ç­†")
    
    print(f"[è³‡æ–™ç²å–] æˆåŠŸè¼‰å…¥ {len(df)} ç­†è³‡æ–™")
    
    return df


# ç‚ºäº†å‘å¾Œç›¸å®¹ï¼Œä¿ç•™èˆŠå‡½æ•¸åç¨±ï¼ˆä½†æ¨™è¨˜ç‚ºæ£„ç”¨ï¼‰
def download_data_by_date_range(start_date: str, end_date: str) -> pd.DataFrame:
    """[å·²æ£„ç”¨] è«‹ä½¿ç”¨ load_data_by_date_range()"""
    return load_data_by_date_range(start_date, end_date)


def download_recent_data(lookback_days: int = 30) -> pd.DataFrame:
    """[å·²æ£„ç”¨] è«‹ä½¿ç”¨ load_recent_data()"""
    return load_recent_data(lookback_days)


def get_feature_columns() -> list:
    """å–å¾—ç‰¹å¾µæ¬„ä½åç¨±"""
    return ['Adj Close', 'Volume_Log', 'K', 'D', 'MACD_Hist']


def preprocess_for_training(df: pd.DataFrame, lookback: int = LOOKBACK, train_ratio: float = TRAIN_RATIO):
    """
    è¨“ç·´ç”¨è³‡æ–™é è™•ç†ï¼ˆå¤šè®Šé‡ç‰ˆæœ¬ï¼Œä½¿ç”¨é›™ç¸®æ”¾å™¨ç­–ç•¥ï¼‰
    
    [ä¿®æ­£] é˜²æ­¢è³‡æ–™æ´©æ¼ (Data Leakage Fix):
    - èˆŠé‚è¼¯ï¼šå…ˆå…¨éƒ¨è³‡æ–™ fit_transform -> å†åˆ‡åˆ† (æ¨¡å‹å·çœ‹æœªä¾†é«˜ä½é»)
    - æ–°é‚è¼¯ï¼šå…ˆåˆ‡åˆ† -> åªç”¨ Train Set fit -> å† transform å…¨é«”
    
    ç­–ç•¥èªªæ˜ï¼š
    - feature_scaler: æ­£è¦åŒ–æ‰€æœ‰è¼¸å…¥ç‰¹å¾µï¼ˆXï¼‰ï¼Œç”¨æ–¼æ¨¡å‹è¼¸å…¥
    - target_scaler: å°ˆé–€æ­£è¦åŒ–ç›®æ¨™æ¬„ä½ï¼ˆAdj Closeï¼‰ï¼Œç”¨æ–¼é‚„åŸé æ¸¬çµæœ
    
    Returns:
        X_train, y_train, X_test, y_test, feature_scaler, target_scaler, price_min, price_max, n_features
    """
    # 1. æ–°å¢æŠ€è¡“æŒ‡æ¨™
    df = add_technical_indicators(df)
    
    # 2. ç¢ºä¿æœ‰ Adj Close æ¬„ä½
    if 'Adj Close' not in df.columns:
        df['Adj Close'] = df['Close']
        print("[é è™•ç†] ä½¿ç”¨ Close æ¬„ä½ä½œç‚º Adj Close")
    
    # 3. æº–å‚™ç‰¹å¾µçŸ©é™£å’Œç›®æ¨™è®Šæ•¸
    feature_columns = get_feature_columns()
    
    for col in feature_columns:
        if col not in df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{col}")
    
    # å–å‡ºåŸå§‹æ•¸æ“š
    raw_features = df[feature_columns].values
    raw_target = df['Adj Close'].values.reshape(-1, 1)
    
    n_features = len(feature_columns)
    total_len = len(df)
    
    # -------------------------------------------------------------------------
    # [é—œéµä¿®æ­£] æ­£ç¢ºçš„åˆ‡åˆ†é»è¨ˆç®—
    # ç”±æ–¼ sequences æ˜¯å¾ lookback é–‹å§‹ï¼Œæ‰€ä»¥å¯¦éš›å¯ç”¨çš„æ¨£æœ¬æ•¸æ˜¯ total_len - lookback
    # æˆ‘å€‘éœ€è¦åœ¨ã€Œæ™‚é–“è»¸ã€ä¸Šåˆ‡åˆ†ï¼Œç¢ºä¿ scaler åªçœ‹åˆ°éå»çš„æ•¸æ“š
    # -------------------------------------------------------------------------
    
    # è¨ˆç®—è¨“ç·´é›†å¤§å°ï¼ˆåŸºæ–¼åŸå§‹æ•¸æ“šé•·åº¦ï¼Œä½†è¦æ‰£é™¤ lookback å½±éŸ¿ï¼‰
    # é€™è£¡çš„é‚è¼¯æ˜¯ï¼šæˆ‘å€‘å…ˆç®—å¥½æ™‚é–“é»ä¸Šçš„åˆ‡åˆ†ç•Œç·š
    split_idx = int(total_len * train_ratio)
    
    # å¦‚æœåˆ‡åˆ†é»å¤ªå°å°è‡´ç„¡æ³•å½¢æˆè¶³å¤  lookbackï¼Œå‰‡å¼·åˆ¶èª¿æ•´
    if split_idx <= lookback:
        raise ValueError(f"è³‡æ–™é‡ä¸è¶³æˆ– train_ratio å¤ªå°ï¼Œç„¡æ³•å»ºç«‹è¨“ç·´é›† (Split: {split_idx}, Lookback: {lookback})")
    
    print(f"[é è™•ç†] è³‡æ–™åˆ‡åˆ†é»ï¼šIndex {split_idx} (Date: {df.index[split_idx].date()})")
    
    # 4. å»ºç«‹é›™ç¸®æ”¾å™¨ (åªç”¨è¨“ç·´é›† Fit)
    train_features = raw_features[:split_idx]
    train_target = raw_target[:split_idx]
    
    feature_scaler = MinMaxScaler(feature_range=(0, 1))
    target_scaler = MinMaxScaler(feature_range=(0, 1))
    
    # Fit (å­¸ç¿’è¨“ç·´é›†çš„ Min/Max)
    feature_scaler.fit(train_features)
    target_scaler.fit(train_target)
    
    # Transform (è½‰æ›æ•´ä»½è³‡æ–™)
    scaled_features = feature_scaler.transform(raw_features)
    scaled_target = target_scaler.transform(raw_target)
    
    # 5. å»ºç«‹æ™‚åºè³‡æ–™é›†
    X, y = [], []
    for i in range(lookback, len(scaled_features)):
        # X: éå» lookback å¤©çš„æ‰€æœ‰ç‰¹å¾µ shape: (lookback, n_features)
        X.append(scaled_features[i - lookback:i])
        # y: ç›®æ¨™æ—¥çš„ Adj Closeï¼ˆå·²ç¸®æ”¾ï¼‰
        y.append(scaled_target[i, 0])
    
    X, y = np.array(X), np.array(y)
    
    # 6. åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›† (åŸºæ–¼ X çš„ç´¢å¼•)
    # æ³¨æ„ï¼šX çš„ç¬¬ 0 ç­† å°æ‡‰åŸå§‹è³‡æ–™çš„ç¬¬ lookback ç­†
    # æ‰€ä»¥åŸå§‹è³‡æ–™çš„ split_idx å°æ‡‰ X ä¸­çš„ split_idx - lookback
    train_size = split_idx - lookback
    
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # 7. è¨˜éŒ„åƒ¹æ ¼ç¯„åœï¼ˆåªè¨˜éŒ„è¨“ç·´é›†çš„ç¯„åœï¼Œé€™æ‰æ˜¯æ¨¡å‹ã€Œå·²çŸ¥ã€çš„ä¸–ç•Œï¼‰
    price_min = float(df['Adj Close'].iloc[:split_idx].min())
    price_max = float(df['Adj Close'].iloc[:split_idx].max())
    
    print(f"[é è™•ç†] è¼¸å…¥å½¢ç‹€ï¼š{X_train.shape} (samples, time_steps, n_features)")
    print(f"[é è™•ç†] ç‰¹å¾µæ•¸é‡ï¼š{n_features} ({', '.join(feature_columns)})")
    print(f"[é è™•ç†] è¨“ç·´é›†ï¼š{len(X_train)} ç­† | æ¸¬è©¦é›†ï¼š{len(X_test)} ç­†")
    print(f"[é è™•ç†] è¨“ç·´é›†åƒ¹æ ¼ç¯„åœï¼š{price_min:.2f} ~ {price_max:.2f}")
    
    return X_train, y_train, X_test, y_test, feature_scaler, target_scaler, price_min, price_max, n_features


def preprocess_for_prediction(
    df: pd.DataFrame, 
    feature_scaler: MinMaxScaler, 
    lookback: int = LOOKBACK
) -> Tuple[np.ndarray, pd.DataFrame]:
    """
    é æ¸¬ç”¨è³‡æ–™é è™•ç†ï¼ˆå¤šè®Šé‡ç‰ˆæœ¬ï¼‰
    
    Args:
        df: åŸå§‹è³‡æ–™ï¼ˆéœ€åŒ…å«è¶³å¤ çš„æ­·å²è³‡æ–™è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ï¼‰
        feature_scaler: è¨“ç·´æ™‚æ“¬åˆçš„ç‰¹å¾µç¸®æ”¾å™¨
        lookback: å›çœ‹å¤©æ•¸
    
    Returns:
        X: æ¨¡å‹è¼¸å…¥ shape (1, lookback, n_features)
        df_processed: è™•ç†å¾Œçš„ DataFrameï¼ˆç”¨æ–¼å–å¾—æœ€å¾Œæ—¥æœŸç­‰è³‡è¨Šï¼‰
    """
    # 1. è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
    df_processed = add_technical_indicators(df)
    
    # 2. ç¢ºä¿æœ‰ Adj Close æ¬„ä½
    if 'Adj Close' not in df_processed.columns:
        df_processed['Adj Close'] = df_processed['Close']
    
    # 3. æº–å‚™ç‰¹å¾µçŸ©é™£
    feature_columns = get_feature_columns()
    features = df_processed[feature_columns].values
    
    # 4. ä½¿ç”¨è¨“ç·´æ™‚çš„ç¸®æ”¾å™¨è½‰æ›
    scaled_features = feature_scaler.transform(features)
    
    # 5. ç¢ºèªè³‡æ–™é‡è¶³å¤ 
    if len(scaled_features) < lookback:
        raise ValueError(f"è³‡æ–™ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {lookback} ç­†è³‡æ–™ï¼Œç›®å‰åªæœ‰ {len(scaled_features)} ç­†")
    
    # 6. å–æœ€å¾Œ lookback ç­†è³‡æ–™
    X = scaled_features[-lookback:].reshape(1, lookback, len(feature_columns))
    
    return X, df_processed


# =============================================================================
# è¨“ç·´çµæœè¦–è¦ºåŒ–
# =============================================================================
def plot_training_results(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    start_date: str,
    end_date: str,
    rmse: float,
    r2: float
) -> Path:
    """
    ç¹ªè£½è¨“ç·´çµæœè¦–è¦ºåŒ–åœ–è¡¨
    
    Args:
        y_true: å¯¦éš›åƒ¹æ ¼
        y_pred: é æ¸¬åƒ¹æ ¼
        start_date: è¨“ç·´èµ·å§‹æ—¥æœŸ
        end_date: è¨“ç·´çµæŸæ—¥æœŸ
        rmse: å‡æ–¹æ ¹èª¤å·®
        r2: RÂ² åˆ†æ•¸
    
    Returns:
        åœ–è¡¨å„²å­˜è·¯å¾‘
    """
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    
    fig, ax = plt.subplots(figsize=(14, 6))
    
    # ç¹ªè£½å¯¦éš›èˆ‡é æ¸¬æ›²ç·š
    x_axis = range(len(y_true))
    ax.plot(x_axis, y_true, label='Actual', color='blue', linewidth=1.5, alpha=0.8)
    ax.plot(x_axis, y_pred, label='Predicted', color='red', linewidth=1.5, alpha=0.8)
    
    # æ¨™é¡Œï¼ˆå«æŒ‡æ¨™ï¼‰
    title = f"TWII Multivariate Prediction ({start_date} ~ {end_date}) | RÂ²: {r2:.4f} | RMSE: {rmse:.2f}"
    ax.set_title(title, fontsize=14, fontweight='bold')
    
    ax.set_xlabel('æ¸¬è©¦é›†æ¨£æœ¬ç´¢å¼•', fontsize=12)
    ax.set_ylabel('åƒ¹æ ¼ (Price)', fontsize=12)
    ax.legend(loc='upper left', fontsize=11)
    ax.grid(True, alpha=0.3)
    
    # æ–‡å­—æ³¨é‡‹æ–¹å¡Šï¼ˆå³ä¸‹è§’ï¼‰
    textstr = f'RÂ² = {r2:.4f}\nRMSE = {rmse:.2f}\nå¤šè®Šé‡æ¨¡å‹'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
    ax.text(0.97, 0.05, textstr, transform=ax.transAxes, fontsize=11,
            verticalalignment='bottom', horizontalalignment='right', bbox=props)
    
    plt.tight_layout()
    
    # å„²å­˜åœ–è¡¨
    plot_path = MODELS_DIR / f"plot_{start_date}_{end_date}.png"
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    plt.close(fig)
    
    print(f"[è¦–è¦ºåŒ–] è¨“ç·´çµæœåœ–è¡¨å·²å„²å­˜è‡³ï¼š{plot_path}")
    
    return plot_path


# =============================================================================
# æ¨¡å‹æˆå“ç®¡ç†
# =============================================================================
def get_artifact_paths(start_date: str, end_date: str) -> Tuple[Path, Path, Path, Path]:
    """
    å–å¾—æ¨¡å‹æˆå“è·¯å¾‘ï¼ˆå¤šè®Šé‡ç‰ˆæœ¬éœ€è¦å…©å€‹ç¸®æ”¾å™¨æª”æ¡ˆï¼‰
    
    Returns:
        model_path, feature_scaler_path, target_scaler_path, meta_path
    """
    model_path = MODELS_DIR / f"model_{start_date}_{end_date}.keras"
    feature_scaler_path = MODELS_DIR / f"feature_scaler_{start_date}_{end_date}.pkl"
    target_scaler_path = MODELS_DIR / f"target_scaler_{start_date}_{end_date}.pkl"
    meta_path = MODELS_DIR / f"meta_{start_date}_{end_date}.json"
    return model_path, feature_scaler_path, target_scaler_path, meta_path


def save_artifacts(
    model,
    feature_scaler: MinMaxScaler,
    target_scaler: MinMaxScaler,
    start_date: str,
    end_date: str,
    price_min: float,
    price_max: float,
    n_features: int,
    rmse: float = None,
    r2: float = None,
    lookback: int = LOOKBACK
):
    """å„²å­˜æ¨¡å‹æˆå“ï¼ˆå«é›™ç¸®æ”¾å™¨èˆ‡æ•ˆèƒ½æŒ‡æ¨™ï¼‰"""
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    
    model_path, feature_scaler_path, target_scaler_path, meta_path = get_artifact_paths(start_date, end_date)
    
    # å„²å­˜æ¨¡å‹
    model.save(model_path)
    print(f"[å„²å­˜] æ¨¡å‹å·²å„²å­˜è‡³ï¼š{model_path}")
    
    # å„²å­˜ç‰¹å¾µç¸®æ”¾å™¨
    with open(feature_scaler_path, 'wb') as f:
        pickle.dump(feature_scaler, f)
    print(f"[å„²å­˜] ç‰¹å¾µç¸®æ”¾å™¨å·²å„²å­˜è‡³ï¼š{feature_scaler_path}")
    
    # å„²å­˜ç›®æ¨™ç¸®æ”¾å™¨
    with open(target_scaler_path, 'wb') as f:
        pickle.dump(target_scaler, f)
    print(f"[å„²å­˜] ç›®æ¨™ç¸®æ”¾å™¨å·²å„²å­˜è‡³ï¼š{target_scaler_path}")
    
    # å„²å­˜å…ƒè³‡æ–™ï¼ˆå«æ•ˆèƒ½æŒ‡æ¨™ï¼‰
    metadata = {
        "model_type": "multivariate",
        "train_start": start_date,
        "train_end": end_date,
        "lookback": lookback,
        "n_features": n_features,
        "feature_columns": get_feature_columns(),
        "price_min": price_min,
        "price_max": price_max,
        "dropout_rate": DROPOUT_RATE,  # [v2.0] è¨˜éŒ„ Dropout ç‡
        "lstm_units": LSTM_UNITS,       # [v2.0] è¨˜éŒ„ LSTM å–®å…ƒæ•¸
        "training_timestamp": datetime.now().isoformat(),
        "technical_indicators": {
            "kd_params": list(KD_PARAMS),
            "macd_params": list(MACD_PARAMS)
        },
        "metrics": {
            "rmse": round(rmse, 2) if rmse is not None else None,
            "r2": round(r2, 4) if r2 is not None else None
        }
    }
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"[å„²å­˜] å…ƒè³‡æ–™å·²å„²å­˜è‡³ï¼š{meta_path}")


def load_artifacts(start_date: str, end_date: str) -> Tuple[Model, MinMaxScaler, MinMaxScaler, Dict[str, Any]]:
    """
    è¼‰å…¥æ¨¡å‹æˆå“ï¼ˆå¤šè®Šé‡ç‰ˆæœ¬ï¼‰
    
    Returns:
        model, feature_scaler, target_scaler, metadata
    """
    model_path, feature_scaler_path, target_scaler_path, meta_path = get_artifact_paths(start_date, end_date)
    
    # è¼‰å…¥æ¨¡å‹ï¼ˆéœ€è¨»å†Šè‡ªè¨‚å±¤ï¼‰
    model = keras.models.load_model(
        model_path,
        custom_objects={'SelfAttention': SelfAttention}
    )
    
    # è¼‰å…¥ç‰¹å¾µç¸®æ”¾å™¨
    with open(feature_scaler_path, 'rb') as f:
        feature_scaler = pickle.load(f)
    
    # è¼‰å…¥ç›®æ¨™ç¸®æ”¾å™¨
    with open(target_scaler_path, 'rb') as f:
        target_scaler = pickle.load(f)
    
    # è¼‰å…¥å…ƒè³‡æ–™
    with open(meta_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    return model, feature_scaler, target_scaler, metadata


# =============================================================================
# æ™ºæ…§æ¨¡å‹é¸æ“‡
# =============================================================================
def parse_date_from_filename(filename: str) -> Tuple[Optional[str], Optional[str]]:
    """å¾æª”åè§£ææ—¥æœŸ"""
    try:
        # æ ¼å¼ï¼šmeta_YYYY-MM-DD_YYYY-MM-DD.json
        parts = filename.replace('meta_', '').replace('.json', '').split('_')
        if len(parts) == 2:
            return parts[0], parts[1]
    except Exception:
        pass
    return None, None


def select_best_model(target_date: date) -> Optional[Dict[str, Any]]:
    """
    æ™ºæ…§é¸æ“‡æœ€é©åˆçš„æ¨¡å‹ï¼ˆå« Tie-Breaker é‚è¼¯ï¼‰
    
    é¸æ“‡é‚è¼¯ï¼š
    1. æƒææ‰€æœ‰ meta_*.json æª”æ¡ˆ
    2. ç¯©é¸ train_end_date < target_dateï¼ˆé¿å…è³‡æ–™æ´©æ¼ï¼‰
    3. æ’åºå„ªå…ˆé †åºï¼š
       - ä¸»éµ (Recency): train_end_date é™å†ªï¼ˆè¶Šæ–°è¶Šå¥½ï¼‰
       - æ¬¡éµ (Tie-breaker): train_start_date é™å†ªï¼ˆè¼ƒæ™šé–‹å§‹çš„æ¨¡å‹æ›´å°ˆç²¾æ–¼è¿‘æœŸå¸‚å ´ï¼‰
    """
    if not MODELS_DIR.exists():
        print("[æœå°‹] æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨")
        return None
    
    meta_files = list(MODELS_DIR.glob("meta_*.json"))
    if not meta_files:
        print("[æœå°‹] æ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹æª”æ¡ˆ")
        return None
    
    candidates = []
    
    for meta_file in meta_files:
        try:
            with open(meta_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            train_start = datetime.strptime(metadata['train_start'], '%Y-%m-%d').date()
            train_end = datetime.strptime(metadata['train_end'], '%Y-%m-%d').date()
            
            # è¨ˆç®—è¨“ç·´å¤©æ•¸
            duration_days = (train_end - train_start).days
            model_name = f"model_{metadata['train_start']}_{metadata['train_end']}"
            
            # ç¯©é¸ 1ï¼šè¨“ç·´å¤©æ•¸å¿…é ˆè‡³å°‘ 4 å¹´
            if duration_days < MIN_TRAIN_DAYS:
                print(f"[ç•¥é] æ¨¡å‹ {model_name} è¨“ç·´å¤©æ•¸ {duration_days} å¤©ä¸è¶³ 4 å¹´ ({MIN_TRAIN_DAYS} å¤©)")
                continue
            
            # ç¯©é¸ 2ï¼štrain_end å¿…é ˆæ—©æ–¼ target_dateï¼ˆé¿å… look-ahead biasï¼‰
            if train_end < target_date:
                candidates.append({
                    'metadata': metadata,
                    'train_start': train_start,
                    'train_end': train_end,
                    'duration_days': duration_days,
                    'gap_days': (target_date - train_end).days,
                    'model_name': model_name,
                    'r2': metadata.get('metrics', {}).get('r2', 0.0) or 0.0
                })
        except Exception as e:
            print(f"[è­¦å‘Š] ç„¡æ³•è§£æ {meta_file}: {e}")
            continue
    
    if not candidates:
        print(f"[æœå°‹] æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ¨¡å‹ï¼ˆtrain_end < {target_date}ï¼‰")
        return None
    
    # æ’åºï¼šä¸»éµ train_end é™å†ªï¼Œæ¬¡éµ r2 é™å†ªï¼Œç¬¬ä¸‰éµ train_start é™å†ª
    # train_end æœ€æ–° -> RÂ² æœ€é«˜ -> train_start æœ€æ–°ï¼ˆæ›´å°ˆç²¾ï¼‰
    candidates.sort(key=lambda x: (x['train_end'], x['r2'], x['train_start']), reverse=True)
    
    # è¼¸å‡ºå€™é¸æ¨¡å‹åˆ—è¡¨
    print(f"\n[æœå°‹] æ‰¾åˆ° {len(candidates)} å€‹å¯ç”¨æ¨¡å‹ï¼š")
    for i, c in enumerate(candidates):
        r2_display = f"RÂ²: {c['r2']:.4f}" if c['r2'] else "RÂ²: N/A"
        status = "Selected (Best Match)" if i == 0 else ""
        if i > 0:
            # åˆ¤æ–·ç‚ºä½•æœªè¢«é¸ä¸­
            if c['train_end'] < candidates[0]['train_end']:
                status = "Backup (Older end date)"
            elif c['r2'] < candidates[0]['r2']:
                status = "Backup (Lower RÂ²)"
            elif c['train_start'] < candidates[0]['train_start']:
                status = "Backup (Older start date)"
            else:
                status = "Backup"
        
        print(f"  {i+1}. {c['model_name']} ({r2_display}) -> {status}")
    
    # è¿”å›æ’åç¬¬ä¸€çš„æ¨¡å‹
    return candidates[0]['metadata']



def validate_model(metadata: Dict[str, Any], target_date: date, current_price: Optional[float] = None):
    """é©—è­‰æ¨¡å‹ä¸¦ç™¼å‡ºè­¦å‘Š"""
    train_end = datetime.strptime(metadata['train_end'], '%Y-%m-%d').date()
    gap_days = (target_date - train_end).days
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦éæœŸ
    if gap_days > MODEL_STALE_DAYS:
        print(f"\nâš ï¸ è­¦å‘Šï¼šé¸æ“‡çš„æ¨¡å‹å·²è¨“ç·´è¶…é {MODEL_STALE_DAYS} å¤©ï¼ˆè·ä»Š {gap_days} å¤©ï¼‰ï¼Œå»ºè­°é‡æ–°è¨“ç·´ã€‚")
    
    # æª¢æŸ¥åƒ¹æ ¼ç¯„åœ
    if current_price is not None:
        price_min = metadata.get('price_min', 0)
        price_max = metadata.get('price_max', float('inf'))
        
        if current_price < price_min or current_price > price_max:
            print(f"\nâš ï¸ è­¦å‘Šï¼šç•¶å‰åƒ¹æ ¼ {current_price:.2f} è¶…å‡ºè¨“ç·´æ™‚çš„åƒ¹æ ¼ç¯„åœ [{price_min:.2f}, {price_max:.2f}]")


# =============================================================================
# è¨“ç·´æ¨¡å¼
# =============================================================================
def train_mode(args):
    """è¨“ç·´æ¨¡å¼"""
    print("\n" + "=" * 60)
    print("  TWII å¤šè®Šé‡æ¨¡å‹è¨»å†Šç³»çµ± - è¨“ç·´æ¨¡å¼")
    print("=" * 60)
    
    start_date = args.start
    end_date = args.end
    
    print(f"\n[è¨­å®š] è¨“ç·´æœŸé–“ï¼š{start_date} ~ {end_date}")
    print(f"[è¨­å®š] Lookback: {LOOKBACK} | LSTM Units: {LSTM_UNITS}")
    print(f"[è¨­å®š] Epochs: {EPOCHS} | Batch Size: {BATCH_SIZE}")
    print(f"[è¨­å®š] KD åƒæ•¸: {KD_PARAMS} | MACD åƒæ•¸: {MACD_PARAMS}")
    print(f"[è¨­å®š] Split Ratio: {args.split_ratio} (è¨“ç·´é›†æ¯”ä¾‹)")
    
    # è¨­å®šéš¨æ©Ÿç¨®å­
    np.random.seed(42)
    tf.random.set_seed(42)
    
    # 1. ä¸‹è¼‰è³‡æ–™
    df = download_data_by_date_range(start_date, end_date)
    
    # 2. é è™•ç†ï¼ˆå«æŠ€è¡“æŒ‡æ¨™è¨ˆç®—ï¼‰- ä½¿ç”¨æŒ‡å®šçš„ split_ratio
    X_train, y_train, X_test, y_test, feature_scaler, target_scaler, price_min, price_max, n_features = preprocess_for_training(df, train_ratio=args.split_ratio)
    
    # 3. å»ºç«‹æ¨¡å‹
    print("\n[æ¨¡å‹] å»ºç«‹ LSTM-SSAM å¤šè®Šé‡æ¨¡å‹...")
    model = build_lstm_ssam_model(time_steps=LOOKBACK, n_features=n_features)
    model.summary()
    
    # 4. è¨“ç·´
    print(f"\n[è¨“ç·´] é–‹å§‹è¨“ç·´...")
    early_stop = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
    
    model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        callbacks=[early_stop],
        verbose=1
    )
    
    # 5. è©•ä¼°
    print("\n[è©•ä¼°] è¨ˆç®—æ¸¬è©¦é›†æŒ‡æ¨™...")
    y_pred_scaled = model.predict(X_test, verbose=0)
    
    # ä½¿ç”¨ target_scaler é‚„åŸåƒ¹æ ¼
    y_actual = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
    y_predicted = target_scaler.inverse_transform(y_pred_scaled).flatten()
    
    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))
    r2 = r2_score(y_actual, y_predicted)
    
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¨¡å‹è©•ä¼°çµæœ (å¤šè®Šé‡)")
    print("=" * 50)
    print(f"  RMSE (å‡æ–¹æ ¹èª¤å·®)  : {rmse:.2f} é»")
    print(f"  RÂ² Score (æ±ºå®šä¿‚æ•¸): {r2:.4f}")
    print("=" * 50)
    
    # 6. å„²å­˜æˆå“ï¼ˆå«é›™ç¸®æ”¾å™¨å’Œæ•ˆèƒ½æŒ‡æ¨™ï¼‰
    save_artifacts(
        model, feature_scaler, target_scaler, 
        start_date, end_date, 
        price_min, price_max, n_features,
        rmse=rmse, r2=r2
    )
    
    # 7. ç¹ªè£½è¨“ç·´çµæœåœ–è¡¨
    plot_training_results(y_actual, y_predicted, start_date, end_date, rmse, r2)
    
    print("\nâœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹æˆå“å·²å„²å­˜è‡³ saved_models_multivariate/ ç›®éŒ„")


# =============================================================================
# é æ¸¬æ¨¡å¼
# =============================================================================
def predict_mode(args):
    """é æ¸¬æ¨¡å¼ - æ”¯æ´å¤šæ­¥éè¿´é æ¸¬ï¼ˆå¤šè®Šé‡ç‰ˆæœ¬ï¼‰"""
    print("\n" + "=" * 60)
    print("  TWII å¤šè®Šé‡æ¨¡å‹è¨»å†Šç³»çµ± - é æ¸¬æ¨¡å¼")
    print("=" * 60)
    
    # è§£æç›®æ¨™æ—¥æœŸ
    if args.target_date == 'tomorrow':
        target_date = date.today() + timedelta(days=1)
        # è·³éé€±æœ«
        while target_date.weekday() >= 5:
            target_date += timedelta(days=1)
        print(f"\n[è¨­å®š] é æ¸¬ç›®æ¨™æ—¥æœŸï¼š{target_date}ï¼ˆæ˜æ—¥ï¼‰")
    else:
        target_date = datetime.strptime(args.target_date, '%Y-%m-%d').date()
        print(f"\n[è¨­å®š] é æ¸¬ç›®æ¨™æ—¥æœŸï¼š{target_date}")
    
    # é¸æ“‡æœ€ä½³æ¨¡å‹
    print("\n[æœå°‹] æ­£åœ¨æœå°‹åˆé©çš„æ¨¡å‹...")
    metadata = select_best_model(target_date)
    
    if metadata is None:
        print(f"\nâŒ æ‰¾ä¸åˆ°é©åˆç›®æ¨™æ—¥æœŸ {target_date} çš„æ­·å²æ¨¡å‹ã€‚")
        print("   è«‹å…ˆè¨“ç·´ä¸€å€‹çµæŸæ—¥æœŸæ—©æ–¼æ­¤æ—¥æœŸçš„æ¨¡å‹ã€‚")
        print(f"   ç¯„ä¾‹ï¼špython {Path(__file__).name} train --start 2020-01-01 --end {target_date - timedelta(days=1)}")
        return
    
    train_start = metadata['train_start']
    train_end = metadata['train_end']
    lookback = metadata.get('lookback', LOOKBACK)
    
    print(f"\nâœ… ä½¿ç”¨æ¨¡å‹ç‰ˆæœ¬ï¼šè¨“ç·´æœŸé–“ {train_start} è‡³ {train_end}")
    print(f"   ç‰¹å¾µæ¬„ä½ï¼š{metadata.get('feature_columns', get_feature_columns())}")
    
    # è¼‰å…¥æ¨¡å‹æˆå“ï¼ˆåŒ…å«é›™ç¸®æ”¾å™¨ï¼‰
    print("\n[è¼‰å…¥] æ­£åœ¨è¼‰å…¥æ¨¡å‹å’Œç¸®æ”¾å™¨...")
    model, feature_scaler, target_scaler, metadata = load_artifacts(train_start, train_end)
    
    # ä¸‹è¼‰æœ€è¿‘è³‡æ–™ï¼ˆéœ€ä¸‹è¼‰è¶³å¤ çš„æ­·å²è³‡æ–™ä¾†è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ï¼‰
    df = download_recent_data(lookback_days=lookback + 10)
    
    # é è™•ç†ä¸¦è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
    X, df_processed = preprocess_for_prediction(df, feature_scaler, lookback)
    
    current_price = df_processed['Adj Close'].iloc[-1]
    last_data_date = df_processed.index[-1].date()
    
    # é©—è­‰æ¨¡å‹
    validate_model(metadata, target_date, current_price)
    
    # è¨ˆç®—éœ€è¦é æ¸¬å¤šå°‘æ­¥
    days_diff = (target_date - last_data_date).days
    if days_diff <= 0:
        print(f"\nâš ï¸ ç›®æ¨™æ—¥æœŸ {target_date} å·²æœ‰æ­·å²è³‡æ–™ï¼Œè«‹é¸æ“‡æœªä¾†æ—¥æœŸã€‚")
        return
    
    # ä¼°ç®—äº¤æ˜“æ—¥æ•¸é‡ï¼ˆæ’é™¤é€±æœ«ï¼‰
    trading_days = 0
    check_date = last_data_date
    while check_date < target_date:
        check_date += timedelta(days=1)
        if check_date.weekday() < 5:  # é€±ä¸€åˆ°é€±äº”
            trading_days += 1
    
    print(f"\n[é æ¸¬] æœ€è¿‘è³‡æ–™æ—¥æœŸï¼š{last_data_date}")
    print(f"[é æ¸¬] ç›®æ¨™æ—¥æœŸï¼š{target_date}")
    print(f"[é æ¸¬] éœ€è¦é€²è¡Œ {trading_days} æ­¥éè¿´é æ¸¬")
    
    # æº–å‚™è¼¸å…¥è³‡æ–™ï¼ˆå¤šè®Šé‡åºåˆ—ï¼‰
    feature_columns = get_feature_columns()
    n_features = len(feature_columns)
    
    # å–æœ€å¾Œ lookback ç­†çš„ç¸®æ”¾å¾Œç‰¹å¾µ
    features = df_processed[feature_columns].values
    scaled_features = feature_scaler.transform(features)
    current_sequence = scaled_features[-lookback:].tolist()
    
    # å¤šæ­¥éè¿´é æ¸¬
    predictions = []
    predict_dates = []
    check_date = last_data_date
    
    for step in range(trading_days):
        # æº–å‚™è¼¸å…¥ shape: (1, lookback, n_features)
        X = np.array(current_sequence[-lookback:]).reshape(1, lookback, n_features)
        
        # é æ¸¬ä¸‹ä¸€å¤©
        y_pred_scaled = model.predict(X, verbose=0)
        predicted_scaled = y_pred_scaled[0, 0]
        
        # æ›´æ–°åºåˆ—ï¼šå°æ–¼å¤šè®Šé‡é æ¸¬ï¼Œéœ€è¦ç”¨é æ¸¬å€¼æ›´æ–° Adj Close ç‰¹å¾µ
        # å…¶ä»–ç‰¹å¾µï¼ˆVolume_Log, K, D, MACD_Histï¼‰ç„¡æ³•é æ¸¬ï¼Œä½¿ç”¨æœ€å¾Œå·²çŸ¥å€¼
        # é€™æ˜¯å¤šæ­¥éè¿´é æ¸¬çš„é™åˆ¶ï¼Œä½†å°æ–¼çŸ­æœŸé æ¸¬å½±éŸ¿è¼ƒå°
        new_row = current_sequence[-1].copy()  # è¤‡è£½æœ€å¾Œä¸€è¡Œ
        new_row[0] = predicted_scaled  # æ›´æ–°ç¬¬ä¸€å€‹ç‰¹å¾µï¼ˆAdj Close çš„ç¸®æ”¾å€¼ï¼‰
        current_sequence.append(new_row)
        
        # è¨˜éŒ„é æ¸¬çµæœ
        # ä½¿ç”¨ target_scaler é‚„åŸçœŸå¯¦åƒ¹æ ¼
        predicted_price = target_scaler.inverse_transform([[predicted_scaled]])[0, 0]
        predictions.append(predicted_price)
        
        # è¨ˆç®—å°æ‡‰çš„äº¤æ˜“æ—¥
        check_date += timedelta(days=1)
        while check_date.weekday() >= 5:  # è·³éé€±æœ«
            check_date += timedelta(days=1)
        predict_dates.append(check_date)
    
    # æœ€çµ‚é æ¸¬åƒ¹æ ¼ï¼ˆç›®æ¨™æ—¥æœŸï¼‰
    final_predicted_price = predictions[-1] if predictions else current_price
    
    # è¨ˆç®—æ¼²è·Œå¹…
    price_change = final_predicted_price - current_price
    price_change_pct = (price_change / current_price) * 100
    trend = "ğŸ“ˆ çœ‹æ¼²" if price_change > 0 else "ğŸ“‰ çœ‹è·Œ"
    
    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 50)
    print(f"ğŸ”® TWII é æ¸¬çµæœ (å¤šè®Šé‡æ¨¡å‹) - ç›®æ¨™æ—¥æœŸï¼š{target_date}")
    print("=" * 50)
    print(f"  æœ€è¿‘æ”¶ç›¤åƒ¹ ({last_data_date}) : {current_price:.2f}")
    print(f"  é æ¸¬åƒ¹æ ¼   ({target_date})   : {final_predicted_price:.2f}")
    print(f"  é æœŸè®ŠåŒ–   : {price_change:+.2f} ({price_change_pct:+.2f}%)")
    print(f"  è¶¨å‹¢åˆ¤æ–·   : {trend}")
    print("=" * 50)
    print(f"  ä½¿ç”¨æ¨¡å‹   : {train_start} ~ {train_end}")
    print(f"  é æ¸¬æ­¥æ•¸   : {trading_days} å€‹äº¤æ˜“æ—¥")
    print(f"  è¼¸å…¥ç‰¹å¾µ   : {n_features} å€‹")
    print("=" * 50)
    
    # é¡¯ç¤ºé€æ—¥é æ¸¬ï¼ˆå¦‚æœæ­¥æ•¸ä¸å¤šï¼‰
    if trading_days <= 60:
        print("\nğŸ“Š é€æ—¥é æ¸¬æ˜ç´°ï¼š")
        print("-" * 40)
        prev_price = current_price
        for i, (pred_date, pred_price) in enumerate(zip(predict_dates, predictions)):
            daily_change = pred_price - prev_price
            daily_pct = (daily_change / prev_price) * 100
            print(f"  {pred_date} : {pred_price:.2f} ({daily_change:+.2f}, {daily_pct:+.2f}%)")
            prev_price = pred_price
        print("-" * 40)


# =============================================================================
# CLI å…¥å£
# =============================================================================
def main():
    parser = argparse.ArgumentParser(
        description='TWII å¤šè®Šé‡æ¨¡å‹è¨»å†Šç³»çµ± - ç‰ˆæœ¬ç®¡ç†èˆ‡è‡ªå‹•æ¨¡å‹é¸æ“‡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  è¨“ç·´æ¨¡å‹ï¼š
    python twii_model_registry_multivariate.py train --start 2020-01-01 --end 2024-01-01
  
  é æ¸¬æ˜å¤©ï¼š
    python twii_model_registry_multivariate.py predict
  
  é æ¸¬æŒ‡å®šæ—¥æœŸï¼š
    python twii_model_registry_multivariate.py predict --target_date 2024-12-10

è¼¸å…¥ç‰¹å¾µï¼ˆå¤šè®Šé‡ï¼‰ï¼š
  - Adj Close: èª¿æ•´å¾Œæ”¶ç›¤åƒ¹
  - Volume_Log: æˆäº¤é‡ï¼ˆLog è½‰æ›ï¼‰
  - K, D: KD æŒ‡æ¨™ï¼ˆ9, 3, 3ï¼‰
  - MACD_Hist: MACD æŸ±ç‹€åœ–ï¼ˆ12, 26, 9ï¼‰
        """
    )
    
    subparsers = parser.add_subparsers(dest='mode', help='é‹ä½œæ¨¡å¼')
    
    # train å­å‘½ä»¤
    train_parser = subparsers.add_parser('train', help='è¨“ç·´æ–°æ¨¡å‹')
    train_parser.add_argument(
        '--start',
        type=str,
        required=True,
        help='è¨“ç·´è³‡æ–™èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)'
    )
    train_parser.add_argument(
        '--end',
        type=str,
        required=True,
        help='è¨“ç·´è³‡æ–™çµæŸæ—¥æœŸ (YYYY-MM-DD)'
    )
    train_parser.add_argument(
        '--split_ratio',
        type=float,
        default=0.9,
        help='è¨“ç·´é›†æ¯”ä¾‹ (é è¨­ 0.9ï¼Œæ¯æ—¥ç¶­é‹å»ºè­°ä½¿ç”¨ 0.99 ä»¥å­¸ç¿’æœ€æ–°æ•¸æ“š)'
    )
    
    # predict å­å‘½ä»¤
    predict_parser = subparsers.add_parser('predict', help='é æ¸¬åƒ¹æ ¼')
    predict_parser.add_argument(
        '--target_date',
        type=str,
        default='tomorrow',
        help='é æ¸¬ç›®æ¨™æ—¥æœŸ (YYYY-MM-DD)ï¼Œé è¨­ç‚ºæ˜å¤©'
    )
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        train_mode(args)
    elif args.mode == 'predict':
        predict_mode(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
